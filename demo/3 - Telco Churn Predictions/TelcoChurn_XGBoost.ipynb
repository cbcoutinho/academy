{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! Look, right now is downloading data that has nulls etc. \n",
    "If you want to change it to the data without problems. Just change the name of the data that is downloaded in the third block below. I left a little comment on where to change the data.\n",
    "It would be from \"telco_churn_broken.csv\" to \"Data.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First imports\n",
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project where your bucket is located\n",
    "gcp_bucket_project = \"ydatasynthetic\"\n",
    "\n",
    "input_files_path = \"gs://test_output_pipelines/\"\n",
    "output_files_path = \"gs://test_output_pipelines/Telco_runs/\"\n",
    "\n",
    "gcp_service_account = {\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"ydatasynthetic\",\n",
    "    \"private_key_id\": \"51bf83da95afddb07974394811dcf70991c445d1\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDeZbexWWw6/KP4\\ncEBKJA0Tvmc6UmqXj/rIvKjODa9Oe9tELwjBxito6FGU5M2M3tgx8FhwaC7KfFdN\\nzm1NNwbIDLLThMN3akjiukUC4wVyjN+oG0237KA3eGHXgi2988Tvc9tGIqV5aWAX\\n5/c74t64vvaLQQY8KJlKxXzWEEESB5WQQPMXMyD7ztjo3BFA2g/Z2/zVQZHMusXb\\n2/19HjZ7n4UV5fCvlRIGKbfJESrhTgXidzB5s92VAbxLkxY18MlXJhH7AGDaor47\\nE0FNJhh8D+Za7B24ZOeDZsw2JB9siX7tbF3OAaOxb2U+SH5DwxpbLhD+Kz6co56c\\nTp30APWHAgMBAAECggEAT8uqUalWooGEab8I6jEuOf/OtQfeM/xWjJDYdnpFhMD+\\n5uODXld2uuDkaKqBsAJDCwQED2zqIP/lKUcgQ5yzQ7L+tNMFeUK63RLdcRA4o6W1\\nF/GX/J6by9tXK8aLGAUvi4UDz5Hd0obDEkDKHgQkSqBGAKf3jz035zKdvIa/ejJw\\nNoh7IB+ZVY7rcbwUbszq8jnQsde0KcBgagckVU0iV9ed1/HLPGK1iZ3u1WpGx13R\\nKkFX516AzwUr+4FXTwHaEDIDnljVf2to8vSv+i4YWLWXvgF5jol2Ml6+Lx9Aktgt\\nclD3pC+hfmCriYQHoiw+MAiCNTDbMLG2Sm9dY9N/QQKBgQDvxPZX95EvH5QAUtZY\\n2qkbCewvRSvlo0wsSaVUsGLBgbYLVoTSPbHzY2GObECHxDtUTby8uN2aIcQI0YJy\\nLer62ZzHedt8JGWt98C+YJFbORSKprzspYUkdDbHyVE8IY8fgkXB8O0yVabK0EvT\\nwU+K14korJWVrpj4LsT9lCvQwQKBgQDtc7Wr42SXlDXTwCS8Oivzeh+TV+owruC1\\niQIOLd2TLT6C2S4JHvTyo2tJNVlJlAGDiLbsTpoLzTBU+gKALU7+NmwWxXy1dcBA\\niXWtRSq7t8QsTRw53WGO96PN56wYgGW1fY5ZvuuQ5HCLpKkGnExaWx75Z0w+0I+Z\\n4Q6KhE4QRwKBgHKV1oTDs4apuLlO3VEpbpeVbmJI1GmyWLzxlSmcAhYoPy+78U3o\\nXhrK69x8r3oytYfcfs5SKtV2PZFZTtXyS9IGHX7XJaV2X4F8XYid7dmCteZtOOx/\\nTdnWEDf6ZfAuuY99WokM8s6TUFqsoRBZ2gGob8wYboRSJJi3AT2OikIBAoGBAI+U\\nQ+nn2c9+Iv4jVRMr6Z+T3LvvkQBW/jFG4yNHwI9Z8E6WnCuPuHVvM1RVGeRPxoqO\\nhG1nKeaw7+SyOAjhXL6CkeFDESF4TgO4GpvnEuvnNovDJKoonvEf9MBwsbMvEFEp\\n1oWV2EMInrURn9MLd2+7YYNVXtksq2hUmcJQNhPJAoGBAOgodT5MyZ9NygREo4cw\\n4jb8XCizRuPDAS+u/i1AfjVfa0ROHrOtUxSMHYbNTW/4+9bChILplp9dEdx7JNoT\\nknbXSsjiR9Z/8E00T5gLwzX6koIFWJzQ1TsXdJ5/7MhzhKKay716n26auUE7+njQ\\np1VStn+K4PjpMDzuP7hVB9+w\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"ydata-development@ydatasynthetic.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"113541910872624890238\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/ydata-development%40ydatasynthetic.iam.gserviceaccount.com\",\n",
    "}\n",
    "\n",
    "pip_packages_to_install = [\n",
    "    \"scikit-learn==0.22.2\",\n",
    "    \"numpy==1.17.2\",\n",
    "    \"pandas==1.0.3\",\n",
    "    \"xgboost==1.0.2\",\n",
    "    \"gcsfs\",\n",
    "    \"tensorflow==2.2.0\",\n",
    "    \"seaborn==0.9.0\",\n",
    "    \"matplotlib==3.1.1\",\n",
    "    \"mpld3==0.5.1\",\n",
    "]\n",
    "\n",
    "# TelcoChurnXGB parameters\n",
    "n_estimators = 100\n",
    "verbosity = 0\n",
    "max_depth = 2\n",
    "eta = 1\n",
    "silent = 0\n",
    "\n",
    "# Kfp\n",
    "pipeline_name = \"NOS Churn Prediction Pipeline\"\n",
    "pipeline_description = \"NOS Churn predictions\"\n",
    "experiment_name = \"NOS experiment\"\n",
    "run_name = \"NOS Run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data file from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(\n",
    "    df_churn_op: OutputPath(),\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_input_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    import os\n",
    "    import io\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_input_path.endswith(\"/\") : in_gcp_bucket_input_path+= \"/\";\n",
    "    \n",
    "    df_churn = pd.read_csv(in_gcp_bucket_input_path + \"broken_telco_churn.csv\")                # HERE IS TAKING THE BROKEN DATA, CLEAN ONE IS \"Data.csv\"\n",
    "    df_churn.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    # A DataFrame too long cannot be displayed in the Artifacts\n",
    "\n",
    "    df_disp = df_churn.iloc[0:5]\n",
    "    df_disp = df_disp[\n",
    "        [\"customerID\", \"gender\", \"tenure\", \"Contract\", \"TotalCharges\", \"Churn\"]\n",
    "    ]\n",
    "\n",
    "    df_disp.to_csv(in_gcp_bucket_input_path + \"Data_Sample.csv\", index=False)\n",
    "\n",
    "    df_show = pd.read_csv(in_gcp_bucket_input_path + \"Data_Sample.csv\")\n",
    "    categorical_cols = [\n",
    "        c\n",
    "        for c in df_show.columns\n",
    "        if df_show[c].dtype == \"object\" or c == \"SeniorCitizen\"\n",
    "    ]\n",
    "\n",
    "    numerical_cols = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "    schema = [\n",
    "        {\"name\": c, \"type\": \"CATEGORY\" if c in categorical_cols else \"NUMBER\"}\n",
    "        for c in df_show.columns\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"table\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"format\": \"csv\",\n",
    "                \"header\": [x[\"name\"] for x in schema],\n",
    "                \"source\": in_gcp_bucket_input_path + \"Data_Sample.csv\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.func_to_container_op(\n",
    "    func=read_data,\n",
    "    output_component_file=\"./read-data-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Expectation Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(\n",
    "          csv_path: InputPath(),\n",
    "          expectation_suite_json: dict,\n",
    "          data_doc_path: OutputPath(),\n",
    "          validation: OutputPath(),\n",
    "):\n",
    "    import json\n",
    "    import os\n",
    "    import sys\n",
    "    import great_expectations as ge\n",
    "    from great_expectations.render import DefaultJinjaPageView\n",
    "    from great_expectations.render.renderer import ValidationResultsPageRenderer\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    df = ge.read_csv(csv_path, expectation_suite=expectation_suite_json)\n",
    "    result = df.validate()\n",
    "    document_model = ValidationResultsPageRenderer().render(result)\n",
    "    os.makedirs(os.path.dirname(data_doc_path), exist_ok=True)\n",
    "    with open(data_doc_path, 'w') as writer:\n",
    "        writer.write(DefaultJinjaPageView().render(document_model))\n",
    "    text = DefaultJinjaPageView().render(document_model)\n",
    "    print(f'Saved: {data_doc_path}')\n",
    "        \n",
    "    if not result.success:\n",
    "        result = 'fail'\n",
    "    else:\n",
    "        result = 'success'\n",
    "        \n",
    "    with open(validation, 'w') as f:\n",
    "            f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the expectation suite for the validation\n",
    "import json\n",
    "with open(\"./Telco Churn Predictions/expectation_suite/telco_expectation_suite.json\") as file:\n",
    "          expectation_suite = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_validate_data = func_to_container_op(\n",
    "    func=validate_data,\n",
    "    output_component_file=\"./validate_ge-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install+[\"great-expectations==0.13.11\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning in case of failed great expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(\n",
    "    df_ip: InputPath(),\n",
    "    cleaned_df: OutputPath(),\n",
    "):\n",
    "    from pandas import read_csv\n",
    "    \n",
    "    df = read_csv(df_ip)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.loc[(df['tenure']<=72) & (df['tenure']>0)]\n",
    "    df.to_csv(cleaned_df,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_clean_data = kfp.components.func_to_container_op(\n",
    "    func=clean_data,\n",
    "    output_component_file=\"./read-data-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(\n",
    "    df_churn_ip: InputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    empty_cols = [\n",
    "        \"customerID\",\n",
    "        \"gender\",\n",
    "        \"SeniorCitizen\",\n",
    "        \"Partner\",\n",
    "        \"Dependents\",\n",
    "        \"tenure\",\n",
    "        \"PhoneService\",\n",
    "        \"MultipleLines\",\n",
    "        \"InternetService\",\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "        \"Contract\",\n",
    "        \"PaperlessBilling\",\n",
    "        \"PaymentMethod\",\n",
    "        \"MonthlyCharges\",\n",
    "        \"TotalCharges\",\n",
    "        \"Churn\",\n",
    "    ]\n",
    "\n",
    "    for i in empty_cols:\n",
    "        df_churn[i] = df_churn[i].replace(\" \", np.nan)\n",
    "\n",
    "    df_churn.drop([\"customerID\"], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    binary_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n",
    "\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    # Encoding column 'gender'\n",
    "    df_churn[\"gender\"] = df_churn[\"gender\"].replace({\"Male\": 1, \"Female\": 0})\n",
    "    \n",
    "    df_churn.to_csv(df_churn_op,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_preprocess_data = kfp.components.func_to_container_op(\n",
    "    func=preprocessing_data,\n",
    "    output_component_file=\"./read-data-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_data(\n",
    "    df_churn_ip: InputPath(),\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "):\n",
    "    \n",
    "    from pandas import read_csv\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    data = read_csv(df_churn_ip)\n",
    "    sample = data.sample(1000)\n",
    "    \n",
    "    cat_cols=[\"PaymentMethod\",\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\n",
    "        \"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\"]\n",
    "\n",
    "    schema = [\n",
    "        {\"name\": c, \"type\": \"CATEGORY\" if c in cat_cols else \"NUMBER\"}\n",
    "        for c in data.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"table\",\n",
    "                \"format\": \"csv\",\n",
    "                \"header\": [x[\"name\"] for x in schema],\n",
    "                \"source\": sample.to_csv(index=False),\n",
    "                \"storage\": \"inline\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_synthesize_data = kfp.components.func_to_container_op(\n",
    "    func=synthesize_data,\n",
    "    output_component_file=\"./read-data-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration of Categorical Data Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the way the data is distributed along the categorical variables we will create some bar plots that can be visualized in the YData's Platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    # Churn Plot\n",
    "    ax = sns.catplot(\n",
    "        y=\"Churn\",\n",
    "        kind=\"count\",\n",
    "        data=df,\n",
    "        height=2.0,\n",
    "        aspect=3.0,\n",
    "        palette=\"bright\",\n",
    "        legend=True,\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Graphs/churn_plot.html\", \"w\") as f:\n",
    "        f.write(s)\n",
    "\n",
    "    def barplot_percentages(feature, orient=\"v\", axis_name=\"percentage of customers\"):\n",
    "        ratios = pd.DataFrame()\n",
    "        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n",
    "        g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n",
    "        g[axis_name] = g[axis_name] / len(df)\n",
    "        if orient == \"v\":\n",
    "            ax = sns.barplot(\n",
    "                x=feature,\n",
    "                y=axis_name,\n",
    "                hue=\"Churn\",\n",
    "                data=g,\n",
    "                orient=orient,\n",
    "                palette=\"bright\",\n",
    "            )\n",
    "            ax.set_yticklabels([\"{:,.0%}\".format(y) for y in ax.get_yticks()])\n",
    "        else:\n",
    "            ax = sns.barplot(\n",
    "                x=axis_name,\n",
    "                y=feature,\n",
    "                hue=\"Churn\",\n",
    "                data=g,\n",
    "                orient=orient,\n",
    "                palette=\"bright\",\n",
    "            )\n",
    "            ax.set_xticklabels([\"{:,.0%}\".format(x) for x in ax.get_xticks()])\n",
    "        ax.plot()\n",
    "\n",
    "    # Genders\n",
    "    df[\"churn_rate\"] = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "    g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=0.9)\n",
    "    ax = g.map(\n",
    "        sns.barplot, \"gender\", \"churn_rate\", palette=\"bright\", order=[\"Female\", \"Male\"]\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Genders.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Multiple Lines\n",
    "    fig = plt.figure(figsize=(9, 4.5))\n",
    "    barplot_percentages(\"MultipleLines\", orient=\"v\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Service-wise Columns analysis\n",
    "    cols = [\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "    ]\n",
    "    df1 = pd.melt(df[df[\"InternetService\"] != \"No\"][cols]).rename(\n",
    "        {\"value\": \"Has service\"}, axis=1\n",
    "    )\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    ax = sns.countplot(data=df1, x=\"variable\", hue=\"Has service\", palette=\"bright\")\n",
    "    ax.set(xlabel=\"Additional service\", ylabel=\"Num of customers\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Service-wise Columns analysis2\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    df1 = df[(df.InternetService != \"No\") & (df.Churn == \"Yes\")]\n",
    "    df1 = pd.melt(df1[cols]).rename({\"value\": \"Has service\"}, axis=1)\n",
    "    ax = sns.countplot(\n",
    "        data=df1,\n",
    "        x=\"variable\",\n",
    "        hue=\"Has service\",\n",
    "        hue_order=[\"No\", \"Yes\"],\n",
    "        palette=\"bright\",\n",
    "    )\n",
    "    ax.set(xlabel=\"Additional service\", ylabel=\"Num of churns\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Generating Metadata\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/churn_plot.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Genders.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_categorical_analysis = kfp.components.func_to_container_op(\n",
    "    func=categorical_analysis,\n",
    "    output_component_file=\"./categorical_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Statistical Analysis and Artifact Generation for Numerical and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    df[\"total_charges_to_tenure_ratio\"] = df[\"TotalCharges\"] / df[\"tenure\"]\n",
    "    df[\"monthly_charges_diff\"] = (\n",
    "        df[\"MonthlyCharges\"] - df[\"total_charges_to_tenure_ratio\"]\n",
    "    )\n",
    "    df[\"churn_rate\"] = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "\n",
    "    # Internet Service vs Monthly Charges\n",
    "    ax = sns.catplot(\n",
    "        x=\"InternetService\",\n",
    "        y=\"MonthlyCharges\",\n",
    "        hue=\"Churn\",\n",
    "        kind=\"violin\",\n",
    "        split=True,\n",
    "        palette=\"pastel\",\n",
    "        data=df,\n",
    "        height=4.2,\n",
    "        aspect=1.4,\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/violinplot2.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/violinplot2.html\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_mixed_analysis = kfp.components.func_to_container_op(\n",
    "    func=mixed_analysis,\n",
    "    output_component_file=\"./mixed_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    # kdeplots - tenure, Monthly Charges, Total Charges\n",
    "    def kdeplot(feature):\n",
    "        fig = plt.figure(figsize=(9, 4))\n",
    "        plt.title(\"KDE for {}\".format(feature))\n",
    "        ax0 = sns.kdeplot(\n",
    "            df[df[\"Churn\"] == \"No\"][feature].dropna(), color=\"navy\", label=\"Churn: No\"\n",
    "        )\n",
    "        ax1 = sns.kdeplot(\n",
    "            df[df[\"Churn\"] == \"Yes\"][feature].dropna(),\n",
    "            color=\"orange\",\n",
    "            label=\"Churn: Yes\",\n",
    "        )\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        s = mpld3.fig_to_html(fig)\n",
    "\n",
    "        with file_io.FileIO(\n",
    "            in_gcp_bucket_output_path + \"Artifacts/Graphs/{}.html\".format(feature), \"w\"\n",
    "        ) as f:\n",
    "            f.write(s)\n",
    "\n",
    "    kdeplot(\"TotalCharges\")\n",
    "\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/TotalCharges.html\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_numerical_analysis = kfp.components.func_to_container_op(\n",
    "    func=numerical_analysis,\n",
    "    output_component_file=\"./numerical_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(\n",
    "    df_churn_ip: InputPath(), df_one_hot: OutputPath()\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    category_cols = [\n",
    "        \"PaymentMethod\",\n",
    "        \"MultipleLines\",\n",
    "        \"InternetService\",\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "        \"Contract\",\n",
    "    ]\n",
    "\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "\n",
    "    df_churn_targets = df_churn[\"Churn\"].unique()\n",
    "    df_churn[\"Churn\"] = df_churn[\"Churn\"].replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    df_churn.to_csv(df_one_hot, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_one_hot_encode = kfp.components.func_to_container_op(\n",
    "    func=one_hot_encode,\n",
    "    output_component_file=\"./one-hot-encode-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def xgb_model(\n",
    "    df_churn_ip: InputPath(),\n",
    "    n_estimators: int,\n",
    "    verbosity: int,\n",
    "    max_depth: int,\n",
    "    eta: int,\n",
    "    silent: int,\n",
    "    conf_matr: OutputPath(),\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    mlpipeline_metrics: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost as xgb\n",
    "    import json\n",
    "    import os\n",
    "    import io\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        accuracy_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "    )\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn[\"Churn\"]\n",
    "    X1 = df_churn.drop([\"Churn\"], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "\n",
    "    clfxg = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\", verbosity=0, max_depth=2, eta=1, silent=0\n",
    "    )\n",
    "    clfxg.fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = clfxg.predict(X_test)\n",
    "\n",
    "    y_test_proba = clfxg.predict_proba(X_test)[:, 0]\n",
    "\n",
    "    xgb_score = float(\"%.4f\" % accuracy_score(y_test, y_test_pred))\n",
    "    xgb_precision = float(\"%.4f\" % precision_score(y_test, y_test_pred))\n",
    "    xgb_recall = float(\"%.4f\" % recall_score(y_test, y_test_pred))\n",
    "    xgb_f1 = float(\"%.4f\" % f1_score(y_test, y_test_pred))\n",
    "\n",
    "    print(\"Accuracy, Precision, Recall, f1: \")\n",
    "    print(xgb_score, xgb_precision, xgb_recall, xgb_f1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "    auc_score = float(\"%.4f\" % roc_auc_score(y_test, y_test_proba))\n",
    "    print(\"Auc score: \")\n",
    "    print(auc_score)\n",
    "\n",
    "    # Converting the matrix to a Dataframe\n",
    "    flags = {0: \"Not Churned\", 1: \"Churned\"}\n",
    "    flag_list = [\"Not Churned\", \"Churned\"]\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "    print(df_cm)\n",
    "\n",
    "    with file_io.FileIO(conf_matr, \"w\") as f:\n",
    "        df_cm.to_csv(\n",
    "            f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/XGBConf_mat.csv\", \"w\") as f:\n",
    "        df_cm.to_csv(\n",
    "            f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    # roc curve\n",
    "    df_roc = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": thresholds})\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/XGBROC_curve.csv\", \"w\") as f:\n",
    "        df_roc.to_csv(\n",
    "            f, columns=[\"fpr\", \"tpr\", \"thresholds\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    # code to generate artifacts\n",
    "\n",
    "    # Artifact generator - metadata\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"source\": in_gcp_bucket_output_path\n",
    "                + \"Artifacts/Conf_matXGB.csv\",  # conf_matr\n",
    "                # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "                \"labels\": flag_list,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"roc\",\n",
    "                \"format\": \"csv\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"fpr\", \"type\": \"NUMBER\"},\n",
    "                    {\"name\": \"tpr\", \"type\": \"NUMBER\"},\n",
    "                    {\"name\": \"thresholds\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/ROC_curveXGB.csv\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": \"accuracy-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_score,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"precision-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_precision,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"recall\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_recall,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"f1-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_f1,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"auc-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": auc_score,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_metrics, \"w\") as f:\n",
    "        json.dump(metrics, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_xgb_model = kfp.components.func_to_container_op(\n",
    "    func=xgb_model,\n",
    "    output_component_file=\"./xgb-model-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=pipeline_name,\n",
    "    description=pipeline_description,\n",
    ")\n",
    "\n",
    "\n",
    "# Create pipeline and set default values\n",
    "def TelcoChurnXGB_func(\n",
    "    in_n_estimators: int = n_estimators,\n",
    "    in_verbosity: int = verbosity,\n",
    "    in_max_depth: int = max_depth,\n",
    "    in_eta: int = eta,\n",
    "    in_silent: int = silent,\n",
    "    in_input_files_path: str = input_files_path,\n",
    "    in_output_files_path: str = output_files_path,\n",
    "    in_gcp_bucket_project: str = gcp_bucket_project,\n",
    "    in_gcp_service_account: dict = gcp_service_account,\n",
    "):\n",
    "    \n",
    "    read_normal = True\n",
    "    # Passing pipeline parameter and add default values\n",
    "    read_data_task = kfp_read_data(\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_input_path=in_input_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    validate_csv = kfp_validate_data(\n",
    "        read_data_task.output, \n",
    "        expectation_suite_json=expectation_suite)\n",
    "\n",
    "    with dsl.Condition(validate_csv.outputs[\"validation\"] == 'fail'):\n",
    "        clean_data = kfp_clean_data(\n",
    "            df_ip=read_data_task.outputs[\"df_churn_op\"]\n",
    "        )\n",
    "\n",
    "        read_normal = False\n",
    "\n",
    "    if read_normal:\n",
    "        data_to_read = read_data_task.outputs[\"df_churn_op\"]\n",
    "    else:\n",
    "        data_to_read = clean_data.outputs[\"cleaned_df\"]\n",
    "        \n",
    "    preprocess_data_task = kfp_preprocess_data(\n",
    "        df_churn_ip=data_to_read\n",
    "    )\n",
    "    \n",
    "    synthesize = kfp_synthesize_data(\n",
    "        df_churn_ip=data_to_read\n",
    "    ) \n",
    "    \n",
    "    cat_analysis_task = kfp_categorical_analysis(\n",
    "        df_churn_ip=data_to_read,\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "     \n",
    "    mix_analysis_task = kfp_mixed_analysis(\n",
    "        df_churn_ip=data_to_read,\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    num_analysis_task = kfp_numerical_analysis(\n",
    "        df_churn_ip=mix_analysis_task.outputs[\"df_churn_op\"],\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    ohe_task = kfp_one_hot_encode(\n",
    "        df_churn_ip=preprocess_data_task.outputs[\"df_churn_op\"],\n",
    "        )\n",
    "\n",
    "    xgb_model_task = kfp_xgb_model(\n",
    "        ohe_task.outputs[\"df_one_hot\"],\n",
    "        in_n_estimators,\n",
    "        in_verbosity,\n",
    "        in_max_depth,\n",
    "        in_eta,\n",
    "        in_silent,\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kfp.compiler as comp\n",
    "\n",
    "pipeline_func = TelcoChurnXGB_func\n",
    "pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\n",
    "\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline already exists: NOS Churn Prediction Pipeline\n",
      "Creating a new pipeline version: NOS Churn Prediction Pipeline [2d6c6a68-60b4-49cf-ade4-3b00906b0784]\n",
      "Creating a new experiment or use an existing one: NOS experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://demo.azure.ydata.ai/_/pipeline//#/experiments/details/3d5dad0e-8605-47e9-9d79-7731724f8c44\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new run: NOS Run [57]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://demo.azure.ydata.ai/_/pipeline//#/runs/details/9e1f0803-a56e-4584-a504-9a4fb0236d5d\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "from random import randrange\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "# check if pipeline already exists -> if not, create a new one\n",
    "filter = json.dumps(\n",
    "    {\n",
    "        \"predicates\": [\n",
    "            {\"key\": \"name\", \"op\": 1, \"string_value\": \"{}\".format(pipeline_name)}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pipeline = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if pipeline.pipelines is None:\n",
    "    print(\"Creating a new pipeline: \" + pipeline_name)\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(\n",
    "        pipeline_filename, name=pipeline_name, description=pipeline_description\n",
    "    )\n",
    "else:\n",
    "    print(\"Pipeline already exists: \" + pipeline_name)\n",
    "    pipeline = pipeline.pipelines[0]\n",
    "    \n",
    "pipeline_version = str(uuid.uuid4())\n",
    "\n",
    "print(\"Creating a new pipeline version: \" + pipeline_name + str(\" [\" + pipeline_version + \"]\"))\n",
    "client.pipeline_uploads.upload_pipeline_version(\n",
    "    pipeline_filename,\n",
    "    name=pipeline_name + str(\" [\" + pipeline_version + \"]\"),\n",
    "    pipelineid=pipeline.id,\n",
    ")\n",
    "\n",
    "# create a new experiment or use an existing one\n",
    "print(\"Creating a new experiment or use an existing one: \" + experiment_name)\n",
    "experiment = client.create_experiment(name=experiment_name)\n",
    "\n",
    "# create a new run with a random identifier\n",
    "run_random_id = str(randrange(1000))\n",
    "print(\"Creating a new run: \" + run_name + \" [\" + run_random_id + \"]\")\n",
    "new_run = client.run_pipeline(\n",
    "    experiment.id, run_name + \" [\" + run_random_id + \"]\", pipeline_id=pipeline.id\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
