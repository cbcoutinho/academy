{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0eb9bfb6d2367f5ad13c071e9ca0a4847a6ac5b11a61adc2b39beabe858de7ad1",
   "display_name": "Python 3.7.9 64-bit ('ydata_tmp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# S3Connector - Quick Start\n",
    "\n",
    "The S3Connector enables you to read/write data within the AWS Simple Storage Service with ease and integrate it with YData's platform.\n",
    "Reading a dataset from S3 directly into a YData's `Dataset` allows its usage for Data Quality, Data Synthetisation and Preprocessing blocks.\n",
    "\n",
    "The following tutorial covers:\n",
    "- How to read data from S3\n",
    "- How to read data (sample) from S3\n",
    "- How to write data to S3\n",
    "- (Advanced) Developer utilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from ydata.connectors import S3Connector\n",
    "from ydata.connectors.filetype import FileType\n",
    "from ydata.utils.formats import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your credentials from a file\n",
    "token = read_json('../../.secrets/s3_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Connector\n",
    "connector = S3Connector(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My data is of type Dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset\n",
    "data = connector.read_file('S3://ydata-demos/teste.csv', file_type=FileType.CSV)\n",
    "print(f'My data is of type {type(data).__name__}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file_type argument is optional. If not provided, we will infer it from the path you have provided.\n",
    "parquet_data = connector.read_file('S3://ydata-demos/teste.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick glimpse, we can load a small subset of the data (e.g. 1%)\n",
    "small_data = connector.read_sample('S3://ydata-demos/teste.csv', sample_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could alternatively define a specific number of rows\n",
    "very_small_data = connector.read_sample('S3://ydata-demos/teste.csv', sample_size=67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows:\nOriginal: 10,000, \nSampled (%): 100\nSampled (n): 67.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Number of rows:\n",
    "Original: {data.shape[0]:,}, \n",
    "Sampled (%): {small_data.shape[0]:,}\n",
    "Sampled (n): {very_small_data.shape[0]:,}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/francisco/anaconda3/envs/ydata_tmp/lib/python3.7/site-packages/dask/dataframe/io/csv.py:815: UserWarning: Appending data to a network storage system may not work.\n  warn(\"Appending data to a network storage system may not work.\")\n"
     ]
    }
   ],
   "source": [
    "# Now imagine we want to store the sampled data.\n",
    "connector.write_file(data, 's3://ydata-dev-connectors/write_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/francisco/anaconda3/envs/ydata_tmp/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  \n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can write a new Dataframe \n",
    "from pandas.util.testing import makeDataFrame\n",
    "dummy_df = makeDataFrame()\n",
    "connector.write_file(dummy_df, 's3://ydata-dev-connectors/write_dummy.parquet', write_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the new dataset to ensure is working well\n",
    "dummy_data = connector.read_file('s3://ydata-dev-connectors/write_dummy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   A         B         C         D\n",
       "1Y2Jcj2XWT  0.839621 -0.808945  0.178300 -1.037180\n",
       "X3uCdC3SLn -0.929310 -0.941703  0.295265 -1.352673\n",
       "GlbjQ8e0MG  0.344733 -0.226315 -1.437262 -1.722771\n",
       "6tEZjlLKKm -1.943021 -0.469168 -0.462391  0.240716\n",
       "L4bl8COz8z -1.201750 -1.032017 -0.741219 -0.709047"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1Y2Jcj2XWT</th>\n      <td>0.839621</td>\n      <td>-0.808945</td>\n      <td>0.178300</td>\n      <td>-1.037180</td>\n    </tr>\n    <tr>\n      <th>X3uCdC3SLn</th>\n      <td>-0.929310</td>\n      <td>-0.941703</td>\n      <td>0.295265</td>\n      <td>-1.352673</td>\n    </tr>\n    <tr>\n      <th>GlbjQ8e0MG</th>\n      <td>0.344733</td>\n      <td>-0.226315</td>\n      <td>-1.437262</td>\n      <td>-1.722771</td>\n    </tr>\n    <tr>\n      <th>6tEZjlLKKm</th>\n      <td>-1.943021</td>\n      <td>-0.469168</td>\n      <td>-0.462391</td>\n      <td>0.240716</td>\n    </tr>\n    <tr>\n      <th>L4bl8COz8z</th>\n      <td>-1.201750</td>\n      <td>-1.032017</td>\n      <td>-0.741219</td>\n      <td>-0.709047</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# This is a sample from the new dataset's original data\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   A         B         C         D\n",
       "1Y2Jcj2XWT  0.839621 -0.808945  0.178300 -1.037180\n",
       "6tEZjlLKKm -1.943021 -0.469168 -0.462391  0.240716\n",
       "9COfBctvk6  0.744983  1.819461 -0.492183  1.054205\n",
       "Aocpqx7uLD  1.565838  0.278864  1.202742 -0.394791\n",
       "GlbjQ8e0MG  0.344733 -0.226315 -1.437262 -1.722771"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1Y2Jcj2XWT</th>\n      <td>0.839621</td>\n      <td>-0.808945</td>\n      <td>0.178300</td>\n      <td>-1.037180</td>\n    </tr>\n    <tr>\n      <th>6tEZjlLKKm</th>\n      <td>-1.943021</td>\n      <td>-0.469168</td>\n      <td>-0.462391</td>\n      <td>0.240716</td>\n    </tr>\n    <tr>\n      <th>9COfBctvk6</th>\n      <td>0.744983</td>\n      <td>1.819461</td>\n      <td>-0.492183</td>\n      <td>1.054205</td>\n    </tr>\n    <tr>\n      <th>Aocpqx7uLD</th>\n      <td>1.565838</td>\n      <td>0.278864</td>\n      <td>1.202742</td>\n      <td>-0.394791</td>\n    </tr>\n    <tr>\n      <th>GlbjQ8e0MG</th>\n      <td>0.344733</td>\n      <td>-0.226315</td>\n      <td>-1.437262</td>\n      <td>-1.722771</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# This is a sample from our \"stored-to-parquet-and-loaded\" data\n",
    "# The order of the rows may not match the original, given parallel-based way of reading and writing data.\n",
    "dummy_data.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All rows equal all columns in both datasets: True.\n"
     ]
    }
   ],
   "source": [
    "# But both datasets do match!\n",
    "print(f'All rows equal all columns in both datasets: {dummy_data.to_pandas().eq(dummy_df, axis=1).all(None)}.')"
   ]
  },
  {
   "source": [
    "## Advanced Features\n",
    "Connectors provided developer utilities that enable Data Scientists to navigate S3 Storage via code blocks.\n",
    "\n",
    "* Check if a bucket exists\n",
    "* List the contents of a bucket"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# We can check if a certain bucket exists\n",
    "connector.check_bucket('ydata-demos'), connector.check_bucket('fake-ydata-bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'keys': [('Synthetic Data_2.png', 15630),\n",
       "  ('teste.csv', 946384),\n",
       "  ('teste.parquet', 202335)],\n",
       " 'prefixes': ['syntheticdata']}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# We can check the contents of a certain bucket\n",
    "# Seems that we have 3 files (i.e. keys) and 1 folder (i.e. prefix)\n",
    "connector.list(bucket_name='ydata-demos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'keys': [('index.json', 2025)],\n",
       " 'prefixes': ['airbnb_newyork',\n",
       "  'cardiovascular_disease',\n",
       "  'census',\n",
       "  'creditcard_fraud',\n",
       "  'movie_lens']}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# We can check the contents of the prefix\n",
    "# We only have 1 key now, but there are other prefixes we can explore\n",
    "connector.list('ydata-demos', prefix='syntheticdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('data.csv', 3811499),\n",
       " ('metadata.json', 4757),\n",
       " ('synthetic_data.csv', 232524)]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# We have 3 files available under /census\n",
    "connector.list('ydata-demos', prefix='syntheticdata/census')['keys']"
   ]
  }
 ]
}