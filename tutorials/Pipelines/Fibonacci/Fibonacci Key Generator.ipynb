{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline that Generates key based on Local Time and Fibonacci Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full README at:\n",
    "\n",
    "https://github.com/ydataai/academy/blob/master/tutorials/Pipelines/README.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The python packages that will be installed in the multiple pipeline steps\n",
    "pip_packages_to_install = [\n",
    "    \"datetime\"\n",
    "]\n",
    "\n",
    "# Kfp\n",
    "pipeline_name = \"Generates key based on Local Time and Fibonacci Sequences\"\n",
    "pipeline_description = \"Generates key based on Local Time and Fibonacci Sequences\"\n",
    "experiment_name = \"Fibonacci experiment\"\n",
    "run_name = \"Sample Run\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def token_generator() -> str:\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now()\n",
    "    token = now.strftime(\"%H%M\")\n",
    "    while len(token) < 4:\n",
    "        token = str(0) + str(token)\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_token_generator = kfp.components.func_to_container_op(\n",
    "    func=token_generator,\n",
    "    output_component_file=\"./tok_gen.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Key Generated based on the Token Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_gen(tok: str) -> NamedTuple(\"S_outs\", [(\"outkey\", int), (\"token\", str)]):\n",
    "\n",
    "    tok = str(tok)\n",
    "\n",
    "    def fib_generate(a, s):\n",
    "        p = 0\n",
    "        q = s\n",
    "        r = s\n",
    "        for x in range(a - 1):\n",
    "            r = p + q\n",
    "            p = q\n",
    "            q = r\n",
    "        return r\n",
    "\n",
    "    out_key = fib_generate(int(str(tok[1]) + str(tok[3])), int(tok[0]) + 1)\n",
    "    print(\"Generated key {}\".format(out_key))\n",
    "    return (out_key, tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Decoder - Decoded key matches with Passed key only if Token is Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_decode(tok: str, seq_two: int) -> str:\n",
    "    def fib_generate(a, s):\n",
    "        p = 0\n",
    "        q = s\n",
    "        r = s\n",
    "        for x in range(a - 1):\n",
    "            r = p + q\n",
    "            p = q\n",
    "            q = r\n",
    "        return r\n",
    "\n",
    "    def fib_decode(f, s):\n",
    "        x = 0\n",
    "        y = fib_generate(0, s)\n",
    "        while f >= fib_generate(x, s):\n",
    "            y = fib_generate(x, s)\n",
    "            x += 1\n",
    "        return y\n",
    "\n",
    "    if fib_decode(seq_two, int(tok[0]) + 1) == seq_two:\n",
    "        print(\"Keys match\")\n",
    "        out_str = \"Keys Match\"\n",
    "    else:\n",
    "        print(\"Keys don't match\")\n",
    "        out_str = \"Keys Don't Match\"\n",
    "\n",
    "    return out_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_key_gen = kfp.components.func_to_container_op(\n",
    "    func=key_gen, output_component_file=\"./kgen.yaml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_key_decode = kfp.components.func_to_container_op(\n",
    "    func=key_decode, output_component_file=\"./kdecode.yaml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Generator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_generator_two() -> str:\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now()\n",
    "    token = now.strftime(\"%H%M\")\n",
    "    token = int(token) + 7\n",
    "    while len(str(token)) < 4:\n",
    "        token = str(0) + str(token)\n",
    "    return str(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_token_generator_two = kfp.components.func_to_container_op(\n",
    "    func=token_generator_two,\n",
    "    output_component_file=\"./tok_gen_two.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Fibonacci Security Key Generator\",\n",
    "    description=\"Generates automatically a random key from a token based on Fibonacci sequences\",\n",
    ")\n",
    "def Keygen_func():\n",
    "    # Passing pipeline parameter and a constant value as operation arguments\n",
    "    tg_one_task = kfp_token_generator()\n",
    "    kg_one_task = kfp_key_gen(tg_one_task.output)\n",
    "    kg_two_task = kfp_key_gen(tg_one_task.output)\n",
    "    tg_two_task = kfp_token_generator_two()\n",
    "    kg_three_task = kfp_key_gen(tg_two_task.output)\n",
    "    kd_one_task = kfp_key_decode(\n",
    "        kg_one_task.outputs[\"token\"], kg_two_task.outputs[\"outkey\"]\n",
    "    )\n",
    "    kd_two_task = kfp_key_decode(\n",
    "        kg_two_task.outputs[\"token\"], kg_three_task.outputs[\"outkey\"]\n",
    "    )\n",
    "    kd_three_task = kfp_key_decode(\n",
    "        kg_three_task.outputs[\"token\"], kg_two_task.outputs[\"outkey\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "# For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = Keygen_func\n",
    "pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\n",
    "\n",
    "import kfp.compiler as comp\n",
    "\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from random import randrange\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "# check if pipeline already exists -> if not, create a new one\n",
    "filter = json.dumps(\n",
    "    {\n",
    "        \"predicates\": [\n",
    "            {\"key\": \"name\", \"op\": 1, \"string_value\": \"{}\".format(pipeline_name)}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pipeline = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if pipeline.pipelines is None:\n",
    "    print(\"Creating a new pipeline: \" + pipeline_name)\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(\n",
    "        pipeline_filename, name=pipeline_name, description=pipeline_description\n",
    "    )\n",
    "else:\n",
    "    print(\"Pipeline already exists: \" + pipeline_name)\n",
    "    pipeline = pipeline.pipelines[0]\n",
    "    \n",
    "pipeline_version = str(uuid.uuid4())\n",
    "\n",
    "print(\"Creating a new pipeline version: \" + pipeline_name + str(\" [\" + pipeline_version + \"]\"))\n",
    "client.pipeline_uploads.upload_pipeline_version(\n",
    "    pipeline_filename,\n",
    "    name=pipeline_name + str(\" [\" + pipeline_version + \"]\"),\n",
    "    pipelineid=pipeline.id,\n",
    ")\n",
    "\n",
    "# create a new experiment or use an existing one\n",
    "print(\"Creating a new experiment or use an existing one: \" + experiment_name)\n",
    "experiment = client.create_experiment(name=experiment_name)\n",
    "\n",
    "# create a new run with a random identifier\n",
    "run_random_id = str(randrange(1000))\n",
    "print(\"Creating a new run: \" + run_name + \" [\" + run_random_id + \"]\")\n",
    "new_run = client.run_pipeline(\n",
    "    experiment.id, run_name + \" [\" + run_random_id + \"]\", pipeline_id=pipeline.id\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}