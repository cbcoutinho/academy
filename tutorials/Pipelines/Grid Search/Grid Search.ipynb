{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search for Getting Best Parameters on all 3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data file from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read the Data file from GCS Bucket## Read Data\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(file_name: str, df_churn_op: OutputPath(), mlpipeline_ui_metadata: OutputPath()): \n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df_churn = pd.read_csv(file_name)\n",
    "    df_churn.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    df_disp = df_churn.iloc[0:5]\n",
    "    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n",
    " \n",
    "    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv', index=False)\n",
    "    \n",
    "    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\")\n",
    "    categorical_cols = [c for c in df_show.columns if df_show[c].dtype == 'object' or c == 'SeniorCitizen']\n",
    "\n",
    "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "    schema = [{'name':c, 'type': 'CATEGORY'if c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'table',\n",
    "          'storage': 'gcs',\n",
    "          'format': 'csv',\n",
    "          'header': [x['name'] for x in schema],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.func_to_container_op(func = read_data, \n",
    "                                                          output_component_file = './read-data-func.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(df_churn_ip: InputPath(), df_one_hot: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    empty_cols = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "    \n",
    "    #Replacing Empty values\n",
    "    for i in empty_cols:\n",
    "        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n",
    "\n",
    "    df_churn.drop(['customerID'], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n",
    "    \n",
    "    #Binary Encoding\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    #Encoding column 'gender'\n",
    "    df_churn['gender'] = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n",
    "\n",
    "\n",
    "    category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n",
    "                   'OnlineBackup','DeviceProtection',\n",
    "                   'TechSupport','StreamingTV','StreamingMovies','Contract']\n",
    "    \n",
    "    #One-hot Encoding of multiple-category columns\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "    \n",
    "    df_churn_targets = df_churn['Churn'].unique()\n",
    "    df_churn['Churn'] = df_churn['Churn'].replace({\"Yes\":1,\"No\":0})\n",
    "    \n",
    "    #Output the encoded file \n",
    "    df_churn.to_csv(df_one_hot, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_one_hot_encode = kfp.components.func_to_container_op(func = one_hot_encode, \n",
    "                                                          output_component_file = './one-hot-encode-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'imbalanced-learn==0.6.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Column Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def data_cleaning(df_churn_ip: InputPath(), df_cleaned: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "    df.drop(['customerID'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    categorical_cols = [c for c in df.columns if df[c].dtype == 'object'\n",
    "                        or c == 'SeniorCitizen']\n",
    "    df_categorical = df[categorical_cols].copy()\n",
    "    for col in categorical_cols:\n",
    "        if df_categorical[col].nunique() == 2:\n",
    "            df_categorical[col], _ = pd.factorize(df_categorical[col])\n",
    "        else:\n",
    "            df_categorical = pd.get_dummies(df_categorical, columns=[col])\n",
    "\n",
    "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    df_std = pd.DataFrame(StandardScaler().fit_transform(df[numerical_cols].astype('float64')),\n",
    "                           columns=numerical_cols)\n",
    "\n",
    "    df_processed = pd.concat([df_std, df_categorical], axis=1)\n",
    "\n",
    "    # Remove Gender\n",
    "    features = ['gender']\n",
    "    df_processed.drop(features, axis=1, inplace=True)\n",
    "\n",
    "    # Remove services with 'no internet' label\n",
    "    features = ['OnlineSecurity_No internet service', 'OnlineBackup_No internet service',\n",
    "               'DeviceProtection_No internet service', 'TechSupport_No internet service',\n",
    "               'StreamingTV_No internet service', 'StreamingMovies_No internet service']\n",
    "    df_processed.drop(features, axis=1, inplace=True)\n",
    "\n",
    "    # Additional services 'No'\n",
    "    features = ['OnlineSecurity_No', 'OnlineBackup_No',\n",
    "               'DeviceProtection_No', 'TechSupport_No',\n",
    "               'StreamingTV_No', 'StreamingMovies_No']\n",
    "    df_processed.drop(features, axis=1, inplace=True)\n",
    "\n",
    "    # Remove PhoneService as MultipleLines has a 'No phone service' label\n",
    "    features = ['PhoneService']\n",
    "    df_processed.drop(features, axis=1, inplace=True)   \n",
    "    \n",
    "    df_processed.to_csv(df_cleaned, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_data_cleaning = kfp.components.func_to_container_op(func = data_cleaning, \n",
    "                                                          output_component_file = './data_cleaning.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def gridsearch(df_churn_ip: InputPath(), model: str, parameters: dict) -> dict:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    if(model=='RandomForest'):\n",
    "        clf = RandomForestClassifier()\n",
    "        \n",
    "    elif(model=='XGBoost'):\n",
    "        clf = xgb.XGBClassifier()\n",
    "        \n",
    "    elif(model=='XGBoostRF'):\n",
    "        clf = xgb.XGBRFClassifier()\n",
    "\n",
    "    gscv = GridSearchCV(estimator=clf, param_grid=parameters, cv = 4, n_jobs = -1, verbose = 0)\n",
    "    gscv.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = gscv.best_params_\n",
    "\n",
    "    print('Best Parameters: {}\\n'.format(best_params))\n",
    "    best_grid = gscv.best_estimator_\n",
    "    print('Train Score: {}\\n'.format(gscv.best_score_))\n",
    "    \n",
    "    test_accuracy = gscv.score(X_test, y_test)\n",
    "    print('Test score: {}'.format(test_accuracy))\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_gridsearch = kfp.components.func_to_container_op(func = gridsearch, \n",
    "                                                          output_component_file = './gridsearch.yaml', \n",
    "                                                   packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', \n",
    "                                                                          'xgboost==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "\n",
    "def rf_model(df_churn_ip: InputPath(), parameters: dict,\n",
    "              conf_matr: OutputPath(),\n",
    "              mlpipeline_ui_metadata: OutputPath(), mlpipeline_metrics: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "    import json\n",
    "    import os\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "    \n",
    "    #Split Data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    #fit the model on the Data and train it\n",
    "    rfc_best = RandomForestClassifier(**parameters)\n",
    "\n",
    "    rfc_best.fit(X_train, y_train) \n",
    "    y_test_pred = rfc_best.predict(X_test)\n",
    "    y_test_proba = rfc_best.predict_proba(X_test)[:,0]\n",
    "    \n",
    "    #Get Metrics scores\n",
    "    \n",
    "    rf_score = float('%.4f' %rfc_best.score(X_test, y_test))   \n",
    "    rf_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n",
    "    rf_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n",
    "    rf_f1 = float('%.4f' %f1_score(y_test, y_test_pred))\n",
    "    \n",
    "    print(\"Accuraccy, Precision, Recall, f1: \")\n",
    "    print(rf_score, rf_precision, rf_recall, rf_f1)\n",
    "    \n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "    \n",
    "    #True and False Positive Rates\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba) \n",
    "    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n",
    "    print('Auc score: ')\n",
    "    print(auc_score)\n",
    "    \n",
    "    #Converting the Confusion matrix to a Dataframe\n",
    "    #Note that for Generating the Confusion Matrix Artifact, the Confusion Matrix has to be converted from\n",
    "    #a numpy array to a DataFrame in the exact format as given below\n",
    "    \n",
    "    flags = {0:'Not Churned',1:'Churned'}\n",
    "    flag_list = ['Not Churned','Churned']\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    print(df_cm)\n",
    "    \n",
    "    \n",
    "    with file_io.FileIO(conf_matr, 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "    \n",
    "    #Save confusion Matrix to GCS\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Conf_matRF.csv', 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "    \n",
    "    \n",
    "    #roc curve\n",
    "    #For generating the ROC curve, the tpr, fpr and thresholds need to be output as a DataFrame in the \n",
    "    #exact format as given below\n",
    "    \n",
    "    df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv', 'w') as f:\n",
    "        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n",
    "\n",
    "    \n",
    "    #code to generate artifacts\n",
    "    \n",
    "    #Artifact generator - metadata\n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type': 'confusion_matrix',\n",
    "            'format': 'csv',\n",
    "            'storage': 'gcs',\n",
    "            'schema': [   #schema is required in the exact same form for generating the artifact\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matRF.csv', #conf_matr\n",
    "            \n",
    "       # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "            'labels': flag_list\n",
    "        },    \n",
    "        {\n",
    "          'type': 'roc',\n",
    "          'format': 'csv',\n",
    "          'storage': 'gcs',\n",
    "          'schema': [  #schema is required in the exact same form for generating the artifact\n",
    "            {'name': 'fpr', 'type': 'NUMBER'},\n",
    "            {'name': 'tpr', 'type': 'NUMBER'},\n",
    "            {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "          ],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #Output the file to the container-level root with the exact same name as below \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    \n",
    "    #Also output to Minio \n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    \n",
    "    #The metric scores can output as Pipeline Metrics by generating a json file as below\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) \n",
    "                            # and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'precision-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_precision, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'recall', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_recall, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'f1-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_f1, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'auc-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  auc_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "    }\n",
    "    \n",
    "    #Dump the metrics json file with the exact same name to the container-root directory\n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    #Also dump it to the Minio file storage\n",
    "    with file_io.FileIO(mlpipeline_metrics, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_rf_model = kfp.components.func_to_container_op(func = rf_model, \n",
    "                                                          output_component_file = './rf-model-func.yaml', \n",
    "                                                   packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', \n",
    "                                                                          'gcsfs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def xgb_model(df_churn_ip: InputPath(), \n",
    "              parameters: dict, \n",
    "              conf_matr: OutputPath(),\n",
    "              mlpipeline_ui_metadata: OutputPath(), mlpipeline_metrics: OutputPath()):\n",
    "        \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "    import json\n",
    "    import os\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    clfxg = xgb.XGBClassifier(**parameters)\n",
    "    clfxg.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = clfxg.predict(X_test)\n",
    "    \n",
    "    y_test_proba = clfxg.predict_proba(X_test)[:,0]\n",
    "    \n",
    "    xgb_score = float('%.4f' %accuracy_score(y_test, y_test_pred))   \n",
    "    xgb_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n",
    "    xgb_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n",
    "    xgb_f1 = float('%.4f' %f1_score(y_test, y_test_pred))\n",
    "    \n",
    "    print(\"Accuracy, Precision, Recall, f1: \")\n",
    "    print(xgb_score, xgb_precision, xgb_recall, xgb_f1)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba) \n",
    "    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n",
    "    print('Auc score: ')\n",
    "    print(auc_score)\n",
    "    \n",
    "    #Converting the matrix to a Dataframe\n",
    "    \n",
    "    flags = {0:'Not Churned',1:'Churned'}\n",
    "    flag_list = ['Not Churned','Churned']\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    print(df_cm)\n",
    "    \n",
    "    with file_io.FileIO(conf_matr, 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "        \n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBConf_mat.csv', 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "    \n",
    "    \n",
    "    #roc curve\n",
    "\n",
    "    df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBROC_curve.csv', 'w') as f:\n",
    "        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n",
    "\n",
    "    \n",
    "    #code to generate artifacts\n",
    "    \n",
    "    #Artifact generator - metadata\n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type': 'confusion_matrix',\n",
    "            'format': 'csv',\n",
    "            'storage': 'gcs',\n",
    "            'schema': [\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'source': 'gs://pipelines_artifacts/Artifacts/XGBConf_mat.csv', #conf_matr\n",
    "            \n",
    "       # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "            'labels': flag_list\n",
    "        },    \n",
    "        {\n",
    "          'type': 'roc',\n",
    "          'format': 'csv',\n",
    "          'storage': 'gcs',\n",
    "          'schema': [\n",
    "            {'name': 'fpr', 'type': 'NUMBER'},\n",
    "            {'name': 'tpr', 'type': 'NUMBER'},\n",
    "            {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "          ],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/XGBROC_curve.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'precision-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_precision, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'recall', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_recall, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'f1-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_f1, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'auc-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  auc_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        \n",
    "    with file_io.FileIO(mlpipeline_metrics, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_xgb_model = kfp.components.func_to_container_op(func = xgb_model, \n",
    "                                                          output_component_file = './xgb-model-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', \n",
    "                                                                                 'xgboost==1.0.2', 'gcsfs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import numpy as np\n",
    "\n",
    "@dsl.pipeline(name='Grid Search Pipeline',description='Run Grid-search on all 3 models for best input Parameters')\n",
    "def GridSearch_func(file_path = \"gs://pipelines_artifacts/Artifacts/Data.csv\"):\n",
    "    \n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    read_data_task = kfp_read_data(file_name = file_path) \n",
    "    \n",
    "    ohe_task = kfp_one_hot_encode(df_churn_ip = read_data_task.outputs['df_churn_op'])\n",
    "    \n",
    "    grid_search_rf_task = kfp_gridsearch(ohe_task.outputs['df_one_hot'], 'RandomForest', \n",
    "                           {'max_depth': [10, 30, 100],\n",
    "                            'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5], 'criterion': ['gini'],\n",
    "                            'min_samples_split': [2, 3, 4], 'n_estimators': [100, 200, 300, 1000]})\n",
    "    grid_search_xgb_task = kfp_gridsearch(ohe_task.outputs['df_one_hot'], 'XGBoost', \n",
    "                           {'n_estimators': [100, 200, 300, 1000], 'verbosity': [0], 'max_depth': [10, 30, 100], \n",
    "                            'eta': [1], 'silent': [0]})\n",
    "    grid_search_xgbrf_task = kfp_gridsearch(ohe_task.outputs['df_one_hot'], 'XGBoostRF', \n",
    "                           {'n_estimators': [100, 200, 300, 1000], 'verbosity': [0], 'max_depth': [10, 30, 100], \n",
    "                            'eta': [1], 'silent': [0]})\n",
    "    rf_model_task = kfp_rf_model(ohe_task.outputs['df_one_hot'], grid_search_rf_task.output)\n",
    "    xgb_model_task = kfp_xgb_model(ohe_task.outputs['df_one_hot'], grid_search_xgb_task.output)\n",
    "\n",
    "#For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "#For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_func = GridSearch_func\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
