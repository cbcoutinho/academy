"apiVersion": |-
  argoproj.io/v1alpha1
"kind": |-
  Workflow
"metadata":
  "annotations":
    "pipelines.kubeflow.org/pipeline_spec": |-
      {"description": "Churn predictions using XGBoost Algorithm", "inputs": [{"default": "gs://pipelines_artifacts/Artifacts/Data.csv", "name": "file_path", "optional": true}, {"default": "1.38", "name": "C", "optional": true}, {"default": "l2", "name": "penalty", "optional": true}, {"default": "0.0001", "name": "tol", "optional": true}, {"default": "100", "name": "max_iter", "optional": true}, {"default": "lbfgs", "name": "solver", "optional": true}], "name": "Telco Merchant Churn Prediction Pipeline"}
  "generateName": |-
    telco-merchant-churn-prediction-pipeline-
"spec":
  "arguments":
    "parameters":
    - "name": |-
        file_path
      "value": |-
        gs://pipelines_artifacts/Artifacts/Data.csv
    - "name": |-
        C
      "value": |-
        1.38
    - "name": |-
        penalty
      "value": |-
        l2
    - "name": |-
        tol
      "value": |-
        0.0001
    - "name": |-
        max_iter
      "value": |-
        100
    - "name": |-
        solver
      "value": |-
        lbfgs
  "entrypoint": |-
    telco-merchant-churn-prediction-pipeline
  "serviceAccountName": |-
    pipeline-runner
  "templates":
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef categorical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n    from sklearn.ensemble import RandomForestClassifier\n\n    import\
        \ warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,\
        \ index=False)\n\n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    #Categorical Analysis\n\n    def barplot_percentages(feature,\
        \ orient='v', axis_name=\"percentage of customers\"):\n        ratios = pd.DataFrame()\n\
        \        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n  \
        \      g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n      \
        \  g[axis_name] = g[axis_name]/len(df)\n        if orient == 'v':\n      \
        \      ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient,\
        \ palette = 'bright')\n            ax.set_yticklabels(['{:,.0%}'.format(y)\
        \ for y in ax.get_yticks()])\n        else:\n            ax = sns.barplot(x=\
        \ axis_name, y=feature, hue='Churn', data=g, orient=orient, palette = 'bright')\n\
        \            ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n\
        \        ax.plot()\n\n    #Paperless Billing\n\n    df['churn_rate'] = df['Churn'].replace(\"\
        No\", 0).replace(\"Yes\", 1)\n    g = sns.FacetGrid(df, col=\"PaperlessBilling\"\
        , height=4, aspect=.9)\n    ax = g.map(sns.barplot, \"Contract\", \"churn_rate\"\
        , palette = \"bright\", order= ['Month-to-month', 'One year', 'Two year'])\n\
        \n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Paperless_billing.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Payments Method\n\n    plt.figure(figsize=(9,\
        \ 4.5))\n    barplot_percentages(\"PaymentMethod\", orient='v')\n\n    fig\
        \ = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Payments_method.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Generating Metadata\n    metadata\
        \ = {\n        'version' : 1, \n        'outputs' : [{        \n         \
        \ 'type': 'web-app',\n          'storage': 'gcs',\n          'source': \"\
        gs://pipelines_artifacts/Artifacts/Graphs/Paperless_billing.html\",\n    \
        \    },\n            {        \n          'type': 'web-app',\n          'storage':\
        \ 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Payments_method.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Categorical analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = categorical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Categorical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      categorical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-imputed
      - |-
        /tmp/inputs/df_churn_imputed/data
      - |-
        --df-cleaned
      - |-
        /tmp/outputs/df_cleaned/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef data_cleaning(df_churn_ip , df_churn_imputed , df_cleaned ):\n\n   \
        \ import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing\
        \ import StandardScaler\n\n    df = pd.read_csv(df_churn_ip)\n    df_churn_imp\
        \ = pd.read_csv(df_churn_imputed)\n\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n    df.drop(['customerID'],axis=1, inplace=True)\n\
        \n    categorical_cols = [c for c in df.columns if df[c].dtype == 'object'\n\
        \                        or c == 'SeniorCitizen']\n    df_categorical = df[categorical_cols].copy()\n\
        \    for col in categorical_cols:\n        if df_categorical[col].nunique()\
        \ == 2:\n            df_categorical[col], _ = pd.factorize(df_categorical[col])\n\
        \        else:\n            df_categorical = pd.get_dummies(df_categorical,\
        \ columns=[col])\n\n    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n\
        \    df_std = pd.DataFrame(StandardScaler().fit_transform(df[numerical_cols].astype('float64')),\n\
        \                           columns=numerical_cols)\n\n    df_processed =\
        \ pd.concat([df_std, df_categorical], axis=1)\n\n    # Remove Gender\n   \
        \ features = ['gender']\n    df_processed.drop(features, axis=1, inplace=True)\n\
        \n    # Remove services with 'no internet' label\n    features = ['OnlineSecurity_No\
        \ internet service', 'OnlineBackup_No internet service',\n               'DeviceProtection_No\
        \ internet service', 'TechSupport_No internet service',\n               'StreamingTV_No\
        \ internet service', 'StreamingMovies_No internet service']\n    df_processed.drop(features,\
        \ axis=1, inplace=True)\n\n    # Additional services 'No'\n    features =\
        \ ['OnlineSecurity_No', 'OnlineBackup_No',\n               'DeviceProtection_No',\
        \ 'TechSupport_No',\n               'StreamingTV_No', 'StreamingMovies_No']\n\
        \    df_processed.drop(features, axis=1, inplace=True)\n\n    # Remove PhoneService\
        \ as MultipleLines has a 'No phone service' label\n    features = ['PhoneService']\n\
        \    df_processed.drop(features, axis=1, inplace=True)   \n\n    df_processed.to_csv(df_cleaned,\
        \ index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Data\
        \ cleaning', description='')\n_parser.add_argument(\"--df-churn-ip\", dest=\"\
        df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --df-churn-imputed\", dest=\"df_churn_imputed\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-cleaned\", dest=\"\
        df_cleaned\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = data_cleaning(**_parsed_args)\n\n_output_serializers\
        \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_imputed/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_imputed"}], "name": "Data cleaning", "outputs": [{"name": "df_cleaned"}]}
    "name": |-
      data-cleaning
    "outputs":
      "artifacts":
      - "name": |-
          data-cleaning-df_cleaned
        "path": |-
          /tmp/outputs/df_cleaned/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --C
      - |-
        {{inputs.parameters.C}}
      - |-
        --penalty
      - |-
        {{inputs.parameters.penalty}}
      - |-
        --tol
      - |-
        {{inputs.parameters.tol}}
      - |-
        --max-iter
      - |-
        {{inputs.parameters.max_iter}}
      - |-
        --solver
      - |-
        {{inputs.parameters.solver}}
      - |-
        --conf-matr
      - |-
        /tmp/outputs/conf_matr/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --mlpipeline-metrics
      - |-
        /tmp/outputs/mlpipeline_metrics/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef lgr_model(df_churn_ip , \n              C , penalty, tol , max_iter\
        \ , solver ,\n              conf_matr ,\n              mlpipeline_ui_metadata\
        \ , mlpipeline_metrics ):\n\n    import pandas as pd\n    import numpy as\
        \ np\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model\
        \ import LogisticRegression\n    from sklearn.preprocessing import StandardScaler\n\
        \    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score,\
        \ roc_curve, precision_score, recall_score, f1_score\n    import json\n  \
        \  import os\n    import gcsfs\n    from tensorflow.python.lib.io import file_io\n\
        \n    df_churn = pd.read_csv(df_churn_ip)\n    df_churn.dropna(inplace=True)\n\
        \n    cval = C\n    pen = penalty\n    toler = tol\n    max_it = max_iter\n\
        \    solv = solver \n\n    y1 = df_churn['Churn']\n    X1 = df_churn.drop(['Churn'],axis=1)\n\
        \n    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n\
        \n    clf = LogisticRegression(random_state=0, C=cval, penalty=pen, tol=toler,\
        \ \n                             max_iter=max_it, solver=solv).fit(X_train,\
        \ y_train)\n\n    y_test_pred = clf.predict(X_test)\n    y_test_proba = clf.predict_proba(X_test)[:,0]\n\
        \n    rf_score = float('%.4f' %accuracy_score(y_test, y_test_pred))   \n \
        \   rf_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n \
        \   rf_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n    rf_f1\
        \ = float('%.4f' %f1_score(y_test, y_test_pred))\n\n    print(\"Accuraccy,\
        \ Precision, Recall, f1: \")\n    print(rf_score, rf_precision, rf_recall,\
        \ rf_f1)\n\n    cm = confusion_matrix(y_test, y_test_pred)\n    print(\"Confusion\
        \ Matrix: {}\".format(cm))\n\n    fpr, tpr, thresholds = roc_curve(y_test,\
        \ y_test_proba) \n    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n\
        \    print('Auc score: ')\n    print(auc_score)\n\n    #Converting the matrix\
        \ to a Dataframe\n\n    flags = {0:'Not Churned',1:'Churned'}\n    flag_list\
        \ = ['Not Churned','Churned']\n    data = []\n    for target_index, target_row\
        \ in enumerate(cm):\n        for predicted_index, count in enumerate(target_row):\n\
        \            data.append((flags[target_index], flags[predicted_index], count))\n\
        \n    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n\
        \    print(df_cm)\n\n    with file_io.FileIO(conf_matr, 'w') as f:\n     \
        \   df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False,\
        \ index=False)\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token\
        \ = 'cloud')\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Conf_matLGR.csv',\
        \ 'w') as f:\n        df_cm.to_csv(f, columns=['target', 'predicted', 'count'],\
        \ header=False, index=False)\n\n    #roc curve\n\n    df_roc = pd.DataFrame({'fpr':\
        \ fpr, 'tpr': tpr, 'thresholds': thresholds})\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/ROC_curveLGR.csv',\
        \ 'w') as f:\n        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'],\
        \ header=False, index=False)\n\n    #code to generate artifacts\n\n    #Artifact\
        \ generator - metadata\n\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    metadata = {\n        'version' : 1, \n        'outputs'\
        \ : [{\n            'type': 'confusion_matrix',\n            'format': 'csv',\n\
        \            'storage': 'gcs',\n            'schema': [\n                {'name':\
        \ 'target', 'type': 'CATEGORY'},\n                {'name': 'predicted', 'type':\
        \ 'CATEGORY'},\n                {'name': 'count', 'type': 'NUMBER'},\n   \
        \         ],\n            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matLGR.csv',\
        \ #conf_matr\n\n       # Convert flags to string because for bealean values\
        \ we want \"True|False\" to match csv data.\n            'labels': flag_list\n\
        \        },    \n        {\n          'type': 'roc',\n          'format':\
        \ 'csv',\n          'storage': 'gcs',\n          'schema': [\n           \
        \ {'name': 'fpr', 'type': 'NUMBER'},\n            {'name': 'tpr', 'type':\
        \ 'NUMBER'},\n            {'name': 'thresholds', 'type': 'NUMBER'},\n    \
        \      ],\n          'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveLGR.csv'\n\
        \        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    metrics = {\n    'metrics':\
        \ [{\n      'name': 'accuracy-score', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  rf_score, #\
        \ The value of the metric. Must be a numeric value.\n      'format': \"RAW\"\
        ,   # The optional format of the metric. Supported values are \"RAW\" (displayed\
        \ in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n  \
        \  },\n    {\n      'name': 'precision-score', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  rf_precision,\
        \ # The value of the metric. Must be a numeric value.\n      'format': \"\
        RAW\",   # The optional format of the metric. Supported values are \"RAW\"\
        \ (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n\
        \    },\n    {\n      'name': 'recall', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  rf_recall,\
        \ # The value of the metric. Must be a numeric value.\n      'format': \"\
        RAW\",   # The optional format of the metric. Supported values are \"RAW\"\
        \ (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n\
        \    },\n    {\n      'name': 'f1-score', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  rf_f1, # The\
        \ value of the metric. Must be a numeric value.\n      'format': \"RAW\",\
        \   # The optional format of the metric. Supported values are \"RAW\" (displayed\
        \ in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n  \
        \  },\n    {\n      'name': 'auc-score', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  auc_score,\
        \ # The value of the metric. Must be a numeric value.\n      'format': \"\
        RAW\",   # The optional format of the metric. Supported values are \"RAW\"\
        \ (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n\
        \    }]\n    }\n\n    with file_io.FileIO('/mlpipeline-metrics.json', 'w')\
        \ as f:\n        json.dump(metrics, f)\n\n    with file_io.FileIO(mlpipeline_metrics,\
        \ 'w') as f:\n        json.dump(metrics, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Lgr model', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--C\", dest=\"C\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--penalty\", dest=\"penalty\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--tol\", dest=\"tol\"\
        , type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --max-iter\", dest=\"max_iter\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--solver\", dest=\"solver\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--conf-matr\", dest=\"\
        conf_matr\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-metrics\", dest=\"mlpipeline_metrics\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = lgr_model(**_parsed_args)\n\n_output_serializers\
        \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          data-cleaning-df_cleaned
        "path": |-
          /tmp/inputs/df_churn_ip/data
      "parameters":
      - "name": |-
          C
      - "name": |-
          max_iter
      - "name": |-
          penalty
      - "name": |-
          solver
      - "name": |-
          tol
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "C", "type": "Float"}, {"name": "penalty", "type": "String"}, {"name": "tol", "type": "Float"}, {"name": "max_iter", "type": "Integer"}, {"name": "solver", "type": "String"}], "name": "Lgr model", "outputs": [{"name": "conf_matr"}, {"name": "mlpipeline_ui_metadata"}, {"name": "mlpipeline_metrics"}]}
    "name": |-
      lgr-model
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mlpipeline-metrics
        "path": |-
          /tmp/outputs/mlpipeline_metrics/data
      - "name": |-
          lgr-model-conf_matr
        "path": |-
          /tmp/outputs/conf_matr/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-ip2
      - |-
        /tmp/inputs/df_churn_ip2/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef mixed_analysis(df_churn_ip , df_churn_ip2 , \n                   mlpipeline_ui_metadata\
        \ , df_churn_op ):\n\n    ## Import Required Libraries\n    import pandas\
        \ as pd\n    import numpy as np\n    import gcsfs\n    import seaborn as sns\n\
        \    import matplotlib.pyplot as plt\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df2 = pd.read_csv(df_churn_ip2)\n\
        \n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op, index=False)\n\
        \n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    df['total_charges_to_tenure_ratio'] = df['TotalCharges']\
        \ / df['tenure']\n    df['monthly_charges_diff'] = df['MonthlyCharges'] -\
        \ df['total_charges_to_tenure_ratio']\n    df['churn_rate'] = df['Churn'].replace(\"\
        No\", 0).replace(\"Yes\", 1)\n\n    #Correlation Heatmap\n\n    plt.figure(figsize=(12,\
        \ 12))\n    df.drop(['customerID', 'churn_rate', 'total_charges_to_tenure_ratio',\
        \ 'monthly_charges_diff'],\n            axis=1, inplace=True)\n    corr =\
        \ df.apply(lambda x: pd.factorize(x)[0]).corr()\n    ax = sns.heatmap(corr,\
        \ xticklabels=corr.columns, yticklabels=corr.columns, \n                 \
        \    linewidths=1, cmap=\"YlGnBu\", square=True)\n\n    fig = plt.gcf()\n\
        \    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/correlation_heatmap.html',\
        \ 'w') as f:\n        f.write(s)\n\n    metadata = {\n        'version' :\
        \ 1, \n        'outputs' : [{        \n          'type': 'web-app',\n    \
        \      'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/correlation_heatmap.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Mixed analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-ip2\", dest=\"df_churn_ip2\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\"\
        , dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = mixed_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip2/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_ip2"}], "name": "Mixed analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      mixed-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef numerical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n    from sklearn.ensemble import RandomForestClassifier\n\n    import\
        \ warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,\
        \ index=False)\n\n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    #kdeplots - tenure, Monthly Charges, Total\
        \ Charges\n\n    def kdeplot(feature):\n        fig = plt.figure(figsize=(9,\
        \ 4))\n        plt.title(\"KDE for {}\".format(feature))\n        ax0 = sns.kdeplot(df[df['Churn']\
        \ == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No')\n       \
        \ ax1 = sns.kdeplot(df[df['Churn'] == 'Yes'][feature].dropna(), color= 'orange',\
        \ label= 'Churn: Yes')\n\n        fig = plt.gcf()\n        s = mpld3.fig_to_html(fig)\n\
        \n        with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/{}.html'.format(feature),\
        \ 'w') as f:\n            f.write(s)\n\n    # Kdeplot - Monthly Charge Differential\n\
        \    df['total_charges_to_tenure_ratio'] = df['TotalCharges'] / df['tenure']\n\
        \    df['monthly_charges_diff'] = df['MonthlyCharges'] - df['total_charges_to_tenure_ratio']\n\
        \n    kdeplot('monthly_charges_diff')\n\n    #Monthly Charges vs Tenure for\
        \ SeniorCitizen\n\n    df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"\
        Yes\", 1)\n    g = sns.FacetGrid(df, row='SeniorCitizen', col=\"gender\",\
        \ hue=\"Churn\", height=3.5)\n    g.map(plt.scatter, \"tenure\", \"MonthlyCharges\"\
        , alpha=0.6)\n    g.add_legend()\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/scatterplot2.html',\
        \ 'w') as f:\n        f.write(s)    \n\n    metadata = {\n        'version'\
        \ : 1, \n        'outputs' : [{        \n          'type': 'web-app',\n  \
        \        'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/monthly_charges_diff.html\"\
        ,\n        },\n            {        \n          'type': 'web-app',\n     \
        \     'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/scatterplot2.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Numerical analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = numerical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Numerical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      numerical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --file-name
      - |-
        {{inputs.parameters.file_path}}
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef read_data(file_name , df_churn_op , mlpipeline_ui_metadata ): \n\n \
        \   ## Import Required Libraries\n    import pandas as pd\n    import numpy\
        \ as np\n    import gcsfs\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic',\
        \ token = 'cloud')\n\n    df_churn = pd.read_csv(file_name)\n    df_churn.to_csv(df_churn_op,\
        \ index=False)\n\n    df_disp = df_churn.iloc[0:5]\n    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n\
        \n    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv',\
        \ index=False)\n\n    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\"\
        )\n    categorical_cols = [c for c in df_show.columns if df_show[c].dtype\
        \ == 'object' or c == 'SeniorCitizen']\n\n    numerical_cols = ['tenure',\
        \ 'MonthlyCharges', 'TotalCharges']\n\n    schema = [{'name':c, 'type': 'CATEGORY'if\
        \ c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n\n    metadata\
        \ = {\n        'outputs' : [{\n          'type': 'table',\n          'storage':\
        \ 'gcs',\n          'format': 'csv',\n          'header': [x['name'] for x\
        \ in schema],\n          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n\
        \        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Read data', description='')\n_parser.add_argument(\"\
        --file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = read_data(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "parameters":
      - "name": |-
          file_path
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "file_name", "type": "String"}], "name": "Read data", "outputs": [{"name": "df_churn_op"}, {"name": "mlpipeline_ui_metadata"}]}
    "name": |-
      read-data
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "dag":
      "tasks":
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          read-data
        "name": |-
          categorical-analysis
        "template": |-
          categorical-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.numerical-analysis.outputs.artifacts.numerical-analysis-df_churn_op}}
            "name": |-
              numerical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          numerical-analysis
        - |-
          read-data
        "name": |-
          data-cleaning
        "template": |-
          data-cleaning
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.data-cleaning.outputs.artifacts.data-cleaning-df_cleaned}}
            "name": |-
              data-cleaning-df_cleaned
          "parameters":
          - "name": |-
              C
            "value": |-
              {{inputs.parameters.C}}
          - "name": |-
              max_iter
            "value": |-
              {{inputs.parameters.max_iter}}
          - "name": |-
              penalty
            "value": |-
              {{inputs.parameters.penalty}}
          - "name": |-
              solver
            "value": |-
              {{inputs.parameters.solver}}
          - "name": |-
              tol
            "value": |-
              {{inputs.parameters.tol}}
        "dependencies":
        - |-
          data-cleaning
        "name": |-
          lgr-model
        "template": |-
          lgr-model
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.categorical-analysis.outputs.artifacts.categorical-analysis-df_churn_op}}
            "name": |-
              categorical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          categorical-analysis
        - |-
          read-data
        "name": |-
          mixed-analysis
        "template": |-
          mixed-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.mixed-analysis.outputs.artifacts.mixed-analysis-df_churn_op}}
            "name": |-
              mixed-analysis-df_churn_op
        "dependencies":
        - |-
          mixed-analysis
        "name": |-
          numerical-analysis
        "template": |-
          numerical-analysis
      - "arguments":
          "parameters":
          - "name": |-
              file_path
            "value": |-
              {{inputs.parameters.file_path}}
        "name": |-
          read-data
        "template": |-
          read-data
    "inputs":
      "parameters":
      - "name": |-
          C
      - "name": |-
          file_path
      - "name": |-
          max_iter
      - "name": |-
          penalty
      - "name": |-
          solver
      - "name": |-
          tol
    "name": |-
      telco-merchant-churn-prediction-pipeline
