"apiVersion": |-
  argoproj.io/v1alpha1
"kind": |-
  Workflow
"metadata":
  "annotations":
    "pipelines.kubeflow.org/pipeline_spec": |-
      {"description": "Churn predictions using Random Forest Algorithm", "inputs": [{"default": "gs://pipelines_artifacts/Artifacts/Data.csv", "name": "file_path", "optional": true}, {"default": "100", "name": "n_estimators", "optional": true}, {"default": "8", "name": "max_depth", "optional": true}, {"default": "gini", "name": "criterion", "optional": true}, {"default": "auto", "name": "max_features", "optional": true}, {"default": "2", "name": "min_samples_split", "optional": true}], "name": "Telco Merchant Churn Prediction Pipeline"}
  "generateName": |-
    telco-merchant-churn-prediction-pipeline-
"spec":
  "arguments":
    "parameters":
    - "name": |-
        file_path
      "value": |-
        gs://pipelines_artifacts/Artifacts/Data.csv
    - "name": |-
        n_estimators
      "value": |-
        100
    - "name": |-
        max_depth
      "value": |-
        8
    - "name": |-
        criterion
      "value": |-
        gini
    - "name": |-
        max_features
      "value": |-
        auto
    - "name": |-
        min_samples_split
      "value": |-
        2
  "entrypoint": |-
    telco-merchant-churn-prediction-pipeline
  "serviceAccountName": |-
    pipeline-runner
  "templates":
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef categorical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n    from sklearn.ensemble import RandomForestClassifier\n\n    import\
        \ warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,\
        \ index=False)\n\n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    #Categorical Analysis\n\n    #Churn Plot\n\
        \n    ax = sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.0, aspect=3.0,\
        \ palette = 'bright',\n                     legend = True)\n\n    #Seaborn\
        \ Plots cannot be directly viewed on the YData Pipelines Dashboard\n    #The\
        \ plot must be converted to .html format, and uploaded to be accessed by the\
        \ Artifacts Generator\n\n    fig = plt.gcf()    #gcf gets the current figure\
        \ generated\n    s = mpld3.fig_to_html(fig)   #mpld3 is a Python library which\
        \ converts matplotlib/dericative library\n                               \
        \  #plots to html \n\n    #write the .html file to your storage bucket using\
        \ file_io\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html',\
        \ 'w') as f:\n        f.write(s)\n\n    def barplot_percentages(feature, orient='v',\
        \ axis_name=\"percentage of customers\"):\n        ratios = pd.DataFrame()\n\
        \        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n  \
        \      g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n      \
        \  g[axis_name] = g[axis_name]/len(df)\n        if orient == 'v':\n      \
        \      ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient,\
        \ palette = 'bright')\n            ax.set_yticklabels(['{:,.0%}'.format(y)\
        \ for y in ax.get_yticks()])\n        else:\n            ax = sns.barplot(x=\
        \ axis_name, y=feature, hue='Churn', data=g, orient=orient, palette = 'bright')\n\
        \            ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n\
        \        ax.plot()\n\n    #Partners and Dependents\n\n    fig, axis = plt.subplots(1,\
        \ 2, figsize=(12,4))\n    axis[0].set_title(\"Has partner\")\n    axis[1].set_title(\"\
        Has dependents\")\n    axis_y = \"percentage of customers\"\n    # Plot Partner\
        \ column\n    gp_partner = df.groupby('Partner')[\"Churn\"].value_counts()/len(df)\n\
        \    gp_partner = gp_partner.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\
        \    ax = sns.barplot(x='Partner', y= axis_y, hue='Churn', data=gp_partner,\
        \ ax=axis[0], palette = 'bright')\n    # Plot Dependents column\n    gp_dep\
        \ = df.groupby('Dependents')[\"Churn\"].value_counts()/len(df)\n    gp_dep\
        \ = gp_dep.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n\
        \    ax = sns.barplot(x='Dependents', y= axis_y, hue='Churn', data=gp_dep,\
        \ ax=axis[1], palette = 'bright')\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Partners_dependents.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #A metadata json file has to be generated\
        \ and dumped to the root level of the container for artifacts to\n    # be\
        \ generated. Metadata supports the following artifacts - \n    # Static HTML\n\
        \    # ROC Curve\n    # Confusion Matrix\n    # Tables\n    # Markdown\n\n\
        \    #Generating Metadata\n    metadata = {\n        'version' : 1,   #Check\
        \ the Version of your Kubeflow \n        'outputs' : [{\n            'type'\
        \ : 'web-app',\n            'storage' : 'gcs',\n            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html'\n\
        \            }, \n            {        \n          'type': 'web-app',\n  \
        \        'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Partners_dependents.html\"\
        ,\n        },\n        ]\n    }\n\n    # Output the metadata file, with the\
        \ exact same name as below to the root level of the Pipeline-block container\n\
        \    # While troubleshooting artifact issues, first check at the container\
        \ level if the file has been created properly\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    #Also output to Minio\
        \ as an Output file\n    with file_io.FileIO(mlpipeline_ui_metadata, 'w')\
        \ as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Categorical\
        \ analysis', description='')\n_parser.add_argument(\"--df-churn-ip\", dest=\"\
        df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\"\
        , dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = categorical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Categorical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      categorical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-ip2
      - |-
        /tmp/inputs/df_churn_ip2/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef mixed_analysis(df_churn_ip , df_churn_ip2 , \n                   mlpipeline_ui_metadata\
        \ , df_churn_op ):\n\n    ## Import Required Libraries\n    import pandas\
        \ as pd\n    import numpy as np\n    import gcsfs\n    import seaborn as sns\n\
        \    import matplotlib.pyplot as plt\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df2 = pd.read_csv(df_churn_ip2)\n\
        \n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op, index=False)\n\
        \n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    df['total_charges_to_tenure_ratio'] = df['TotalCharges']\
        \ / df['tenure']\n    df['monthly_charges_diff'] = df['MonthlyCharges'] -\
        \ df['total_charges_to_tenure_ratio']\n    df['churn_rate'] = df['Churn'].replace(\"\
        No\", 0).replace(\"Yes\", 1)\n\n    #Multiple-Lines vs Monthly Charges\n\n\
        \    ax = sns.catplot(x=\"MultipleLines\", y=\"MonthlyCharges\", hue=\"Churn\"\
        , kind=\"violin\",\n                     split=True, palette=\"pastel\", data=df,\
        \ height=4.2, aspect=1.4)\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/violinplot1.html',\
        \ 'w') as f:\n        f.write(s)\n\n    metadata = {\n        'version' :\
        \ 1, \n        'outputs' : [{\n            'type' : 'web-app',\n         \
        \   'storage' : 'gcs',\n            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/violinplot1.html'\n\
        \            }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Mixed analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-ip2\", dest=\"df_churn_ip2\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\"\
        , dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = mixed_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip2/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_ip2"}], "name": "Mixed analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      mixed-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef numerical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n\n    import warnings\n    warnings.simplefilter(action='ignore',\
        \ category=FutureWarning)\n    warnings.simplefilter(action='ignore', category=UserWarning)\n\
        \n    import mpld3\n    from tensorflow.python.lib.io import file_io\n   \
        \ import json\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token\
        \ = 'cloud')\n\n    df = pd.read_csv(df_churn_ip)\n\n    df1 = df.copy(deep=True)\n\
        \    df1.to_csv(df_churn_op, index=False)\n\n    sns.set(style=\"white\")\n\
        \    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n\
        \n    #kdeplots - tenure, Monthly Charges, Total Charges\n\n    def kdeplot(feature):\n\
        \        fig = plt.figure(figsize=(9, 4))\n        plt.title(\"KDE for {}\"\
        .format(feature))\n        ax0 = sns.kdeplot(df[df['Churn'] == 'No'][feature].dropna(),\
        \ color= 'navy', label= 'Churn: No')\n        ax1 = sns.kdeplot(df[df['Churn']\
        \ == 'Yes'][feature].dropna(), color= 'orange', label= 'Churn: Yes')\n\n \
        \       fig = plt.gcf()\n        s = mpld3.fig_to_html(fig)\n\n        with\
        \ file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/{}.html'.format(feature),\
        \ 'w') as f:\n            f.write(s)\n\n    kdeplot('tenure')\n\n    #scatterplot\
        \ - Monthly and Total Charges vs Tenure\n    fig = plt.figure(figsize=(9,\
        \ 4))\n    g = sns.PairGrid(df, y_vars=[\"tenure\"], x_vars=[\"MonthlyCharges\"\
        , \"TotalCharges\"], height=4.5, hue=\"Churn\", aspect=1.1)\n    ax = g.map(plt.scatter,\
        \ alpha=0.6)\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with\
        \ file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/scatterplot1.html',\
        \ 'w') as f:\n        f.write(s)    \n\n    metadata = {\n        'version'\
        \ : 1, \n        'outputs' : [{\n            'type' : 'web-app',\n       \
        \     'storage' : 'gcs',\n            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/tenure.html'\n\
        \            }, \n            {        \n          'type': 'web-app',\n  \
        \        'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/scatterplot1.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Numerical analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = numerical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Numerical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      numerical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-imputed
      - |-
        /tmp/inputs/df_churn_imputed/data
      - |-
        --df-one-hot
      - |-
        /tmp/outputs/df_one_hot/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'imbalanced-learn==0.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'imbalanced-learn==0.6.2' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef one_hot_encode(df_churn_ip , df_churn_imputed , df_one_hot ):\n\n  \
        \  import pandas as pd\n    import numpy as np\n\n    df_churn = pd.read_csv(df_churn_ip)\n\
        \    df_churn_imp = pd.read_csv(df_churn_imputed)\n    empty_cols = ['customerID',\
        \ 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n           'tenure',\
        \ 'PhoneService', 'MultipleLines', 'InternetService',\n           'OnlineSecurity',\
        \ 'OnlineBackup', 'DeviceProtection','TechSupport',\n           'StreamingTV',\
        \ 'StreamingMovies', 'Contract', 'PaperlessBilling',\n           'PaymentMethod',\
        \ 'MonthlyCharges', 'TotalCharges', 'Churn']\n\n    #Replacing Empty values\n\
        \    for i in empty_cols:\n        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n\
        \n    df_churn.drop(['customerID'], axis=1, inplace=True)\n    df_churn =\
        \ df_churn.dropna()\n    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n\
        \n    #Binary Encoding\n    for i in binary_cols:\n        df_churn[i] = df_churn[i].replace({\"\
        Yes\":1,\"No\":0})\n\n    #Encoding column 'gender'\n    df_churn['gender']\
        \ = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n\n    category_cols\
        \ = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n\
        \                   'OnlineBackup','DeviceProtection',\n                 \
        \  'TechSupport','StreamingTV','StreamingMovies','Contract']\n\n    #One-hot\
        \ Encoding of multiple-category columns\n    for cc in category_cols:\n  \
        \      dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n        dummies\
        \ = dummies.add_prefix(\"{}#\".format(cc))\n        df_churn.drop(cc, axis=1,\
        \ inplace=True)\n        df_churn = df_churn.join(dummies)\n\n    df_churn_targets\
        \ = df_churn['Churn'].unique()\n    df_churn['Churn'] = df_churn['Churn'].replace({\"\
        Yes\":1,\"No\":0})\n\n    #Output the encoded file \n    df_churn.to_csv(df_one_hot,\
        \ index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='One\
        \ hot encode', description='')\n_parser.add_argument(\"--df-churn-ip\", dest=\"\
        df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --df-churn-imputed\", dest=\"df_churn_imputed\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-one-hot\", dest=\"\
        df_one_hot\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = one_hot_encode(**_parsed_args)\n\n_output_serializers\
        \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_imputed/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_imputed"}], "name": "One hot encode", "outputs": [{"name": "df_one_hot"}]}
    "name": |-
      one-hot-encode
    "outputs":
      "artifacts":
      - "name": |-
          one-hot-encode-df_one_hot
        "path": |-
          /tmp/outputs/df_one_hot/data
  - "container":
      "args":
      - |-
        --file-name
      - |-
        {{inputs.parameters.file_path}}
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef read_data(file_name , df_churn_op , mlpipeline_ui_metadata ): \n\n \
        \   ## Import Required Libraries\n    import pandas as pd\n    import numpy\
        \ as np\n    import gcsfs\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic',\
        \ token = 'cloud')\n\n    df_churn = pd.read_csv(file_name)\n    df_churn.to_csv(df_churn_op,\
        \ index=False)\n\n    #A DataFrame too long cannot be displayed in the Artifacts\n\
        \n    df_disp = df_churn.iloc[0:5]\n    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n\
        \n    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv',\
        \ index=False)\n\n    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\"\
        )\n    categorical_cols = [c for c in df_show.columns if df_show[c].dtype\
        \ == 'object' or c == 'SeniorCitizen']\n\n    numerical_cols = ['tenure',\
        \ 'MonthlyCharges', 'TotalCharges']\n\n    schema = [{'name':c, 'type': 'CATEGORY'if\
        \ c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n\n    metadata\
        \ = {\n        'outputs' : [{\n          'type': 'table',\n          'storage':\
        \ 'gcs',\n          'format': 'csv',\n          'header': [x['name'] for x\
        \ in schema],\n          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n\
        \        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Read data', description='')\n_parser.add_argument(\"\
        --file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = read_data(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "parameters":
      - "name": |-
          file_path
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "file_name", "type": "String"}], "name": "Read data", "outputs": [{"name": "df_churn_op"}, {"name": "mlpipeline_ui_metadata"}]}
    "name": |-
      read-data
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --n-estimators
      - |-
        {{inputs.parameters.n_estimators}}
      - |-
        --max-depth
      - |-
        {{inputs.parameters.max_depth}}
      - |-
        --criterion
      - |-
        {{inputs.parameters.criterion}}
      - |-
        --max-features
      - |-
        {{inputs.parameters.max_features}}
      - |-
        --min-samples-split
      - |-
        {{inputs.parameters.min_samples_split}}
      - |-
        --conf-matr
      - |-
        /tmp/outputs/conf_matr/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --mlpipeline-metrics
      - |-
        /tmp/outputs/mlpipeline_metrics/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef rf_model(df_churn_ip , n_estimators , max_depth , criterion , max_features\
        \ ,\n              min_samples_split ,\n              conf_matr ,\n      \
        \        mlpipeline_ui_metadata , mlpipeline_metrics ):\n\n    import pandas\
        \ as pd\n    import numpy as np\n    import sklearn\n    from sklearn.ensemble\
        \ import RandomForestClassifier\n    from sklearn.model_selection import train_test_split\n\
        \    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score,\
        \ roc_curve, precision_score, recall_score, f1_score\n    import json\n  \
        \  import os\n    import gcsfs\n    from tensorflow.python.lib.io import file_io\n\
        \n    n_est = n_estimators\n    m_dep = max_depth\n    crit = criterion\n\
        \    m_feat = max_features\n    min_ss = min_samples_split\n\n    df_churn\
        \ = pd.read_csv(df_churn_ip)\n    df_churn.dropna(inplace=True)\n\n    y1\
        \ = df_churn['Churn']\n    X1 = df_churn.drop(['Churn'],axis=1)\n\n    #Split\
        \ Data into training and testing sets\n    X_train, X_test, y_train, y_test\
        \ = train_test_split(X1, y1, random_state=0)\n\n    #fit the model on the\
        \ Data and train it\n    rfc_best = RandomForestClassifier(random_state=42,\
        \ max_features=m_feat, n_estimators = n_est, \n                          \
        \            max_depth = m_dep, criterion = crit, min_samples_split = min_ss)\n\
        \n    rfc_best.fit(X_train, y_train) \n    y_test_pred = rfc_best.predict(X_test)\n\
        \    y_test_proba = rfc_best.predict_proba(X_test)[:,0]\n\n    #Get Metrics\
        \ scores\n\n    rf_score = float('%.4f' %rfc_best.score(X_test, y_test)) \
        \  \n    rf_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n\
        \    rf_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n    rf_f1\
        \ = float('%.4f' %f1_score(y_test, y_test_pred))\n\n    print(\"Accuraccy,\
        \ Precision, Recall, f1: \")\n    print(rf_score, rf_precision, rf_recall,\
        \ rf_f1)\n\n    #Confusion Matrix\n    cm = confusion_matrix(y_test, y_test_pred)\n\
        \    print(\"Confusion Matrix: {}\".format(cm))\n\n    #True and False Positive\
        \ Rates\n    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba) \n   \
        \ auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n    print('Auc\
        \ score: ')\n    print(auc_score)\n\n    #Converting the Confusion matrix\
        \ to a Dataframe\n    #Note that for Generating the Confusion Matrix Artifact,\
        \ the Confusion Matrix has to be converted from\n    #a numpy array to a DataFrame\
        \ in the exact format as given below\n\n    flags = {0:'Not Churned',1:'Churned'}\n\
        \    flag_list = ['Not Churned','Churned']\n    data = []\n    for target_index,\
        \ target_row in enumerate(cm):\n        for predicted_index, count in enumerate(target_row):\n\
        \            data.append((flags[target_index], flags[predicted_index], count))\n\
        \n    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n\
        \    print(df_cm)\n\n    with file_io.FileIO(conf_matr, 'w') as f:\n     \
        \   df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False,\
        \ index=False)\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token\
        \ = 'cloud')\n\n    #Save confusion Matrix to GCS\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Conf_matRF.csv',\
        \ 'w') as f:\n        df_cm.to_csv(f, columns=['target', 'predicted', 'count'],\
        \ header=False, index=False)\n\n    #roc curve\n    #For generating the ROC\
        \ curve, the tpr, fpr and thresholds need to be output as a DataFrame in the\
        \ \n    #exact format as given below\n\n    df_roc = pd.DataFrame({'fpr':\
        \ fpr, 'tpr': tpr, 'thresholds': thresholds})\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv',\
        \ 'w') as f:\n        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'],\
        \ header=False, index=False)\n\n    #code to generate artifacts\n\n    #Artifact\
        \ generator - metadata\n\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    metadata = {\n        'version' : 1, \n        'outputs'\
        \ : [{\n            'type': 'confusion_matrix',\n            'format': 'csv',\n\
        \            'storage': 'gcs',\n            'schema': [   #schema is required\
        \ in the exact same form for generating the artifact\n                {'name':\
        \ 'target', 'type': 'CATEGORY'},\n                {'name': 'predicted', 'type':\
        \ 'CATEGORY'},\n                {'name': 'count', 'type': 'NUMBER'},\n   \
        \         ],\n            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matRF.csv',\
        \ #conf_matr\n\n       # Convert flags to string because for bealean values\
        \ we want \"True|False\" to match csv data.\n            'labels': flag_list\n\
        \        },    \n        {\n          'type': 'roc',\n          'format':\
        \ 'csv',\n          'storage': 'gcs',\n          'schema': [  #schema is required\
        \ in the exact same form for generating the artifact\n            {'name':\
        \ 'fpr', 'type': 'NUMBER'},\n            {'name': 'tpr', 'type': 'NUMBER'},\n\
        \            {'name': 'thresholds', 'type': 'NUMBER'},\n          ],\n   \
        \       'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv'\n \
        \       }\n        ]\n    }\n\n    #Output the file to the container-level\
        \ root with the exact same name as below \n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    #Also output to Minio\
        \ \n    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n        json.dump(metadata,\
        \ f)\n\n    #The metric scores can output as Pipeline Metrics by generating\
        \ a json file as below\n    metrics = {\n    'metrics': [{\n      'name':\
        \ 'accuracy-score', # The name of the metric. Visualized as the column name\
        \ in the runs table.\n      'numberValue':  rf_score, # The value of the metric.\
        \ Must be a numeric value.\n      'format': \"RAW\",   # The optional format\
        \ of the metric. Supported values are \"RAW\" (displayed in raw format) \n\
        \                            # and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'precision-score', # The name of\
        \ the metric. Visualized as the column name in the runs table.\n      'numberValue':\
        \  rf_precision, # The value of the metric. Must be a numeric value.\n   \
        \   'format': \"RAW\",   # The optional format of the metric. Supported values\
        \ are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'recall', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ rf_recall, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'f1-score', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ rf_f1, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'auc-score', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ auc_score, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    }]\n    }\n\n    #Dump the metrics json file with the exact\
        \ same name to the container-root directory\n    with file_io.FileIO('/mlpipeline-metrics.json',\
        \ 'w') as f:\n        json.dump(metrics, f)\n\n    #Also dump it to the Minio\
        \ file storage\n    with file_io.FileIO(mlpipeline_metrics, 'w') as f:\n \
        \       json.dump(metrics, f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf\
        \ model', description='')\n_parser.add_argument(\"--df-churn-ip\", dest=\"\
        df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --n-estimators\", dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--max-depth\", dest=\"max_depth\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--criterion\", dest=\"\
        criterion\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --max-features\", dest=\"max_features\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--min-samples-split\", dest=\"min_samples_split\",\
        \ type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --conf-matr\", dest=\"conf_matr\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\"\
        , dest=\"mlpipeline_metrics\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = rf_model(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          one-hot-encode-df_one_hot
        "path": |-
          /tmp/inputs/df_churn_ip/data
      "parameters":
      - "name": |-
          criterion
      - "name": |-
          max_depth
      - "name": |-
          max_features
      - "name": |-
          min_samples_split
      - "name": |-
          n_estimators
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "n_estimators", "type": "Integer"}, {"name": "max_depth", "type": "Integer"}, {"name": "criterion", "type": "String"}, {"name": "max_features", "type": "String"}, {"name": "min_samples_split", "type": "Integer"}], "name": "Rf model", "outputs": [{"name": "conf_matr"}, {"name": "mlpipeline_ui_metadata"}, {"name": "mlpipeline_metrics"}]}
    "name": |-
      rf-model
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mlpipeline-metrics
        "path": |-
          /tmp/outputs/mlpipeline_metrics/data
      - "name": |-
          rf-model-conf_matr
        "path": |-
          /tmp/outputs/conf_matr/data
  - "dag":
      "tasks":
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          read-data
        "name": |-
          categorical-analysis
        "template": |-
          categorical-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.categorical-analysis.outputs.artifacts.categorical-analysis-df_churn_op}}
            "name": |-
              categorical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          categorical-analysis
        - |-
          read-data
        "name": |-
          mixed-analysis
        "template": |-
          mixed-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.mixed-analysis.outputs.artifacts.mixed-analysis-df_churn_op}}
            "name": |-
              mixed-analysis-df_churn_op
        "dependencies":
        - |-
          mixed-analysis
        "name": |-
          numerical-analysis
        "template": |-
          numerical-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.numerical-analysis.outputs.artifacts.numerical-analysis-df_churn_op}}
            "name": |-
              numerical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          numerical-analysis
        - |-
          read-data
        "name": |-
          one-hot-encode
        "template": |-
          one-hot-encode
      - "arguments":
          "parameters":
          - "name": |-
              file_path
            "value": |-
              {{inputs.parameters.file_path}}
        "name": |-
          read-data
        "template": |-
          read-data
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.one-hot-encode.outputs.artifacts.one-hot-encode-df_one_hot}}
            "name": |-
              one-hot-encode-df_one_hot
          "parameters":
          - "name": |-
              criterion
            "value": |-
              {{inputs.parameters.criterion}}
          - "name": |-
              max_depth
            "value": |-
              {{inputs.parameters.max_depth}}
          - "name": |-
              max_features
            "value": |-
              {{inputs.parameters.max_features}}
          - "name": |-
              min_samples_split
            "value": |-
              {{inputs.parameters.min_samples_split}}
          - "name": |-
              n_estimators
            "value": |-
              {{inputs.parameters.n_estimators}}
        "dependencies":
        - |-
          one-hot-encode
        "name": |-
          rf-model
        "template": |-
          rf-model
    "inputs":
      "parameters":
      - "name": |-
          criterion
      - "name": |-
          file_path
      - "name": |-
          max_depth
      - "name": |-
          max_features
      - "name": |-
          min_samples_split
      - "name": |-
          n_estimators
    "name": |-
      telco-merchant-churn-prediction-pipeline
