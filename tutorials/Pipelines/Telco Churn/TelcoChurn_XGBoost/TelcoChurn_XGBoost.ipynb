{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Pipeline for Artifacts Generation, File Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full README at:\n",
    "\n",
    "https://github.com/ydataai/academy/blob/master/tutorials/Pipelines/README.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project where your bucket is located\n",
    "gcp_bucket_project = \"gcp_project\"\n",
    "\n",
    "# Bucket path where the input files will be downloaded and the output files will be uploaded\n",
    "# The input files will be read from the defined folder (this example will read the files from the root folder - /)\n",
    "# The output files will be uploaded to defined folder (this example will upload the files to the Telco_runs folder - /Telco_runs/)\n",
    "input_files_path = \"gs://bucket_name/\"\n",
    "output_files_path = \"gs://bucket_name/Telco_runs/\"\n",
    "\n",
    "# GCP service account file used to download the samples and upload the results to your private bucket\n",
    "gcp_service_account = {\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"XXX\",\n",
    "    \"private_key_id\": \"XXX\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nXXX\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"XXX\",\n",
    "    \"client_id\": \"XXX\",\n",
    "    \"auth_uri\": \"XXX\",\n",
    "    \"token_uri\": \"XXX\",\n",
    "    \"auth_provider_x509_cert_url\": \"XXX\",\n",
    "    \"client_x509_cert_url\": \"XXX\",\n",
    "}\n",
    "\n",
    "# The python packages that will be installed in the multiple pipeline steps\n",
    "pip_packages_to_install = [\n",
    "    \"scikit-learn==0.22.2\",\n",
    "    \"numpy==1.17.2\",\n",
    "    \"pandas==1.0.3\",\n",
    "    \"xgboost==1.0.2\",\n",
    "    \"gcsfs\",\n",
    "    \"tensorflow==2.2.0\",\n",
    "    \"seaborn==0.9.0\",\n",
    "    \"matplotlib==3.1.1\",\n",
    "    \"mpld3==0.5.1\",\n",
    "]\n",
    "\n",
    "# TelcoChurnXGB parameters\n",
    "n_estimators = 100\n",
    "verbosity = 0\n",
    "max_depth = 2\n",
    "eta = 1\n",
    "silent = 0\n",
    "\n",
    "# Kfp\n",
    "pipeline_name = \"Telco Merchant Churn Prediction Pipeline\"\n",
    "pipeline_description = \"Churn predictions using XGBoost Algorithm\"\n",
    "experiment_name = \"Telco experiment\"\n",
    "run_name = \"Sample Run\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data file from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(\n",
    "    df_churn_op: OutputPath(),\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_input_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    import os\n",
    "    import io\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_input_path.endswith(\"/\") : in_gcp_bucket_input_path+= \"/\";\n",
    "    \n",
    "    df_churn = pd.read_csv(in_gcp_bucket_input_path + \"Data.csv\")\n",
    "    df_churn.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    # A DataFrame too long cannot be displayed in the Artifacts\n",
    "\n",
    "    df_disp = df_churn.iloc[0:5]\n",
    "    df_disp = df_disp[\n",
    "        [\"customerID\", \"gender\", \"tenure\", \"Contract\", \"TotalCharges\", \"Churn\"]\n",
    "    ]\n",
    "\n",
    "    df_disp.to_csv(in_gcp_bucket_input_path + \"Data_Sample.csv\", index=False)\n",
    "\n",
    "    df_show = pd.read_csv(in_gcp_bucket_input_path + \"Data_Sample.csv\")\n",
    "    categorical_cols = [\n",
    "        c\n",
    "        for c in df_show.columns\n",
    "        if df_show[c].dtype == \"object\" or c == \"SeniorCitizen\"\n",
    "    ]\n",
    "\n",
    "    numerical_cols = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "    schema = [\n",
    "        {\"name\": c, \"type\": \"CATEGORY\" if c in categorical_cols else \"NUMBER\"}\n",
    "        for c in df_show.columns\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"table\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"format\": \"csv\",\n",
    "                \"header\": [x[\"name\"] for x in schema],\n",
    "                \"source\": in_gcp_bucket_input_path + \"Data_Sample.csv\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.func_to_container_op(\n",
    "    func=read_data,\n",
    "    output_component_file=\"./read-data-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis and Artifact Generation for Categorical Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    # Churn Plot\n",
    "    ax = sns.catplot(\n",
    "        y=\"Churn\",\n",
    "        kind=\"count\",\n",
    "        data=df,\n",
    "        height=2.0,\n",
    "        aspect=3.0,\n",
    "        palette=\"bright\",\n",
    "        legend=True,\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/Graphs/churn_plot.html\", \"w\") as f:\n",
    "        f.write(s)\n",
    "\n",
    "    def barplot_percentages(feature, orient=\"v\", axis_name=\"percentage of customers\"):\n",
    "        ratios = pd.DataFrame()\n",
    "        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n",
    "        g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n",
    "        g[axis_name] = g[axis_name] / len(df)\n",
    "        if orient == \"v\":\n",
    "            ax = sns.barplot(\n",
    "                x=feature,\n",
    "                y=axis_name,\n",
    "                hue=\"Churn\",\n",
    "                data=g,\n",
    "                orient=orient,\n",
    "                palette=\"bright\",\n",
    "            )\n",
    "            ax.set_yticklabels([\"{:,.0%}\".format(y) for y in ax.get_yticks()])\n",
    "        else:\n",
    "            ax = sns.barplot(\n",
    "                x=axis_name,\n",
    "                y=feature,\n",
    "                hue=\"Churn\",\n",
    "                data=g,\n",
    "                orient=orient,\n",
    "                palette=\"bright\",\n",
    "            )\n",
    "            ax.set_xticklabels([\"{:,.0%}\".format(x) for x in ax.get_xticks()])\n",
    "        ax.plot()\n",
    "\n",
    "    # Genders\n",
    "    df[\"churn_rate\"] = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "    g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=0.9)\n",
    "    ax = g.map(\n",
    "        sns.barplot, \"gender\", \"churn_rate\", palette=\"bright\", order=[\"Female\", \"Male\"]\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Genders.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Multiple Lines\n",
    "    fig = plt.figure(figsize=(9, 4.5))\n",
    "    barplot_percentages(\"MultipleLines\", orient=\"v\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Service-wise Columns analysis\n",
    "    cols = [\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "    ]\n",
    "    df1 = pd.melt(df[df[\"InternetService\"] != \"No\"][cols]).rename(\n",
    "        {\"value\": \"Has service\"}, axis=1\n",
    "    )\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    ax = sns.countplot(data=df1, x=\"variable\", hue=\"Has service\", palette=\"bright\")\n",
    "    ax.set(xlabel=\"Additional service\", ylabel=\"Num of customers\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Service-wise Columns analysis2\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    df1 = df[(df.InternetService != \"No\") & (df.Churn == \"Yes\")]\n",
    "    df1 = pd.melt(df1[cols]).rename({\"value\": \"Has service\"}, axis=1)\n",
    "    ax = sns.countplot(\n",
    "        data=df1,\n",
    "        x=\"variable\",\n",
    "        hue=\"Has service\",\n",
    "        hue_order=[\"No\", \"Yes\"],\n",
    "        palette=\"bright\",\n",
    "    )\n",
    "    ax.set(xlabel=\"Additional service\", ylabel=\"Num of churns\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    # Generating Metadata\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/churn_plot.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Genders.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis.html\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_categorical_analysis = kfp.components.func_to_container_op(\n",
    "    func=categorical_analysis,\n",
    "    output_component_file=\"./categorical_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Statistical Analysis and Artifact Generation for Numerical and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    df_churn_ip2: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df2 = pd.read_csv(df_churn_ip2)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    df[\"total_charges_to_tenure_ratio\"] = df[\"TotalCharges\"] / df[\"tenure\"]\n",
    "    df[\"monthly_charges_diff\"] = (\n",
    "        df[\"MonthlyCharges\"] - df[\"total_charges_to_tenure_ratio\"]\n",
    "    )\n",
    "    df[\"churn_rate\"] = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "\n",
    "    # Internet Service vs Monthly Charges\n",
    "    ax = sns.catplot(\n",
    "        x=\"InternetService\",\n",
    "        y=\"MonthlyCharges\",\n",
    "        hue=\"Churn\",\n",
    "        kind=\"violin\",\n",
    "        split=True,\n",
    "        palette=\"pastel\",\n",
    "        data=df,\n",
    "        height=4.2,\n",
    "        aspect=1.4,\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO(\n",
    "        in_gcp_bucket_output_path + \"Artifacts/Graphs/violinplot2.html\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(s)\n",
    "\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/violinplot2.html\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_mixed_analysis = kfp.components.func_to_container_op(\n",
    "    func=mixed_analysis,\n",
    "    output_component_file=\"./mixed_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(\n",
    "    df_churn_ip: InputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    df_churn_op: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import mpld3\n",
    "    import json\n",
    "    import io\n",
    "    import os\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "    \n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "\n",
    "    # kdeplots - tenure, Monthly Charges, Total Charges\n",
    "    def kdeplot(feature):\n",
    "        fig = plt.figure(figsize=(9, 4))\n",
    "        plt.title(\"KDE for {}\".format(feature))\n",
    "        ax0 = sns.kdeplot(\n",
    "            df[df[\"Churn\"] == \"No\"][feature].dropna(), color=\"navy\", label=\"Churn: No\"\n",
    "        )\n",
    "        ax1 = sns.kdeplot(\n",
    "            df[df[\"Churn\"] == \"Yes\"][feature].dropna(),\n",
    "            color=\"orange\",\n",
    "            label=\"Churn: Yes\",\n",
    "        )\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        s = mpld3.fig_to_html(fig)\n",
    "\n",
    "        with file_io.FileIO(\n",
    "            in_gcp_bucket_output_path + \"Artifacts/Graphs/{}.html\".format(feature), \"w\"\n",
    "        ) as f:\n",
    "            f.write(s)\n",
    "\n",
    "    kdeplot(\"TotalCharges\")\n",
    "\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/Graphs/TotalCharges.html\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_numerical_analysis = kfp.components.func_to_container_op(\n",
    "    func=numerical_analysis,\n",
    "    output_component_file=\"./numerical_analysis.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(\n",
    "    df_churn_ip: InputPath(), df_churn_imputed: InputPath(), df_one_hot: OutputPath()\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn_imp = pd.read_csv(df_churn_imputed)\n",
    "    empty_cols = [\n",
    "        \"customerID\",\n",
    "        \"gender\",\n",
    "        \"SeniorCitizen\",\n",
    "        \"Partner\",\n",
    "        \"Dependents\",\n",
    "        \"tenure\",\n",
    "        \"PhoneService\",\n",
    "        \"MultipleLines\",\n",
    "        \"InternetService\",\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "        \"Contract\",\n",
    "        \"PaperlessBilling\",\n",
    "        \"PaymentMethod\",\n",
    "        \"MonthlyCharges\",\n",
    "        \"TotalCharges\",\n",
    "        \"Churn\",\n",
    "    ]\n",
    "\n",
    "    for i in empty_cols:\n",
    "        df_churn[i] = df_churn[i].replace(\" \", np.nan)\n",
    "\n",
    "    df_churn.drop([\"customerID\"], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    binary_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n",
    "\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    # Encoding column 'gender'\n",
    "    df_churn[\"gender\"] = df_churn[\"gender\"].replace({\"Male\": 1, \"Female\": 0})\n",
    "\n",
    "    category_cols = [\n",
    "        \"PaymentMethod\",\n",
    "        \"MultipleLines\",\n",
    "        \"InternetService\",\n",
    "        \"OnlineSecurity\",\n",
    "        \"OnlineBackup\",\n",
    "        \"DeviceProtection\",\n",
    "        \"TechSupport\",\n",
    "        \"StreamingTV\",\n",
    "        \"StreamingMovies\",\n",
    "        \"Contract\",\n",
    "    ]\n",
    "\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "\n",
    "    df_churn_targets = df_churn[\"Churn\"].unique()\n",
    "    df_churn[\"Churn\"] = df_churn[\"Churn\"].replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    df_churn.to_csv(df_one_hot, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_one_hot_encode = kfp.components.func_to_container_op(\n",
    "    func=one_hot_encode,\n",
    "    output_component_file=\"./one-hot-encode-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def xgb_model(\n",
    "    df_churn_ip: InputPath(),\n",
    "    n_estimators: int,\n",
    "    verbosity: int,\n",
    "    max_depth: int,\n",
    "    eta: int,\n",
    "    silent: int,\n",
    "    conf_matr: OutputPath(),\n",
    "    mlpipeline_ui_metadata: OutputPath(),\n",
    "    in_gcp_bucket_project: str,\n",
    "    in_gcp_sa_json: dict,\n",
    "    in_gcp_bucket_output_path: str,\n",
    "    mlpipeline_metrics: OutputPath(),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost as xgb\n",
    "    import json\n",
    "    import os\n",
    "    import io\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        accuracy_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "    )\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/gcp_sa.json\"\n",
    "\n",
    "    # include slash at the end of path\n",
    "    if not in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+= \"/\";\n",
    "        \n",
    "    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n",
    "\n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn[\"Churn\"]\n",
    "    X1 = df_churn.drop([\"Churn\"], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "\n",
    "    clfxg = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\", verbosity=0, max_depth=2, eta=1, silent=0\n",
    "    )\n",
    "    clfxg.fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = clfxg.predict(X_test)\n",
    "\n",
    "    y_test_proba = clfxg.predict_proba(X_test)[:, 0]\n",
    "\n",
    "    xgb_score = float(\"%.4f\" % accuracy_score(y_test, y_test_pred))\n",
    "    xgb_precision = float(\"%.4f\" % precision_score(y_test, y_test_pred))\n",
    "    xgb_recall = float(\"%.4f\" % recall_score(y_test, y_test_pred))\n",
    "    xgb_f1 = float(\"%.4f\" % f1_score(y_test, y_test_pred))\n",
    "\n",
    "    print(\"Accuracy, Precision, Recall, f1: \")\n",
    "    print(xgb_score, xgb_precision, xgb_recall, xgb_f1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "    auc_score = float(\"%.4f\" % roc_auc_score(y_test, y_test_proba))\n",
    "    print(\"Auc score: \")\n",
    "    print(auc_score)\n",
    "\n",
    "    # Converting the matrix to a Dataframe\n",
    "    flags = {0: \"Not Churned\", 1: \"Churned\"}\n",
    "    flag_list = [\"Not Churned\", \"Churned\"]\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "    print(df_cm)\n",
    "\n",
    "    with file_io.FileIO(conf_matr, \"w\") as f:\n",
    "        df_cm.to_csv(\n",
    "            f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n",
    "\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/XGBConf_mat.csv\", \"w\") as f:\n",
    "        df_cm.to_csv(\n",
    "            f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    # roc curve\n",
    "    df_roc = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": thresholds})\n",
    "    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/XGBROC_curve.csv\", \"w\") as f:\n",
    "        df_roc.to_csv(\n",
    "            f, columns=[\"fpr\", \"tpr\", \"thresholds\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "    # code to generate artifacts\n",
    "\n",
    "    # Artifact generator - metadata\n",
    "    metadata = {\n",
    "        \"version\": 1,\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"source\": in_gcp_bucket_output_path\n",
    "                + \"Artifacts/XGBConf_mat.csv\",  # conf_matr\n",
    "                # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "                \"labels\": flag_list,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"roc\",\n",
    "                \"format\": \"csv\",\n",
    "                \"storage\": \"gcs\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"fpr\", \"type\": \"NUMBER\"},\n",
    "                    {\"name\": \"tpr\", \"type\": \"NUMBER\"},\n",
    "                    {\"name\": \"thresholds\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"source\": in_gcp_bucket_output_path + \"Artifacts/XGBROC_curve.csv\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": \"accuracy-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_score,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"precision-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_precision,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"recall\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_recall,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"f1-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": xgb_f1,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"auc-score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": auc_score,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO(\"/mlpipeline-metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_metrics, \"w\") as f:\n",
    "        json.dump(metrics, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_xgb_model = kfp.components.func_to_container_op(\n",
    "    func=xgb_model,\n",
    "    output_component_file=\"./xgb-model-func.yaml\",\n",
    "    packages_to_install=pip_packages_to_install,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=pipeline_name,\n",
    "    description=pipeline_description,\n",
    ")\n",
    "\n",
    "# Create pipeline and set default values\n",
    "def TelcoChurnXGB_func(\n",
    "    in_n_estimators: int = n_estimators,\n",
    "    in_verbosity: int = verbosity,\n",
    "    in_max_depth: int = max_depth,\n",
    "    in_eta: int = eta,\n",
    "    in_silent: int = silent,\n",
    "    in_input_files_path: str = input_files_path,\n",
    "    in_output_files_path: str = output_files_path,\n",
    "    in_gcp_bucket_project: str = gcp_bucket_project,\n",
    "    in_gcp_service_account: dict = gcp_service_account,\n",
    "):\n",
    "  \n",
    "    # Passing pipeline parameter and add default values\n",
    "    read_data_task = kfp_read_data(\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_input_path=in_input_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    cat_analysis_task = kfp_categorical_analysis(\n",
    "        df_churn_ip=read_data_task.outputs[\"df_churn_op\"],\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    mix_analysis_task = kfp_mixed_analysis(\n",
    "        df_churn_ip=read_data_task.outputs[\"df_churn_op\"],\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "        df_churn_ip2=cat_analysis_task.outputs[\"df_churn_op\"],\n",
    "    )\n",
    "\n",
    "    num_analysis_task = kfp_numerical_analysis(\n",
    "        df_churn_ip=mix_analysis_task.outputs[\"df_churn_op\"],\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n",
    "\n",
    "    ohe_task = kfp_one_hot_encode(\n",
    "        df_churn_ip=read_data_task.outputs[\"df_churn_op\"],\n",
    "        df_churn_imputed=num_analysis_task.outputs[\"df_churn_op\"],\n",
    "    )\n",
    "\n",
    "    xgb_model_task = kfp_xgb_model(\n",
    "        ohe_task.outputs[\"df_one_hot\"],\n",
    "        in_n_estimators,\n",
    "        in_verbosity,\n",
    "        in_max_depth,\n",
    "        in_eta,\n",
    "        in_silent,\n",
    "        in_gcp_bucket_project=in_gcp_bucket_project,\n",
    "        in_gcp_bucket_output_path=in_output_files_path,\n",
    "        in_gcp_sa_json=in_gcp_service_account,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kfp.compiler as comp\n",
    "\n",
    "pipeline_func = TelcoChurnXGB_func\n",
    "pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\n",
    "\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from random import randrange\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "# check if pipeline already exists -> if not, create a new one\n",
    "filter = json.dumps(\n",
    "    {\n",
    "        \"predicates\": [\n",
    "            {\"key\": \"name\", \"op\": 1, \"string_value\": \"{}\".format(pipeline_name)}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pipeline = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if pipeline.pipelines is None:\n",
    "    print(\"Creating a new pipeline: \" + pipeline_name)\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(\n",
    "        pipeline_filename, name=pipeline_name, description=pipeline_description\n",
    "    )\n",
    "else:\n",
    "    print(\"Pipeline already exists: \" + pipeline_name)\n",
    "    pipeline = pipeline.pipelines[0]\n",
    "    \n",
    "pipeline_version = str(uuid.uuid4())\n",
    "\n",
    "print(\"Creating a new pipeline version: \" + pipeline_name + str(\" [\" + pipeline_version + \"]\"))\n",
    "client.pipeline_uploads.upload_pipeline_version(\n",
    "    pipeline_filename,\n",
    "    name=pipeline_name + str(\" [\" + pipeline_version + \"]\"),\n",
    "    pipelineid=pipeline.id,\n",
    ")\n",
    "\n",
    "# create a new experiment or use an existing one\n",
    "print(\"Creating a new experiment or use an existing one: \" + experiment_name)\n",
    "experiment = client.create_experiment(name=experiment_name)\n",
    "\n",
    "# create a new run with a random identifier\n",
    "run_random_id = str(randrange(1000))\n",
    "print(\"Creating a new run: \" + run_name + \" [\" + run_random_id + \"]\")\n",
    "new_run = client.run_pipeline(\n",
    "    experiment.id, run_name + \" [\" + run_random_id + \"]\", pipeline_id=pipeline.id\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}