{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Pipeline for Artifacts Generation, File Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data file from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read the Data file from GCS Bucket## Read Data\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(file_name: str, df_churn_op: OutputPath(), mlpipeline_ui_metadata: OutputPath()): \n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df_churn = pd.read_csv(file_name)\n",
    "    df_churn.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    #A DataFrame too long cannot be displayed in the Artifacts\n",
    "    \n",
    "    df_disp = df_churn.iloc[0:5]\n",
    "    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n",
    " \n",
    "    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv', index=False)\n",
    "    \n",
    "    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\")\n",
    "    categorical_cols = [c for c in df_show.columns if df_show[c].dtype == 'object' or c == 'SeniorCitizen']\n",
    "\n",
    "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "    schema = [{'name':c, 'type': 'CATEGORY'if c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'table',\n",
    "          'storage': 'gcs',\n",
    "          'format': 'csv',\n",
    "          'header': [x['name'] for x in schema],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.func_to_container_op(func = read_data, \n",
    "                                                          output_component_file = './read-data-func.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis and Artifact Generation for Categorical Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df_churn_ip :InputPath(), mlpipeline_ui_metadata: OutputPath(), df_churn_op :OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "\n",
    "\n",
    "    \n",
    "    #Categorical Analysis\n",
    "    \n",
    "    #Churn Plot\n",
    "\n",
    "    ax = sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.0, aspect=3.0, palette = 'bright',\n",
    "                     legend = True)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html', 'w') as f:\n",
    "        f.write(s)\n",
    "        \n",
    "\n",
    "    def barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n",
    "        ratios = pd.DataFrame()\n",
    "        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n",
    "        g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n",
    "        g[axis_name] = g[axis_name]/len(df)\n",
    "        if orient == 'v':\n",
    "            ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient, palette = 'bright')\n",
    "            ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n",
    "        else:\n",
    "            ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient, palette = 'bright')\n",
    "            ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n",
    "        ax.plot()\n",
    "\n",
    "    #Genders \n",
    "    \n",
    "    df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "    g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=.9)\n",
    "    ax = g.map(sns.barplot, \"gender\", \"churn_rate\", palette = \"bright\", order= ['Female', 'Male'])\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Genders.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "        \n",
    "    #Multiple Lines\n",
    "    \n",
    "    fig = plt.figure(figsize=(9, 4.5))\n",
    "    barplot_percentages(\"MultipleLines\", orient='v')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Multiple_lines.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "    #Service-wise Columns analysis\n",
    "    \n",
    "    cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "    df1 = pd.melt(df[df[\"InternetService\"] != \"No\"][cols]).rename({'value': 'Has service'}, axis=1)\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    ax = sns.countplot(data=df1, x='variable', hue='Has service', palette = 'bright')\n",
    "    ax.set(xlabel='Additional service', ylabel='Num of customers')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "\n",
    "    #Service-wise Columns analysis2\n",
    "    \n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    df1 = df[(df.InternetService != \"No\") & (df.Churn == \"Yes\")]\n",
    "    df1 = pd.melt(df1[cols]).rename({'value': 'Has service'}, axis=1)\n",
    "    ax = sns.countplot(data=df1, x='variable', hue='Has service', hue_order=['No', 'Yes'], palette = 'bright')\n",
    "    ax.set(xlabel='Additional service', ylabel='Num of churns')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis2.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "\n",
    "    #Generating Metadata\n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type' : 'web-app',\n",
    "            'storage' : 'gcs',\n",
    "            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html'\n",
    "            }, \n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Genders.html\",\n",
    "        },\n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Multiple_lines.html\",\n",
    "        },\n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis.html\",\n",
    "        },\n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis2.html\",\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_categorical_analysis = kfp.components.func_to_container_op(func = categorical_analysis, \n",
    "                                                          output_component_file = './categorical_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Statistical Analysis and Artifact Generation for Numerical and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_analysis(df_churn_ip :InputPath(), df_churn_ip2 :InputPath(), \n",
    "                   mlpipeline_ui_metadata: OutputPath(), df_churn_op :OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df2 = pd.read_csv(df_churn_ip2)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "    \n",
    "    \n",
    "    df['total_charges_to_tenure_ratio'] = df['TotalCharges'] / df['tenure']\n",
    "    df['monthly_charges_diff'] = df['MonthlyCharges'] - df['total_charges_to_tenure_ratio']\n",
    "    df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "    \n",
    "    \n",
    "    #Internet Service vs Monthly Charges\n",
    "    \n",
    "    ax = sns.catplot(x=\"InternetService\", y=\"MonthlyCharges\", hue=\"Churn\", kind=\"violin\",\n",
    "                     split=True, palette=\"pastel\", data=df, height=4.2, aspect=1.4);\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/violinplot2.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "        \n",
    "\n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/violinplot2.html\",\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_mixed_analysis = kfp.components.func_to_container_op(func = mixed_analysis, \n",
    "                                                          output_component_file = './mixed_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(df_churn_ip :InputPath(), mlpipeline_ui_metadata: OutputPath(), df_churn_op: OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "    \n",
    "    \n",
    "    #kdeplots - tenure, Monthly Charges, Total Charges\n",
    "    \n",
    "    def kdeplot(feature):\n",
    "        fig = plt.figure(figsize=(9, 4))\n",
    "        plt.title(\"KDE for {}\".format(feature))\n",
    "        ax0 = sns.kdeplot(df[df['Churn'] == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No')\n",
    "        ax1 = sns.kdeplot(df[df['Churn'] == 'Yes'][feature].dropna(), color= 'orange', label= 'Churn: Yes')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        s = mpld3.fig_to_html(fig)\n",
    "\n",
    "        with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/{}.html'.format(feature), 'w') as f:\n",
    "            f.write(s)\n",
    "\n",
    "    kdeplot('TotalCharges')\n",
    "\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/TotalCharges.html\",\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_numerical_analysis = kfp.components.func_to_container_op(func = numerical_analysis, \n",
    "                                                          output_component_file = './numerical_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(df_churn_ip: InputPath(), df_churn_imputed :InputPath(), df_one_hot: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn_imp = pd.read_csv(df_churn_imputed)\n",
    "    empty_cols = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "\n",
    "    for i in empty_cols:\n",
    "        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n",
    "\n",
    "    df_churn.drop(['customerID'], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n",
    "\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    #Encoding column 'gender'\n",
    "    df_churn['gender'] = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n",
    "\n",
    "\n",
    "    category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n",
    "                   'OnlineBackup','DeviceProtection',\n",
    "                   'TechSupport','StreamingTV','StreamingMovies','Contract']\n",
    "\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "    \n",
    "    df_churn_targets = df_churn['Churn'].unique()\n",
    "    df_churn['Churn'] = df_churn['Churn'].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    df_churn.to_csv(df_one_hot, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_one_hot_encode = kfp.components.func_to_container_op(func = one_hot_encode, \n",
    "                                                          output_component_file = './one-hot-encode-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'imbalanced-learn==0.6.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def xgb_model(df_churn_ip: InputPath(), \n",
    "              n_estimators: int, verbosity: int, max_depth: int, eta: int, silent: int, \n",
    "              conf_matr: OutputPath(),\n",
    "              mlpipeline_ui_metadata: OutputPath(), mlpipeline_metrics: OutputPath()):\n",
    "        \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "    import json\n",
    "    import os\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "\n",
    "\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    clfxg = xgb.XGBClassifier(objective='binary:logistic', verbosity=0, max_depth=2, eta = 1, silent=0)\n",
    "    clfxg.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = clfxg.predict(X_test)\n",
    "    \n",
    "    y_test_proba = clfxg.predict_proba(X_test)[:,0]\n",
    "    \n",
    "    xgb_score = float('%.4f' %accuracy_score(y_test, y_test_pred))   \n",
    "    xgb_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n",
    "    xgb_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n",
    "    xgb_f1 = float('%.4f' %f1_score(y_test, y_test_pred))\n",
    "    \n",
    "    print(\"Accuracy, Precision, Recall, f1: \")\n",
    "    print(xgb_score, xgb_precision, xgb_recall, xgb_f1)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba) \n",
    "    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n",
    "    print('Auc score: ')\n",
    "    print(auc_score)\n",
    "    \n",
    "    #Converting the matrix to a Dataframe\n",
    "    \n",
    "    flags = {0:'Not Churned',1:'Churned'}\n",
    "    flag_list = ['Not Churned','Churned']\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    print(df_cm)\n",
    "    \n",
    "    with file_io.FileIO(conf_matr, 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "        \n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBConf_mat.csv', 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "    \n",
    "    \n",
    "    #roc curve\n",
    "\n",
    "    df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBROC_curve.csv', 'w') as f:\n",
    "        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n",
    "\n",
    "    \n",
    "    #code to generate artifacts\n",
    "    \n",
    "    #Artifact generator - metadata\n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type': 'confusion_matrix',\n",
    "            'format': 'csv',\n",
    "            'storage': 'gcs',\n",
    "            'schema': [\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matXGB.csv', #conf_matr\n",
    "            \n",
    "       # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "            'labels': flag_list\n",
    "        },    \n",
    "        {\n",
    "          'type': 'roc',\n",
    "          'format': 'csv',\n",
    "          'storage': 'gcs',\n",
    "          'schema': [\n",
    "            {'name': 'fpr', 'type': 'NUMBER'},\n",
    "            {'name': 'tpr', 'type': 'NUMBER'},\n",
    "            {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "          ],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveXGB.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'precision-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_precision, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'recall', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_recall, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'f1-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  xgb_f1, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'auc-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  auc_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        \n",
    "    with file_io.FileIO(mlpipeline_metrics, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_xgb_model = kfp.components.func_to_container_op(func = xgb_model, \n",
    "                                                          output_component_file = './xgb-model-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', \n",
    "                                                                                 'xgboost==1.0.2', 'gcsfs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(name='Telco Merchant Churn Prediction Pipeline',description='Churn predictions using XGBoost Algorithm')\n",
    "def TelcoChurnXGB_func(file_path = \"gs://pipelines_artifacts/Artifacts/Data.csv\", \n",
    "                n_estimators=100, verbosity=0, max_depth=2, eta = 1, silent=0):\n",
    "    \n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    read_data_task = kfp_read_data(file_name = file_path) \n",
    "    \n",
    "    cat_analysis_task = kfp_categorical_analysis(df_churn_ip = read_data_task.outputs['df_churn_op'])\n",
    "    mix_analysis_task = kfp_mixed_analysis(df_churn_ip = read_data_task.outputs['df_churn_op'], \n",
    "                                           df_churn_ip2 = cat_analysis_task.outputs['df_churn_op'])\n",
    "    num_analysis_task = kfp_numerical_analysis(df_churn_ip = mix_analysis_task.outputs['df_churn_op'])\n",
    "\n",
    "    ohe_task = kfp_one_hot_encode(df_churn_ip = read_data_task.outputs['df_churn_op'],\n",
    "                                  df_churn_imputed = num_analysis_task.outputs['df_churn_op'])\n",
    "    xgb_model_task = kfp_xgb_model(ohe_task.outputs['df_one_hot'],\n",
    "                                   n_estimators, verbosity, max_depth, eta, silent)\n",
    "\n",
    "#For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "#For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"100\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"0\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"1\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = TelcoChurnXGB_func\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
