apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: telco-merchant-churn-prediction-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.2.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-02-24T18:40:22.588365',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Churn predictions using
      XGBoost Algorithm", "inputs": [{"default": "100", "name": "in_n_estimators",
      "optional": true, "type": "Integer"}, {"default": "0", "name": "in_verbosity",
      "optional": true, "type": "Integer"}, {"default": "2", "name": "in_max_depth",
      "optional": true, "type": "Integer"}, {"default": "1", "name": "in_eta", "optional":
      true, "type": "Integer"}, {"default": "0", "name": "in_silent", "optional":
      true, "type": "Integer"}, {"default": "gs://bucket_name/", "name": "in_input_files_path",
      "optional": true, "type": "String"}, {"default": "gs://bucket_name/Telco_runs/",
      "name": "in_output_files_path", "optional": true, "type": "String"}, {"default":
      "gcp_project", "name": "in_gcp_bucket_project", "optional": true, "type": "String"},
      {"default": "{\"auth_provider_x509_cert_url\": \"XXX\", \"auth_uri\": \"XXX\",
      \"client_email\": \"XXX\", \"client_id\": \"XXX\", \"client_x509_cert_url\":
      \"XXX\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nXXX\\n-----END PRIVATE
      KEY-----\\n\", \"private_key_id\": \"XXX\", \"project_id\": \"XXX\", \"token_uri\":
      \"XXX\", \"type\": \"service_account\"}", "name": "in_gcp_service_account",
      "optional": true, "type": "JsonObject"}], "name": "Telco Merchant Churn Prediction
      Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.2.0}
spec:
  entrypoint: telco-merchant-churn-prediction-pipeline
  templates:
  - name: categorical-analysis
    container:
      args: [--df-churn-ip, /tmp/inputs/df_churn_ip/data, --in-gcp-bucket-project,
        '{{inputs.parameters.in_gcp_bucket_project}}', --in-gcp-bucket-output-path,
        '{{inputs.parameters.in_output_files_path}}', --in-gcp-sa-json, '{{inputs.parameters.in_gcp_service_account}}',
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data, --df-churn-op,
        /tmp/outputs/df_churn_op/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def categorical_analysis(
            df_churn_ip,
            in_gcp_bucket_project,
            in_gcp_bucket_output_path,
            in_gcp_sa_json,
            mlpipeline_ui_metadata,
            df_churn_op,
        ):

            import pandas as pd
            import numpy as np
            import gcsfs
            import seaborn as sns
            import matplotlib.pyplot as plt
            import warnings
            import mpld3
            import json
            import io
            import os
            from sklearn.ensemble import RandomForestClassifier
            from tensorflow.python.lib.io import file_io

            warnings.simplefilter(action="ignore", category=FutureWarning)
            warnings.simplefilter(action="ignore", category=UserWarning)

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/gcp_sa.json"

            with io.open("gcp_sa.json", "w", encoding="utf-8") as f:
                json.dump(in_gcp_sa_json, f, ensure_ascii=False)

            fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)

            # include slash at the end of path
            if not in_gcp_bucket_output_path.endswith("/") : in_gcp_bucket_output_path+= "/";

            df = pd.read_csv(df_churn_ip)
            df1 = df.copy(deep=True)
            df1.to_csv(df_churn_op, index=False)

            sns.set(style="white")
            df["TotalCharges"] = df["TotalCharges"].replace(" ", 0).astype("float32")

            # Churn Plot
            ax = sns.catplot(
                y="Churn",
                kind="count",
                data=df,
                height=2.0,
                aspect=3.0,
                palette="bright",
                legend=True,
            )

            fig = plt.gcf()
            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(in_gcp_bucket_output_path + "Artifacts/Graphs/churn_plot.html", "w") as f:
                f.write(s)

            def barplot_percentages(feature, orient="v", axis_name="percentage of customers"):
                ratios = pd.DataFrame()
                g = df.groupby(feature)["Churn"].value_counts().to_frame()
                g = g.rename({"Churn": axis_name}, axis=1).reset_index()
                g[axis_name] = g[axis_name] / len(df)
                if orient == "v":
                    ax = sns.barplot(
                        x=feature,
                        y=axis_name,
                        hue="Churn",
                        data=g,
                        orient=orient,
                        palette="bright",
                    )
                    ax.set_yticklabels(["{:,.0%}".format(y) for y in ax.get_yticks()])
                else:
                    ax = sns.barplot(
                        x=axis_name,
                        y=feature,
                        hue="Churn",
                        data=g,
                        orient=orient,
                        palette="bright",
                    )
                    ax.set_xticklabels(["{:,.0%}".format(x) for x in ax.get_xticks()])
                ax.plot()

            # Genders
            df["churn_rate"] = df["Churn"].replace("No", 0).replace("Yes", 1)
            g = sns.FacetGrid(df, col="SeniorCitizen", height=4, aspect=0.9)
            ax = g.map(
                sns.barplot, "gender", "churn_rate", palette="bright", order=["Female", "Male"]
            )

            fig = plt.gcf()
            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(
                in_gcp_bucket_output_path + "Artifacts/Graphs/Genders.html", "w"
            ) as f:
                f.write(s)

            # Multiple Lines
            fig = plt.figure(figsize=(9, 4.5))
            barplot_percentages("MultipleLines", orient="v")

            fig = plt.gcf()
            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(
                in_gcp_bucket_output_path + "Artifacts/Graphs/Multiple_lines.html", "w"
            ) as f:
                f.write(s)

            # Service-wise Columns analysis
            cols = [
                "OnlineSecurity",
                "OnlineBackup",
                "DeviceProtection",
                "TechSupport",
                "StreamingTV",
                "StreamingMovies",
            ]
            df1 = pd.melt(df[df["InternetService"] != "No"][cols]).rename(
                {"value": "Has service"}, axis=1
            )
            plt.figure(figsize=(10, 4.5))
            ax = sns.countplot(data=df1, x="variable", hue="Has service", palette="bright")
            ax.set(xlabel="Additional service", ylabel="Num of customers")

            fig = plt.gcf()

            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(
                in_gcp_bucket_output_path + "Artifacts/Graphs/Servicewise_analysis.html", "w"
            ) as f:
                f.write(s)

            # Service-wise Columns analysis2
            plt.figure(figsize=(10, 4.5))
            df1 = df[(df.InternetService != "No") & (df.Churn == "Yes")]
            df1 = pd.melt(df1[cols]).rename({"value": "Has service"}, axis=1)
            ax = sns.countplot(
                data=df1,
                x="variable",
                hue="Has service",
                hue_order=["No", "Yes"],
                palette="bright",
            )
            ax.set(xlabel="Additional service", ylabel="Num of churns")

            fig = plt.gcf()
            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(
                in_gcp_bucket_output_path + "Artifacts/Graphs/Servicewise_analysis2.html", "w"
            ) as f:
                f.write(s)

            # Generating Metadata
            metadata = {
                "version": 1,
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/churn_plot.html",
                    },
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/Genders.html",
                    },
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/Multiple_lines.html",
                    },
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/Servicewise_analysis.html",
                    },
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/Servicewise_analysis2.html",
                    },
                ],
            }

            with file_io.FileIO("/mlpipeline-ui-metadata.json", "w") as f:
                json.dump(metadata, f)

            with file_io.FileIO(mlpipeline_ui_metadata, "w") as f:
                json.dump(metadata, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Categorical analysis', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-project", dest="in_gcp_bucket_project", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-output-path", dest="in_gcp_bucket_output_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-sa-json", dest="in_gcp_sa_json", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-op", dest="df_churn_op", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = categorical_analysis(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_output_files_path}
      artifacts:
      - {name: read-data-df_churn_op, path: /tmp/inputs/df_churn_ip/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: categorical-analysis-df_churn_op, path: /tmp/outputs/df_churn_op/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df-churn-ip", {"inputPath": "df_churn_ip"}, "--in-gcp-bucket-project",
          {"inputValue": "in_gcp_bucket_project"}, "--in-gcp-bucket-output-path",
          {"inputValue": "in_gcp_bucket_output_path"}, "--in-gcp-sa-json", {"inputValue":
          "in_gcp_sa_json"}, "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"},
          "--df-churn-op", {"outputPath": "df_churn_op"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''scikit-learn==0.22.2'' ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2''
          ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0'' ''matplotlib==3.1.1''
          ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''scikit-learn==0.22.2'' ''numpy==1.17.2''
          ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0''
          ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\" \"$@\"", "sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef categorical_analysis(\n    df_churn_ip,\n    in_gcp_bucket_project,\n    in_gcp_bucket_output_path,\n    in_gcp_sa_json,\n    mlpipeline_ui_metadata,\n    df_churn_op,\n):\n\n    import
          pandas as pd\n    import numpy as np\n    import gcsfs\n    import seaborn
          as sns\n    import matplotlib.pyplot as plt\n    import warnings\n    import
          mpld3\n    import json\n    import io\n    import os\n    from sklearn.ensemble
          import RandomForestClassifier\n    from tensorflow.python.lib.io import
          file_io\n\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n    warnings.simplefilter(action=\"ignore\",
          category=UserWarning)\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]
          = \"/gcp_sa.json\"\n\n    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\")
          as f:\n        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n\n    fs
          = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n\n    # include slash
          at the end of path\n    if not in_gcp_bucket_output_path.endswith(\"/\")
          : in_gcp_bucket_output_path+= \"/\";\n\n    df = pd.read_csv(df_churn_ip)\n    df1
          = df.copy(deep=True)\n    df1.to_csv(df_churn_op, index=False)\n\n    sns.set(style=\"white\")\n    df[\"TotalCharges\"]
          = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n\n    # Churn
          Plot\n    ax = sns.catplot(\n        y=\"Churn\",\n        kind=\"count\",\n        data=df,\n        height=2.0,\n        aspect=3.0,\n        palette=\"bright\",\n        legend=True,\n    )\n\n    fig
          = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO(in_gcp_bucket_output_path
          + \"Artifacts/Graphs/churn_plot.html\", \"w\") as f:\n        f.write(s)\n\n    def
          barplot_percentages(feature, orient=\"v\", axis_name=\"percentage of customers\"):\n        ratios
          = pd.DataFrame()\n        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n        g
          = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n        g[axis_name]
          = g[axis_name] / len(df)\n        if orient == \"v\":\n            ax =
          sns.barplot(\n                x=feature,\n                y=axis_name,\n                hue=\"Churn\",\n                data=g,\n                orient=orient,\n                palette=\"bright\",\n            )\n            ax.set_yticklabels([\"{:,.0%}\".format(y)
          for y in ax.get_yticks()])\n        else:\n            ax = sns.barplot(\n                x=axis_name,\n                y=feature,\n                hue=\"Churn\",\n                data=g,\n                orient=orient,\n                palette=\"bright\",\n            )\n            ax.set_xticklabels([\"{:,.0%}\".format(x)
          for x in ax.get_xticks()])\n        ax.plot()\n\n    # Genders\n    df[\"churn_rate\"]
          = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n    g = sns.FacetGrid(df,
          col=\"SeniorCitizen\", height=4, aspect=0.9)\n    ax = g.map(\n        sns.barplot,
          \"gender\", \"churn_rate\", palette=\"bright\", order=[\"Female\", \"Male\"]\n    )\n\n    fig
          = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO(\n        in_gcp_bucket_output_path
          + \"Artifacts/Graphs/Genders.html\", \"w\"\n    ) as f:\n        f.write(s)\n\n    #
          Multiple Lines\n    fig = plt.figure(figsize=(9, 4.5))\n    barplot_percentages(\"MultipleLines\",
          orient=\"v\")\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with
          file_io.FileIO(\n        in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\",
          \"w\"\n    ) as f:\n        f.write(s)\n\n    # Service-wise Columns analysis\n    cols
          = [\n        \"OnlineSecurity\",\n        \"OnlineBackup\",\n        \"DeviceProtection\",\n        \"TechSupport\",\n        \"StreamingTV\",\n        \"StreamingMovies\",\n    ]\n    df1
          = pd.melt(df[df[\"InternetService\"] != \"No\"][cols]).rename(\n        {\"value\":
          \"Has service\"}, axis=1\n    )\n    plt.figure(figsize=(10, 4.5))\n    ax
          = sns.countplot(data=df1, x=\"variable\", hue=\"Has service\", palette=\"bright\")\n    ax.set(xlabel=\"Additional
          service\", ylabel=\"Num of customers\")\n\n    fig = plt.gcf()\n\n    s
          = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO(\n        in_gcp_bucket_output_path
          + \"Artifacts/Graphs/Servicewise_analysis.html\", \"w\"\n    ) as f:\n        f.write(s)\n\n    #
          Service-wise Columns analysis2\n    plt.figure(figsize=(10, 4.5))\n    df1
          = df[(df.InternetService != \"No\") & (df.Churn == \"Yes\")]\n    df1 =
          pd.melt(df1[cols]).rename({\"value\": \"Has service\"}, axis=1)\n    ax
          = sns.countplot(\n        data=df1,\n        x=\"variable\",\n        hue=\"Has
          service\",\n        hue_order=[\"No\", \"Yes\"],\n        palette=\"bright\",\n    )\n    ax.set(xlabel=\"Additional
          service\", ylabel=\"Num of churns\")\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with
          file_io.FileIO(\n        in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\",
          \"w\"\n    ) as f:\n        f.write(s)\n\n    # Generating Metadata\n    metadata
          = {\n        \"version\": 1,\n        \"outputs\": [\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/churn_plot.html\",\n            },\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/Genders.html\",\n            },\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/Multiple_lines.html\",\n            },\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis.html\",\n            },\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/Servicewise_analysis2.html\",\n            },\n        ],\n    }\n\n    with
          file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Categorical
          analysis'', description='''')\n_parser.add_argument(\"--df-churn-ip\", dest=\"df_churn_ip\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-project\",
          dest=\"in_gcp_bucket_project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-output-path\",
          dest=\"in_gcp_bucket_output_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-sa-json\",
          dest=\"in_gcp_sa_json\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\",
          dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = categorical_analysis(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "df_churn_ip"}, {"name": "in_gcp_bucket_project", "type": "String"},
          {"name": "in_gcp_bucket_output_path", "type": "String"}, {"name": "in_gcp_sa_json",
          "type": "JsonObject"}], "name": "Categorical analysis", "outputs": [{"name":
          "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"in_gcp_bucket_output_path":
          "{{inputs.parameters.in_output_files_path}}", "in_gcp_bucket_project": "{{inputs.parameters.in_gcp_bucket_project}}",
          "in_gcp_sa_json": "{{inputs.parameters.in_gcp_service_account}}"}'}
  - name: mixed-analysis
    container:
      args: [--df-churn-ip, /tmp/inputs/df_churn_ip/data, --df-churn-ip2, /tmp/inputs/df_churn_ip2/data,
        --in-gcp-bucket-project, '{{inputs.parameters.in_gcp_bucket_project}}', --in-gcp-bucket-output-path,
        '{{inputs.parameters.in_output_files_path}}', --in-gcp-sa-json, '{{inputs.parameters.in_gcp_service_account}}',
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data, --df-churn-op,
        /tmp/outputs/df_churn_op/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def mixed_analysis(
            df_churn_ip,
            df_churn_ip2,
            in_gcp_bucket_project,
            in_gcp_bucket_output_path,
            in_gcp_sa_json,
            mlpipeline_ui_metadata,
            df_churn_op,
        ):

            import pandas as pd
            import numpy as np
            import gcsfs
            import seaborn as sns
            import matplotlib.pyplot as plt
            import warnings
            import mpld3
            import json
            import io
            import os
            from sklearn.ensemble import RandomForestClassifier
            from tensorflow.python.lib.io import file_io

            warnings.simplefilter(action="ignore", category=FutureWarning)
            warnings.simplefilter(action="ignore", category=UserWarning)

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/gcp_sa.json"

            with io.open("gcp_sa.json", "w", encoding="utf-8") as f:
                json.dump(in_gcp_sa_json, f, ensure_ascii=False)

            fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)

            # include slash at the end of path
            if not in_gcp_bucket_output_path.endswith("/") : in_gcp_bucket_output_path+= "/";

            df = pd.read_csv(df_churn_ip)
            df2 = pd.read_csv(df_churn_ip2)

            df1 = df.copy(deep=True)
            df1.to_csv(df_churn_op, index=False)

            sns.set(style="white")
            df["TotalCharges"] = df["TotalCharges"].replace(" ", 0).astype("float32")

            df["total_charges_to_tenure_ratio"] = df["TotalCharges"] / df["tenure"]
            df["monthly_charges_diff"] = (
                df["MonthlyCharges"] - df["total_charges_to_tenure_ratio"]
            )
            df["churn_rate"] = df["Churn"].replace("No", 0).replace("Yes", 1)

            # Internet Service vs Monthly Charges
            ax = sns.catplot(
                x="InternetService",
                y="MonthlyCharges",
                hue="Churn",
                kind="violin",
                split=True,
                palette="pastel",
                data=df,
                height=4.2,
                aspect=1.4,
            )

            fig = plt.gcf()
            s = mpld3.fig_to_html(fig)

            with file_io.FileIO(
                in_gcp_bucket_output_path + "Artifacts/Graphs/violinplot2.html", "w"
            ) as f:
                f.write(s)

            metadata = {
                "version": 1,
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/violinplot2.html",
                    }
                ],
            }

            with file_io.FileIO("/mlpipeline-ui-metadata.json", "w") as f:
                json.dump(metadata, f)

            with file_io.FileIO(mlpipeline_ui_metadata, "w") as f:
                json.dump(metadata, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Mixed analysis', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-ip2", dest="df_churn_ip2", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-project", dest="in_gcp_bucket_project", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-output-path", dest="in_gcp_bucket_output_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-sa-json", dest="in_gcp_sa_json", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-op", dest="df_churn_op", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = mixed_analysis(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_output_files_path}
      artifacts:
      - {name: read-data-df_churn_op, path: /tmp/inputs/df_churn_ip/data}
      - {name: categorical-analysis-df_churn_op, path: /tmp/inputs/df_churn_ip2/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: mixed-analysis-df_churn_op, path: /tmp/outputs/df_churn_op/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df-churn-ip", {"inputPath": "df_churn_ip"}, "--df-churn-ip2",
          {"inputPath": "df_churn_ip2"}, "--in-gcp-bucket-project", {"inputValue":
          "in_gcp_bucket_project"}, "--in-gcp-bucket-output-path", {"inputValue":
          "in_gcp_bucket_output_path"}, "--in-gcp-sa-json", {"inputValue": "in_gcp_sa_json"},
          "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"}, "--df-churn-op",
          {"outputPath": "df_churn_op"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef mixed_analysis(\n    df_churn_ip,\n    df_churn_ip2,\n    in_gcp_bucket_project,\n    in_gcp_bucket_output_path,\n    in_gcp_sa_json,\n    mlpipeline_ui_metadata,\n    df_churn_op,\n):\n\n    import
          pandas as pd\n    import numpy as np\n    import gcsfs\n    import seaborn
          as sns\n    import matplotlib.pyplot as plt\n    import warnings\n    import
          mpld3\n    import json\n    import io\n    import os\n    from sklearn.ensemble
          import RandomForestClassifier\n    from tensorflow.python.lib.io import
          file_io\n\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n    warnings.simplefilter(action=\"ignore\",
          category=UserWarning)\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]
          = \"/gcp_sa.json\"\n\n    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\")
          as f:\n        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n\n    fs
          = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n\n    # include slash
          at the end of path\n    if not in_gcp_bucket_output_path.endswith(\"/\")
          : in_gcp_bucket_output_path+= \"/\";\n\n    df = pd.read_csv(df_churn_ip)\n    df2
          = pd.read_csv(df_churn_ip2)\n\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,
          index=False)\n\n    sns.set(style=\"white\")\n    df[\"TotalCharges\"] =
          df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n\n    df[\"total_charges_to_tenure_ratio\"]
          = df[\"TotalCharges\"] / df[\"tenure\"]\n    df[\"monthly_charges_diff\"]
          = (\n        df[\"MonthlyCharges\"] - df[\"total_charges_to_tenure_ratio\"]\n    )\n    df[\"churn_rate\"]
          = df[\"Churn\"].replace(\"No\", 0).replace(\"Yes\", 1)\n\n    # Internet
          Service vs Monthly Charges\n    ax = sns.catplot(\n        x=\"InternetService\",\n        y=\"MonthlyCharges\",\n        hue=\"Churn\",\n        kind=\"violin\",\n        split=True,\n        palette=\"pastel\",\n        data=df,\n        height=4.2,\n        aspect=1.4,\n    )\n\n    fig
          = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO(\n        in_gcp_bucket_output_path
          + \"Artifacts/Graphs/violinplot2.html\", \"w\"\n    ) as f:\n        f.write(s)\n\n    metadata
          = {\n        \"version\": 1,\n        \"outputs\": [\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/violinplot2.html\",\n            }\n        ],\n    }\n\n    with
          file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Mixed
          analysis'', description='''')\n_parser.add_argument(\"--df-churn-ip\", dest=\"df_churn_ip\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-ip2\",
          dest=\"df_churn_ip2\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-project\",
          dest=\"in_gcp_bucket_project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-output-path\",
          dest=\"in_gcp_bucket_output_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-sa-json\",
          dest=\"in_gcp_sa_json\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\",
          dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = mixed_analysis(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "df_churn_ip"}, {"name": "df_churn_ip2"}, {"name": "in_gcp_bucket_project",
          "type": "String"}, {"name": "in_gcp_bucket_output_path", "type": "String"},
          {"name": "in_gcp_sa_json", "type": "JsonObject"}], "name": "Mixed analysis",
          "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"in_gcp_bucket_output_path":
          "{{inputs.parameters.in_output_files_path}}", "in_gcp_bucket_project": "{{inputs.parameters.in_gcp_bucket_project}}",
          "in_gcp_sa_json": "{{inputs.parameters.in_gcp_service_account}}"}'}
  - name: numerical-analysis
    container:
      args: [--df-churn-ip, /tmp/inputs/df_churn_ip/data, --in-gcp-bucket-project,
        '{{inputs.parameters.in_gcp_bucket_project}}', --in-gcp-bucket-output-path,
        '{{inputs.parameters.in_output_files_path}}', --in-gcp-sa-json, '{{inputs.parameters.in_gcp_service_account}}',
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data, --df-churn-op,
        /tmp/outputs/df_churn_op/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def numerical_analysis(
            df_churn_ip,
            in_gcp_bucket_project,
            in_gcp_bucket_output_path,
            in_gcp_sa_json,
            mlpipeline_ui_metadata,
            df_churn_op,
        ):

            import pandas as pd
            import numpy as np
            import gcsfs
            import seaborn as sns
            import matplotlib.pyplot as plt
            import warnings
            import mpld3
            import json
            import io
            import os
            from sklearn.ensemble import RandomForestClassifier
            from tensorflow.python.lib.io import file_io

            warnings.simplefilter(action="ignore", category=FutureWarning)
            warnings.simplefilter(action="ignore", category=UserWarning)

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/gcp_sa.json"

            with io.open("gcp_sa.json", "w", encoding="utf-8") as f:
                json.dump(in_gcp_sa_json, f, ensure_ascii=False)

            fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)

            # include slash at the end of path
            if not in_gcp_bucket_output_path.endswith("/") : in_gcp_bucket_output_path+= "/";

            df = pd.read_csv(df_churn_ip)

            df1 = df.copy(deep=True)
            df1.to_csv(df_churn_op, index=False)

            sns.set(style="white")
            df["TotalCharges"] = df["TotalCharges"].replace(" ", 0).astype("float32")

            # kdeplots - tenure, Monthly Charges, Total Charges
            def kdeplot(feature):
                fig = plt.figure(figsize=(9, 4))
                plt.title("KDE for {}".format(feature))
                ax0 = sns.kdeplot(
                    df[df["Churn"] == "No"][feature].dropna(), color="navy", label="Churn: No"
                )
                ax1 = sns.kdeplot(
                    df[df["Churn"] == "Yes"][feature].dropna(),
                    color="orange",
                    label="Churn: Yes",
                )

                fig = plt.gcf()
                s = mpld3.fig_to_html(fig)

                with file_io.FileIO(
                    in_gcp_bucket_output_path + "Artifacts/Graphs/{}.html".format(feature), "w"
                ) as f:
                    f.write(s)

            kdeplot("TotalCharges")

            metadata = {
                "version": 1,
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "gcs",
                        "source": in_gcp_bucket_output_path + "Artifacts/Graphs/TotalCharges.html",
                    }
                ],
            }

            with file_io.FileIO("/mlpipeline-ui-metadata.json", "w") as f:
                json.dump(metadata, f)

            with file_io.FileIO(mlpipeline_ui_metadata, "w") as f:
                json.dump(metadata, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Numerical analysis', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-project", dest="in_gcp_bucket_project", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-output-path", dest="in_gcp_bucket_output_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-sa-json", dest="in_gcp_sa_json", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-op", dest="df_churn_op", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = numerical_analysis(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_output_files_path}
      artifacts:
      - {name: mixed-analysis-df_churn_op, path: /tmp/inputs/df_churn_ip/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: numerical-analysis-df_churn_op, path: /tmp/outputs/df_churn_op/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df-churn-ip", {"inputPath": "df_churn_ip"}, "--in-gcp-bucket-project",
          {"inputValue": "in_gcp_bucket_project"}, "--in-gcp-bucket-output-path",
          {"inputValue": "in_gcp_bucket_output_path"}, "--in-gcp-sa-json", {"inputValue":
          "in_gcp_sa_json"}, "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"},
          "--df-churn-op", {"outputPath": "df_churn_op"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''scikit-learn==0.22.2'' ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2''
          ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0'' ''matplotlib==3.1.1''
          ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''scikit-learn==0.22.2'' ''numpy==1.17.2''
          ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0''
          ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\" \"$@\"", "sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef numerical_analysis(\n    df_churn_ip,\n    in_gcp_bucket_project,\n    in_gcp_bucket_output_path,\n    in_gcp_sa_json,\n    mlpipeline_ui_metadata,\n    df_churn_op,\n):\n\n    import
          pandas as pd\n    import numpy as np\n    import gcsfs\n    import seaborn
          as sns\n    import matplotlib.pyplot as plt\n    import warnings\n    import
          mpld3\n    import json\n    import io\n    import os\n    from sklearn.ensemble
          import RandomForestClassifier\n    from tensorflow.python.lib.io import
          file_io\n\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n    warnings.simplefilter(action=\"ignore\",
          category=UserWarning)\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]
          = \"/gcp_sa.json\"\n\n    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\")
          as f:\n        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n\n    fs
          = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n\n    # include slash
          at the end of path\n    if not in_gcp_bucket_output_path.endswith(\"/\")
          : in_gcp_bucket_output_path+= \"/\";\n\n    df = pd.read_csv(df_churn_ip)\n\n    df1
          = df.copy(deep=True)\n    df1.to_csv(df_churn_op, index=False)\n\n    sns.set(style=\"white\")\n    df[\"TotalCharges\"]
          = df[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n\n    # kdeplots
          - tenure, Monthly Charges, Total Charges\n    def kdeplot(feature):\n        fig
          = plt.figure(figsize=(9, 4))\n        plt.title(\"KDE for {}\".format(feature))\n        ax0
          = sns.kdeplot(\n            df[df[\"Churn\"] == \"No\"][feature].dropna(),
          color=\"navy\", label=\"Churn: No\"\n        )\n        ax1 = sns.kdeplot(\n            df[df[\"Churn\"]
          == \"Yes\"][feature].dropna(),\n            color=\"orange\",\n            label=\"Churn:
          Yes\",\n        )\n\n        fig = plt.gcf()\n        s = mpld3.fig_to_html(fig)\n\n        with
          file_io.FileIO(\n            in_gcp_bucket_output_path + \"Artifacts/Graphs/{}.html\".format(feature),
          \"w\"\n        ) as f:\n            f.write(s)\n\n    kdeplot(\"TotalCharges\")\n\n    metadata
          = {\n        \"version\": 1,\n        \"outputs\": [\n            {\n                \"type\":
          \"web-app\",\n                \"storage\": \"gcs\",\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/Graphs/TotalCharges.html\",\n            }\n        ],\n    }\n\n    with
          file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Numerical
          analysis'', description='''')\n_parser.add_argument(\"--df-churn-ip\", dest=\"df_churn_ip\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-project\",
          dest=\"in_gcp_bucket_project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-output-path\",
          dest=\"in_gcp_bucket_output_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-sa-json\",
          dest=\"in_gcp_sa_json\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\",
          dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = numerical_analysis(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "df_churn_ip"}, {"name": "in_gcp_bucket_project", "type": "String"},
          {"name": "in_gcp_bucket_output_path", "type": "String"}, {"name": "in_gcp_sa_json",
          "type": "JsonObject"}], "name": "Numerical analysis", "outputs": [{"name":
          "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"in_gcp_bucket_output_path":
          "{{inputs.parameters.in_output_files_path}}", "in_gcp_bucket_project": "{{inputs.parameters.in_gcp_bucket_project}}",
          "in_gcp_sa_json": "{{inputs.parameters.in_gcp_service_account}}"}'}
  - name: one-hot-encode
    container:
      args: [--df-churn-ip, /tmp/inputs/df_churn_ip/data, --df-churn-imputed, /tmp/inputs/df_churn_imputed/data,
        --df-one-hot, /tmp/outputs/df_one_hot/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def one_hot_encode(
            df_churn_ip, df_churn_imputed, df_one_hot
        ):

            import pandas as pd
            import numpy as np

            df_churn = pd.read_csv(df_churn_ip)
            df_churn_imp = pd.read_csv(df_churn_imputed)
            empty_cols = [
                "customerID",
                "gender",
                "SeniorCitizen",
                "Partner",
                "Dependents",
                "tenure",
                "PhoneService",
                "MultipleLines",
                "InternetService",
                "OnlineSecurity",
                "OnlineBackup",
                "DeviceProtection",
                "TechSupport",
                "StreamingTV",
                "StreamingMovies",
                "Contract",
                "PaperlessBilling",
                "PaymentMethod",
                "MonthlyCharges",
                "TotalCharges",
                "Churn",
            ]

            for i in empty_cols:
                df_churn[i] = df_churn[i].replace(" ", np.nan)

            df_churn.drop(["customerID"], axis=1, inplace=True)
            df_churn = df_churn.dropna()
            binary_cols = ["Partner", "Dependents", "PhoneService", "PaperlessBilling"]

            for i in binary_cols:
                df_churn[i] = df_churn[i].replace({"Yes": 1, "No": 0})

            # Encoding column 'gender'
            df_churn["gender"] = df_churn["gender"].replace({"Male": 1, "Female": 0})

            category_cols = [
                "PaymentMethod",
                "MultipleLines",
                "InternetService",
                "OnlineSecurity",
                "OnlineBackup",
                "DeviceProtection",
                "TechSupport",
                "StreamingTV",
                "StreamingMovies",
                "Contract",
            ]

            for cc in category_cols:
                dummies = pd.get_dummies(df_churn[cc], drop_first=False)
                dummies = dummies.add_prefix("{}#".format(cc))
                df_churn.drop(cc, axis=1, inplace=True)
                df_churn = df_churn.join(dummies)

            df_churn_targets = df_churn["Churn"].unique()
            df_churn["Churn"] = df_churn["Churn"].replace({"Yes": 1, "No": 0})

            df_churn.to_csv(df_one_hot, index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='One hot encode', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-imputed", dest="df_churn_imputed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-one-hot", dest="df_one_hot", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = one_hot_encode(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: numerical-analysis-df_churn_op, path: /tmp/inputs/df_churn_imputed/data}
      - {name: read-data-df_churn_op, path: /tmp/inputs/df_churn_ip/data}
    outputs:
      artifacts:
      - {name: one-hot-encode-df_one_hot, path: /tmp/outputs/df_one_hot/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df-churn-ip", {"inputPath": "df_churn_ip"}, "--df-churn-imputed",
          {"inputPath": "df_churn_imputed"}, "--df-one-hot", {"outputPath": "df_one_hot"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''scikit-learn==0.22.2'' ''numpy==1.17.2''
          ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0''
          ''matplotlib==3.1.1'' ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef one_hot_encode(\n    df_churn_ip, df_churn_imputed, df_one_hot\n):\n\n    import
          pandas as pd\n    import numpy as np\n\n    df_churn = pd.read_csv(df_churn_ip)\n    df_churn_imp
          = pd.read_csv(df_churn_imputed)\n    empty_cols = [\n        \"customerID\",\n        \"gender\",\n        \"SeniorCitizen\",\n        \"Partner\",\n        \"Dependents\",\n        \"tenure\",\n        \"PhoneService\",\n        \"MultipleLines\",\n        \"InternetService\",\n        \"OnlineSecurity\",\n        \"OnlineBackup\",\n        \"DeviceProtection\",\n        \"TechSupport\",\n        \"StreamingTV\",\n        \"StreamingMovies\",\n        \"Contract\",\n        \"PaperlessBilling\",\n        \"PaymentMethod\",\n        \"MonthlyCharges\",\n        \"TotalCharges\",\n        \"Churn\",\n    ]\n\n    for
          i in empty_cols:\n        df_churn[i] = df_churn[i].replace(\" \", np.nan)\n\n    df_churn.drop([\"customerID\"],
          axis=1, inplace=True)\n    df_churn = df_churn.dropna()\n    binary_cols
          = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n\n    for
          i in binary_cols:\n        df_churn[i] = df_churn[i].replace({\"Yes\": 1,
          \"No\": 0})\n\n    # Encoding column ''gender''\n    df_churn[\"gender\"]
          = df_churn[\"gender\"].replace({\"Male\": 1, \"Female\": 0})\n\n    category_cols
          = [\n        \"PaymentMethod\",\n        \"MultipleLines\",\n        \"InternetService\",\n        \"OnlineSecurity\",\n        \"OnlineBackup\",\n        \"DeviceProtection\",\n        \"TechSupport\",\n        \"StreamingTV\",\n        \"StreamingMovies\",\n        \"Contract\",\n    ]\n\n    for
          cc in category_cols:\n        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n        dummies
          = dummies.add_prefix(\"{}#\".format(cc))\n        df_churn.drop(cc, axis=1,
          inplace=True)\n        df_churn = df_churn.join(dummies)\n\n    df_churn_targets
          = df_churn[\"Churn\"].unique()\n    df_churn[\"Churn\"] = df_churn[\"Churn\"].replace({\"Yes\":
          1, \"No\": 0})\n\n    df_churn.to_csv(df_one_hot, index=False)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''One hot encode'', description='''')\n_parser.add_argument(\"--df-churn-ip\",
          dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-imputed\",
          dest=\"df_churn_imputed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-one-hot\",
          dest=\"df_one_hot\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = one_hot_encode(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "df_churn_ip"}, {"name": "df_churn_imputed"}], "name": "One hot
          encode", "outputs": [{"name": "df_one_hot"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: read-data
    container:
      args: [--in-gcp-bucket-project, '{{inputs.parameters.in_gcp_bucket_project}}',
        --in-gcp-bucket-input-path, '{{inputs.parameters.in_input_files_path}}', --in-gcp-sa-json,
        '{{inputs.parameters.in_gcp_service_account}}', --df-churn-op, /tmp/outputs/df_churn_op/data,
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def read_data(
            df_churn_op,
            mlpipeline_ui_metadata,
            in_gcp_bucket_project,
            in_gcp_bucket_input_path,
            in_gcp_sa_json,
        ):

            import pandas as pd
            import numpy as np
            import gcsfs
            from tensorflow.python.lib.io import file_io
            import json
            import os
            import io

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/gcp_sa.json"

            with io.open("gcp_sa.json", "w", encoding="utf-8") as f:
                json.dump(in_gcp_sa_json, f, ensure_ascii=False)

            fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)

            # include slash at the end of path
            if not in_gcp_bucket_input_path.endswith("/") : in_gcp_bucket_input_path+= "/";

            df_churn = pd.read_csv(in_gcp_bucket_input_path + "Data.csv")
            df_churn.to_csv(df_churn_op, index=False)

            # A DataFrame too long cannot be displayed in the Artifacts

            df_disp = df_churn.iloc[0:5]
            df_disp = df_disp[
                ["customerID", "gender", "tenure", "Contract", "TotalCharges", "Churn"]
            ]

            df_disp.to_csv(in_gcp_bucket_input_path + "Data_Sample.csv", index=False)

            df_show = pd.read_csv(in_gcp_bucket_input_path + "Data_Sample.csv")
            categorical_cols = [
                c
                for c in df_show.columns
                if df_show[c].dtype == "object" or c == "SeniorCitizen"
            ]

            numerical_cols = ["tenure", "MonthlyCharges", "TotalCharges"]

            schema = [
                {"name": c, "type": "CATEGORY" if c in categorical_cols else "NUMBER"}
                for c in df_show.columns
            ]

            metadata = {
                "outputs": [
                    {
                        "type": "table",
                        "storage": "gcs",
                        "format": "csv",
                        "header": [x["name"] for x in schema],
                        "source": in_gcp_bucket_input_path + "Data_Sample.csv",
                    }
                ]
            }

            with file_io.FileIO("/mlpipeline-ui-metadata.json", "w") as f:
                json.dump(metadata, f)

            with file_io.FileIO(mlpipeline_ui_metadata, "w") as f:
                json.dump(metadata, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Read data', description='')
        _parser.add_argument("--in-gcp-bucket-project", dest="in_gcp_bucket_project", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-input-path", dest="in_gcp_bucket_input_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-sa-json", dest="in_gcp_sa_json", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-op", dest="df_churn_op", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = read_data(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_input_files_path}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: read-data-df_churn_op, path: /tmp/outputs/df_churn_op/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--in-gcp-bucket-project", {"inputValue": "in_gcp_bucket_project"},
          "--in-gcp-bucket-input-path", {"inputValue": "in_gcp_bucket_input_path"},
          "--in-gcp-sa-json", {"inputValue": "in_gcp_sa_json"}, "--df-churn-op", {"outputPath":
          "df_churn_op"}, "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''scikit-learn==0.22.2'' ''numpy==1.17.2''
          ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0'' ''seaborn==0.9.0''
          ''matplotlib==3.1.1'' ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef read_data(\n    df_churn_op,\n    mlpipeline_ui_metadata,\n    in_gcp_bucket_project,\n    in_gcp_bucket_input_path,\n    in_gcp_sa_json,\n):\n\n    import
          pandas as pd\n    import numpy as np\n    import gcsfs\n    from tensorflow.python.lib.io
          import file_io\n    import json\n    import os\n    import io\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]
          = \"/gcp_sa.json\"\n\n    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\")
          as f:\n        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n\n    fs
          = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n\n    # include slash
          at the end of path\n    if not in_gcp_bucket_input_path.endswith(\"/\")
          : in_gcp_bucket_input_path+= \"/\";\n\n    df_churn = pd.read_csv(in_gcp_bucket_input_path
          + \"Data.csv\")\n    df_churn.to_csv(df_churn_op, index=False)\n\n    #
          A DataFrame too long cannot be displayed in the Artifacts\n\n    df_disp
          = df_churn.iloc[0:5]\n    df_disp = df_disp[\n        [\"customerID\", \"gender\",
          \"tenure\", \"Contract\", \"TotalCharges\", \"Churn\"]\n    ]\n\n    df_disp.to_csv(in_gcp_bucket_input_path
          + \"Data_Sample.csv\", index=False)\n\n    df_show = pd.read_csv(in_gcp_bucket_input_path
          + \"Data_Sample.csv\")\n    categorical_cols = [\n        c\n        for
          c in df_show.columns\n        if df_show[c].dtype == \"object\" or c ==
          \"SeniorCitizen\"\n    ]\n\n    numerical_cols = [\"tenure\", \"MonthlyCharges\",
          \"TotalCharges\"]\n\n    schema = [\n        {\"name\": c, \"type\": \"CATEGORY\"
          if c in categorical_cols else \"NUMBER\"}\n        for c in df_show.columns\n    ]\n\n    metadata
          = {\n        \"outputs\": [\n            {\n                \"type\": \"table\",\n                \"storage\":
          \"gcs\",\n                \"format\": \"csv\",\n                \"header\":
          [x[\"name\"] for x in schema],\n                \"source\": in_gcp_bucket_input_path
          + \"Data_Sample.csv\",\n            }\n        ]\n    }\n\n    with file_io.FileIO(\"/mlpipeline-ui-metadata.json\",
          \"w\") as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,
          \"w\") as f:\n        json.dump(metadata, f)\n\nimport json\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Read data'', description='''')\n_parser.add_argument(\"--in-gcp-bucket-project\",
          dest=\"in_gcp_bucket_project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-input-path\",
          dest=\"in_gcp_bucket_input_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-sa-json\",
          dest=\"in_gcp_sa_json\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\",
          dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = read_data(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "in_gcp_bucket_project", "type": "String"}, {"name": "in_gcp_bucket_input_path",
          "type": "String"}, {"name": "in_gcp_sa_json", "type": "JsonObject"}], "name":
          "Read data", "outputs": [{"name": "df_churn_op"}, {"name": "mlpipeline_ui_metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"in_gcp_bucket_input_path":
          "{{inputs.parameters.in_input_files_path}}", "in_gcp_bucket_project": "{{inputs.parameters.in_gcp_bucket_project}}",
          "in_gcp_sa_json": "{{inputs.parameters.in_gcp_service_account}}"}'}
  - name: telco-merchant-churn-prediction-pipeline
    inputs:
      parameters:
      - {name: in_eta}
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_input_files_path}
      - {name: in_max_depth}
      - {name: in_n_estimators}
      - {name: in_output_files_path}
      - {name: in_silent}
      - {name: in_verbosity}
    dag:
      tasks:
      - name: categorical-analysis
        template: categorical-analysis
        dependencies: [read-data]
        arguments:
          parameters:
          - {name: in_gcp_bucket_project, value: '{{inputs.parameters.in_gcp_bucket_project}}'}
          - {name: in_gcp_service_account, value: '{{inputs.parameters.in_gcp_service_account}}'}
          - {name: in_output_files_path, value: '{{inputs.parameters.in_output_files_path}}'}
          artifacts:
          - {name: read-data-df_churn_op, from: '{{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}'}
      - name: mixed-analysis
        template: mixed-analysis
        dependencies: [categorical-analysis, read-data]
        arguments:
          parameters:
          - {name: in_gcp_bucket_project, value: '{{inputs.parameters.in_gcp_bucket_project}}'}
          - {name: in_gcp_service_account, value: '{{inputs.parameters.in_gcp_service_account}}'}
          - {name: in_output_files_path, value: '{{inputs.parameters.in_output_files_path}}'}
          artifacts:
          - {name: categorical-analysis-df_churn_op, from: '{{tasks.categorical-analysis.outputs.artifacts.categorical-analysis-df_churn_op}}'}
          - {name: read-data-df_churn_op, from: '{{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}'}
      - name: numerical-analysis
        template: numerical-analysis
        dependencies: [mixed-analysis]
        arguments:
          parameters:
          - {name: in_gcp_bucket_project, value: '{{inputs.parameters.in_gcp_bucket_project}}'}
          - {name: in_gcp_service_account, value: '{{inputs.parameters.in_gcp_service_account}}'}
          - {name: in_output_files_path, value: '{{inputs.parameters.in_output_files_path}}'}
          artifacts:
          - {name: mixed-analysis-df_churn_op, from: '{{tasks.mixed-analysis.outputs.artifacts.mixed-analysis-df_churn_op}}'}
      - name: one-hot-encode
        template: one-hot-encode
        dependencies: [numerical-analysis, read-data]
        arguments:
          artifacts:
          - {name: numerical-analysis-df_churn_op, from: '{{tasks.numerical-analysis.outputs.artifacts.numerical-analysis-df_churn_op}}'}
          - {name: read-data-df_churn_op, from: '{{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}'}
      - name: read-data
        template: read-data
        arguments:
          parameters:
          - {name: in_gcp_bucket_project, value: '{{inputs.parameters.in_gcp_bucket_project}}'}
          - {name: in_gcp_service_account, value: '{{inputs.parameters.in_gcp_service_account}}'}
          - {name: in_input_files_path, value: '{{inputs.parameters.in_input_files_path}}'}
      - name: xgb-model
        template: xgb-model
        dependencies: [one-hot-encode]
        arguments:
          parameters:
          - {name: in_eta, value: '{{inputs.parameters.in_eta}}'}
          - {name: in_gcp_bucket_project, value: '{{inputs.parameters.in_gcp_bucket_project}}'}
          - {name: in_gcp_service_account, value: '{{inputs.parameters.in_gcp_service_account}}'}
          - {name: in_max_depth, value: '{{inputs.parameters.in_max_depth}}'}
          - {name: in_n_estimators, value: '{{inputs.parameters.in_n_estimators}}'}
          - {name: in_output_files_path, value: '{{inputs.parameters.in_output_files_path}}'}
          - {name: in_silent, value: '{{inputs.parameters.in_silent}}'}
          - {name: in_verbosity, value: '{{inputs.parameters.in_verbosity}}'}
          artifacts:
          - {name: one-hot-encode-df_one_hot, from: '{{tasks.one-hot-encode.outputs.artifacts.one-hot-encode-df_one_hot}}'}
  - name: xgb-model
    container:
      args: [--df-churn-ip, /tmp/inputs/df_churn_ip/data, --n-estimators, '{{inputs.parameters.in_n_estimators}}',
        --verbosity, '{{inputs.parameters.in_verbosity}}', --max-depth, '{{inputs.parameters.in_max_depth}}',
        --eta, '{{inputs.parameters.in_eta}}', --silent, '{{inputs.parameters.in_silent}}',
        --in-gcp-bucket-project, '{{inputs.parameters.in_gcp_bucket_project}}', --in-gcp-sa-json,
        '{{inputs.parameters.in_gcp_service_account}}', --in-gcp-bucket-output-path,
        '{{inputs.parameters.in_output_files_path}}', --conf-matr, /tmp/outputs/conf_matr/data,
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data, --mlpipeline-metrics,
        /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' ||
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs'
        'tensorflow==2.2.0' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def xgb_model(
            df_churn_ip,
            n_estimators,
            verbosity,
            max_depth,
            eta,
            silent,
            conf_matr,
            mlpipeline_ui_metadata,
            in_gcp_bucket_project,
            in_gcp_sa_json,
            in_gcp_bucket_output_path,
            mlpipeline_metrics,
        ):

            import pandas as pd
            import numpy as np
            import xgboost as xgb
            import json
            import os
            import io
            import gcsfs
            from tensorflow.python.lib.io import file_io
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import (
                confusion_matrix,
                accuracy_score,
                roc_auc_score,
                roc_curve,
                precision_score,
                recall_score,
                f1_score,
            )

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/gcp_sa.json"

            # include slash at the end of path
            if not in_gcp_bucket_output_path.endswith("/") : in_gcp_bucket_output_path+= "/";

            with io.open("gcp_sa.json", "w", encoding="utf-8") as f:
                json.dump(in_gcp_sa_json, f, ensure_ascii=False)

            df_churn = pd.read_csv(df_churn_ip)
            df_churn.dropna(inplace=True)

            y1 = df_churn["Churn"]
            X1 = df_churn.drop(["Churn"], axis=1)

            X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)

            clfxg = xgb.XGBClassifier(
                objective="binary:logistic", verbosity=0, max_depth=2, eta=1, silent=0
            )
            clfxg.fit(X_train, y_train)

            y_test_pred = clfxg.predict(X_test)

            y_test_proba = clfxg.predict_proba(X_test)[:, 0]

            xgb_score = float("%.4f" % accuracy_score(y_test, y_test_pred))
            xgb_precision = float("%.4f" % precision_score(y_test, y_test_pred))
            xgb_recall = float("%.4f" % recall_score(y_test, y_test_pred))
            xgb_f1 = float("%.4f" % f1_score(y_test, y_test_pred))

            print("Accuracy, Precision, Recall, f1: ")
            print(xgb_score, xgb_precision, xgb_recall, xgb_f1)

            cm = confusion_matrix(y_test, y_test_pred)
            print("Confusion Matrix: {}".format(cm))

            fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)
            auc_score = float("%.4f" % roc_auc_score(y_test, y_test_proba))
            print("Auc score: ")
            print(auc_score)

            # Converting the matrix to a Dataframe
            flags = {0: "Not Churned", 1: "Churned"}
            flag_list = ["Not Churned", "Churned"]
            data = []
            for target_index, target_row in enumerate(cm):
                for predicted_index, count in enumerate(target_row):
                    data.append((flags[target_index], flags[predicted_index], count))

            df_cm = pd.DataFrame(data, columns=["target", "predicted", "count"])
            print(df_cm)

            with file_io.FileIO(conf_matr, "w") as f:
                df_cm.to_csv(
                    f, columns=["target", "predicted", "count"], header=False, index=False
                )

            fs = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)

            with file_io.FileIO(in_gcp_bucket_output_path + "Artifacts/XGBConf_mat.csv", "w") as f:
                df_cm.to_csv(
                    f, columns=["target", "predicted", "count"], header=False, index=False
                )

            # roc curve
            df_roc = pd.DataFrame({"fpr": fpr, "tpr": tpr, "thresholds": thresholds})
            with file_io.FileIO(in_gcp_bucket_output_path + "Artifacts/XGBROC_curve.csv", "w") as f:
                df_roc.to_csv(
                    f, columns=["fpr", "tpr", "thresholds"], header=False, index=False
                )

            # code to generate artifacts

            # Artifact generator - metadata
            metadata = {
                "version": 1,
                "outputs": [
                    {
                        "type": "confusion_matrix",
                        "format": "csv",
                        "storage": "gcs",
                        "schema": [
                            {"name": "target", "type": "CATEGORY"},
                            {"name": "predicted", "type": "CATEGORY"},
                            {"name": "count", "type": "NUMBER"},
                        ],
                        "source": in_gcp_bucket_output_path
                        + "Artifacts/XGBConf_mat.csv",  # conf_matr
                        # Convert flags to string because for bealean values we want "True|False" to match csv data.
                        "labels": flag_list,
                    },
                    {
                        "type": "roc",
                        "format": "csv",
                        "storage": "gcs",
                        "schema": [
                            {"name": "fpr", "type": "NUMBER"},
                            {"name": "tpr", "type": "NUMBER"},
                            {"name": "thresholds", "type": "NUMBER"},
                        ],
                        "source": in_gcp_bucket_output_path + "Artifacts/XGBROC_curve.csv",
                    },
                ],
            }

            with file_io.FileIO("/mlpipeline-ui-metadata.json", "w") as f:
                json.dump(metadata, f)

            with file_io.FileIO(mlpipeline_ui_metadata, "w") as f:
                json.dump(metadata, f)

            metrics = {
                "metrics": [
                    {
                        "name": "accuracy-score",  # The name of the metric. Visualized as the column name in the runs table.
                        "numberValue": xgb_score,  # The value of the metric. Must be a numeric value.
                        "format": "RAW",  # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                    },
                    {
                        "name": "precision-score",  # The name of the metric. Visualized as the column name in the runs table.
                        "numberValue": xgb_precision,  # The value of the metric. Must be a numeric value.
                        "format": "RAW",  # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                    },
                    {
                        "name": "recall",  # The name of the metric. Visualized as the column name in the runs table.
                        "numberValue": xgb_recall,  # The value of the metric. Must be a numeric value.
                        "format": "RAW",  # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                    },
                    {
                        "name": "f1-score",  # The name of the metric. Visualized as the column name in the runs table.
                        "numberValue": xgb_f1,  # The value of the metric. Must be a numeric value.
                        "format": "RAW",  # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                    },
                    {
                        "name": "auc-score",  # The name of the metric. Visualized as the column name in the runs table.
                        "numberValue": auc_score,  # The value of the metric. Must be a numeric value.
                        "format": "RAW",  # The optional format of the metric. Supported values are "RAW" (displayed in raw format) and "PERCENTAGE" (displayed in percentage format).
                    },
                ]
            }

            with file_io.FileIO("/mlpipeline-metrics.json", "w") as f:
                json.dump(metrics, f)

            with file_io.FileIO(mlpipeline_metrics, "w") as f:
                json.dump(metrics, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Xgb model', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--n-estimators", dest="n_estimators", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--verbosity", dest="verbosity", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--max-depth", dest="max_depth", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--eta", dest="eta", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--silent", dest="silent", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-project", dest="in_gcp_bucket_project", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-sa-json", dest="in_gcp_sa_json", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--in-gcp-bucket-output-path", dest="in_gcp_bucket_output_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--conf-matr", dest="conf_matr", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = xgb_model(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: in_eta}
      - {name: in_gcp_bucket_project}
      - {name: in_gcp_service_account}
      - {name: in_max_depth}
      - {name: in_n_estimators}
      - {name: in_output_files_path}
      - {name: in_silent}
      - {name: in_verbosity}
      artifacts:
      - {name: one-hot-encode-df_one_hot, path: /tmp/inputs/df_churn_ip/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
      - {name: xgb-model-conf_matr, path: /tmp/outputs/conf_matr/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df-churn-ip", {"inputPath": "df_churn_ip"}, "--n-estimators",
          {"inputValue": "n_estimators"}, "--verbosity", {"inputValue": "verbosity"},
          "--max-depth", {"inputValue": "max_depth"}, "--eta", {"inputValue": "eta"},
          "--silent", {"inputValue": "silent"}, "--in-gcp-bucket-project", {"inputValue":
          "in_gcp_bucket_project"}, "--in-gcp-sa-json", {"inputValue": "in_gcp_sa_json"},
          "--in-gcp-bucket-output-path", {"inputValue": "in_gcp_bucket_output_path"},
          "--conf-matr", {"outputPath": "conf_matr"}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}, "--mlpipeline-metrics", {"outputPath":
          "mlpipeline_metrics"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn==0.22.2''
          ''numpy==1.17.2'' ''pandas==1.0.3'' ''xgboost==1.0.2'' ''gcsfs'' ''tensorflow==2.2.0''
          ''seaborn==0.9.0'' ''matplotlib==3.1.1'' ''mpld3==0.5.1'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef xgb_model(\n    df_churn_ip,\n    n_estimators,\n    verbosity,\n    max_depth,\n    eta,\n    silent,\n    conf_matr,\n    mlpipeline_ui_metadata,\n    in_gcp_bucket_project,\n    in_gcp_sa_json,\n    in_gcp_bucket_output_path,\n    mlpipeline_metrics,\n):\n\n    import
          pandas as pd\n    import numpy as np\n    import xgboost as xgb\n    import
          json\n    import os\n    import io\n    import gcsfs\n    from tensorflow.python.lib.io
          import file_io\n    from sklearn.model_selection import train_test_split\n    from
          sklearn.metrics import (\n        confusion_matrix,\n        accuracy_score,\n        roc_auc_score,\n        roc_curve,\n        precision_score,\n        recall_score,\n        f1_score,\n    )\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]
          = \"/gcp_sa.json\"\n\n    # include slash at the end of path\n    if not
          in_gcp_bucket_output_path.endswith(\"/\") : in_gcp_bucket_output_path+=
          \"/\";\n\n    with io.open(\"gcp_sa.json\", \"w\", encoding=\"utf-8\") as
          f:\n        json.dump(in_gcp_sa_json, f, ensure_ascii=False)\n\n    df_churn
          = pd.read_csv(df_churn_ip)\n    df_churn.dropna(inplace=True)\n\n    y1
          = df_churn[\"Churn\"]\n    X1 = df_churn.drop([\"Churn\"], axis=1)\n\n    X_train,
          X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n\n    clfxg
          = xgb.XGBClassifier(\n        objective=\"binary:logistic\", verbosity=0,
          max_depth=2, eta=1, silent=0\n    )\n    clfxg.fit(X_train, y_train)\n\n    y_test_pred
          = clfxg.predict(X_test)\n\n    y_test_proba = clfxg.predict_proba(X_test)[:,
          0]\n\n    xgb_score = float(\"%.4f\" % accuracy_score(y_test, y_test_pred))\n    xgb_precision
          = float(\"%.4f\" % precision_score(y_test, y_test_pred))\n    xgb_recall
          = float(\"%.4f\" % recall_score(y_test, y_test_pred))\n    xgb_f1 = float(\"%.4f\"
          % f1_score(y_test, y_test_pred))\n\n    print(\"Accuracy, Precision, Recall,
          f1: \")\n    print(xgb_score, xgb_precision, xgb_recall, xgb_f1)\n\n    cm
          = confusion_matrix(y_test, y_test_pred)\n    print(\"Confusion Matrix: {}\".format(cm))\n\n    fpr,
          tpr, thresholds = roc_curve(y_test, y_test_proba)\n    auc_score = float(\"%.4f\"
          % roc_auc_score(y_test, y_test_proba))\n    print(\"Auc score: \")\n    print(auc_score)\n\n    #
          Converting the matrix to a Dataframe\n    flags = {0: \"Not Churned\", 1:
          \"Churned\"}\n    flag_list = [\"Not Churned\", \"Churned\"]\n    data =
          []\n    for target_index, target_row in enumerate(cm):\n        for predicted_index,
          count in enumerate(target_row):\n            data.append((flags[target_index],
          flags[predicted_index], count))\n\n    df_cm = pd.DataFrame(data, columns=[\"target\",
          \"predicted\", \"count\"])\n    print(df_cm)\n\n    with file_io.FileIO(conf_matr,
          \"w\") as f:\n        df_cm.to_csv(\n            f, columns=[\"target\",
          \"predicted\", \"count\"], header=False, index=False\n        )\n\n    fs
          = gcsfs.GCSFileSystem(project=in_gcp_bucket_project)\n\n    with file_io.FileIO(in_gcp_bucket_output_path
          + \"Artifacts/XGBConf_mat.csv\", \"w\") as f:\n        df_cm.to_csv(\n            f,
          columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n        )\n\n    #
          roc curve\n    df_roc = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"thresholds\":
          thresholds})\n    with file_io.FileIO(in_gcp_bucket_output_path + \"Artifacts/XGBROC_curve.csv\",
          \"w\") as f:\n        df_roc.to_csv(\n            f, columns=[\"fpr\", \"tpr\",
          \"thresholds\"], header=False, index=False\n        )\n\n    # code to generate
          artifacts\n\n    # Artifact generator - metadata\n    metadata = {\n        \"version\":
          1,\n        \"outputs\": [\n            {\n                \"type\": \"confusion_matrix\",\n                \"format\":
          \"csv\",\n                \"storage\": \"gcs\",\n                \"schema\":
          [\n                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n                    {\"name\":
          \"predicted\", \"type\": \"CATEGORY\"},\n                    {\"name\":
          \"count\", \"type\": \"NUMBER\"},\n                ],\n                \"source\":
          in_gcp_bucket_output_path\n                + \"Artifacts/XGBConf_mat.csv\",  #
          conf_matr\n                # Convert flags to string because for bealean
          values we want \"True|False\" to match csv data.\n                \"labels\":
          flag_list,\n            },\n            {\n                \"type\": \"roc\",\n                \"format\":
          \"csv\",\n                \"storage\": \"gcs\",\n                \"schema\":
          [\n                    {\"name\": \"fpr\", \"type\": \"NUMBER\"},\n                    {\"name\":
          \"tpr\", \"type\": \"NUMBER\"},\n                    {\"name\": \"thresholds\",
          \"type\": \"NUMBER\"},\n                ],\n                \"source\":
          in_gcp_bucket_output_path + \"Artifacts/XGBROC_curve.csv\",\n            },\n        ],\n    }\n\n    with
          file_io.FileIO(\"/mlpipeline-ui-metadata.json\", \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata, \"w\") as f:\n        json.dump(metadata,
          f)\n\n    metrics = {\n        \"metrics\": [\n            {\n                \"name\":
          \"accuracy-score\",  # The name of the metric. Visualized as the column
          name in the runs table.\n                \"numberValue\": xgb_score,  #
          The value of the metric. Must be a numeric value.\n                \"format\":
          \"RAW\",  # The optional format of the metric. Supported values are \"RAW\"
          (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n            },\n            {\n                \"name\":
          \"precision-score\",  # The name of the metric. Visualized as the column
          name in the runs table.\n                \"numberValue\": xgb_precision,  #
          The value of the metric. Must be a numeric value.\n                \"format\":
          \"RAW\",  # The optional format of the metric. Supported values are \"RAW\"
          (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n            },\n            {\n                \"name\":
          \"recall\",  # The name of the metric. Visualized as the column name in
          the runs table.\n                \"numberValue\": xgb_recall,  # The value
          of the metric. Must be a numeric value.\n                \"format\": \"RAW\",  #
          The optional format of the metric. Supported values are \"RAW\" (displayed
          in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n            },\n            {\n                \"name\":
          \"f1-score\",  # The name of the metric. Visualized as the column name in
          the runs table.\n                \"numberValue\": xgb_f1,  # The value of
          the metric. Must be a numeric value.\n                \"format\": \"RAW\",  #
          The optional format of the metric. Supported values are \"RAW\" (displayed
          in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n            },\n            {\n                \"name\":
          \"auc-score\",  # The name of the metric. Visualized as the column name
          in the runs table.\n                \"numberValue\": auc_score,  # The value
          of the metric. Must be a numeric value.\n                \"format\": \"RAW\",  #
          The optional format of the metric. Supported values are \"RAW\" (displayed
          in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n            },\n        ]\n    }\n\n    with
          file_io.FileIO(\"/mlpipeline-metrics.json\", \"w\") as f:\n        json.dump(metrics,
          f)\n\n    with file_io.FileIO(mlpipeline_metrics, \"w\") as f:\n        json.dump(metrics,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Xgb
          model'', description='''')\n_parser.add_argument(\"--df-churn-ip\", dest=\"df_churn_ip\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--n-estimators\",
          dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--verbosity\",
          dest=\"verbosity\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--max-depth\",
          dest=\"max_depth\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eta\",
          dest=\"eta\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--silent\",
          dest=\"silent\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-project\",
          dest=\"in_gcp_bucket_project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-sa-json\",
          dest=\"in_gcp_sa_json\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--in-gcp-bucket-output-path\",
          dest=\"in_gcp_bucket_output_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--conf-matr\",
          dest=\"conf_matr\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\",
          dest=\"mlpipeline_metrics\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = xgb_model(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "df_churn_ip"}, {"name": "n_estimators", "type": "Integer"}, {"name": "verbosity",
          "type": "Integer"}, {"name": "max_depth", "type": "Integer"}, {"name": "eta",
          "type": "Integer"}, {"name": "silent", "type": "Integer"}, {"name": "in_gcp_bucket_project",
          "type": "String"}, {"name": "in_gcp_sa_json", "type": "JsonObject"}, {"name":
          "in_gcp_bucket_output_path", "type": "String"}], "name": "Xgb model", "outputs":
          [{"name": "conf_matr"}, {"name": "mlpipeline_ui_metadata"}, {"name": "mlpipeline_metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"eta":
          "{{inputs.parameters.in_eta}}", "in_gcp_bucket_output_path": "{{inputs.parameters.in_output_files_path}}",
          "in_gcp_bucket_project": "{{inputs.parameters.in_gcp_bucket_project}}",
          "in_gcp_sa_json": "{{inputs.parameters.in_gcp_service_account}}", "max_depth":
          "{{inputs.parameters.in_max_depth}}", "n_estimators": "{{inputs.parameters.in_n_estimators}}",
          "silent": "{{inputs.parameters.in_silent}}", "verbosity": "{{inputs.parameters.in_verbosity}}"}'}
  arguments:
    parameters:
    - {name: in_n_estimators, value: '100'}
    - {name: in_verbosity, value: '0'}
    - {name: in_max_depth, value: '2'}
    - {name: in_eta, value: '1'}
    - {name: in_silent, value: '0'}
    - {name: in_input_files_path, value: 'gs://bucket_name/'}
    - {name: in_output_files_path, value: 'gs://bucket_name/Telco_runs/'}
    - {name: in_gcp_bucket_project, value: gcp_project}
    - {name: in_gcp_service_account, value: '{"auth_provider_x509_cert_url": "XXX",
        "auth_uri": "XXX", "client_email": "XXX", "client_id": "XXX", "client_x509_cert_url":
        "XXX", "private_key": "-----BEGIN PRIVATE KEY-----\nXXX\n-----END PRIVATE
        KEY-----\n", "private_key_id": "XXX", "project_id": "XXX", "token_uri": "XXX",
        "type": "service_account"}'}
  serviceAccountName: pipeline-runner
