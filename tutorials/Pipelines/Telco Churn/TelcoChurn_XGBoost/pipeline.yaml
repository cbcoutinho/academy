"apiVersion": |-
  argoproj.io/v1alpha1
"kind": |-
  Workflow
"metadata":
  "annotations":
    "pipelines.kubeflow.org/pipeline_spec": |-
      {"description": "Churn predictions using XGBoost Algorithm", "inputs": [{"default": "gs://pipelines_artifacts/Artifacts/Data.csv", "name": "file_path", "optional": true}, {"default": "100", "name": "n_estimators", "optional": true}, {"default": "0", "name": "verbosity", "optional": true}, {"default": "2", "name": "max_depth", "optional": true}, {"default": "1", "name": "eta", "optional": true}, {"default": "0", "name": "silent", "optional": true}], "name": "Telco Merchant Churn Prediction Pipeline"}
  "generateName": |-
    telco-merchant-churn-prediction-pipeline-
"spec":
  "arguments":
    "parameters":
    - "name": |-
        file_path
      "value": |-
        gs://pipelines_artifacts/Artifacts/Data.csv
    - "name": |-
        n_estimators
      "value": |-
        100
    - "name": |-
        verbosity
      "value": |-
        0
    - "name": |-
        max_depth
      "value": |-
        2
    - "name": |-
        eta
      "value": |-
        1
    - "name": |-
        silent
      "value": |-
        0
  "entrypoint": |-
    telco-merchant-churn-prediction-pipeline
  "serviceAccountName": |-
    pipeline-runner
  "templates":
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef categorical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n    from sklearn.ensemble import RandomForestClassifier\n\n    import\
        \ warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,\
        \ index=False)\n\n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    #Categorical Analysis\n\n    #Churn Plot\n\
        \n    ax = sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.0, aspect=3.0,\
        \ palette = 'bright',\n                     legend = True)\n\n    fig = plt.gcf()\n\
        \    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html',\
        \ 'w') as f:\n        f.write(s)\n\n    def barplot_percentages(feature, orient='v',\
        \ axis_name=\"percentage of customers\"):\n        ratios = pd.DataFrame()\n\
        \        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n  \
        \      g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n      \
        \  g[axis_name] = g[axis_name]/len(df)\n        if orient == 'v':\n      \
        \      ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient,\
        \ palette = 'bright')\n            ax.set_yticklabels(['{:,.0%}'.format(y)\
        \ for y in ax.get_yticks()])\n        else:\n            ax = sns.barplot(x=\
        \ axis_name, y=feature, hue='Churn', data=g, orient=orient, palette = 'bright')\n\
        \            ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n\
        \        ax.plot()\n\n    #Genders \n\n    df['churn_rate'] = df['Churn'].replace(\"\
        No\", 0).replace(\"Yes\", 1)\n    g = sns.FacetGrid(df, col=\"SeniorCitizen\"\
        , height=4, aspect=.9)\n    ax = g.map(sns.barplot, \"gender\", \"churn_rate\"\
        , palette = \"bright\", order= ['Female', 'Male'])\n\n    fig = plt.gcf()\n\
        \    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Genders.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Multiple Lines\n\n    fig = plt.figure(figsize=(9,\
        \ 4.5))\n    barplot_percentages(\"MultipleLines\", orient='v')\n\n    fig\
        \ = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Multiple_lines.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Service-wise Columns analysis\n\n\
        \    cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"\
        TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n    df1 = pd.melt(df[df[\"\
        InternetService\"] != \"No\"][cols]).rename({'value': 'Has service'}, axis=1)\n\
        \    plt.figure(figsize=(10, 4.5))\n    ax = sns.countplot(data=df1, x='variable',\
        \ hue='Has service', palette = 'bright')\n    ax.set(xlabel='Additional service',\
        \ ylabel='Num of customers')\n\n    fig = plt.gcf()\n\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Service-wise Columns analysis2\n\n\
        \    plt.figure(figsize=(10, 4.5))\n    df1 = df[(df.InternetService != \"\
        No\") & (df.Churn == \"Yes\")]\n    df1 = pd.melt(df1[cols]).rename({'value':\
        \ 'Has service'}, axis=1)\n    ax = sns.countplot(data=df1, x='variable',\
        \ hue='Has service', hue_order=['No', 'Yes'], palette = 'bright')\n    ax.set(xlabel='Additional\
        \ service', ylabel='Num of churns')\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis2.html',\
        \ 'w') as f:\n        f.write(s)\n\n    #Generating Metadata\n    metadata\
        \ = {\n        'version' : 1, \n        'outputs' : [{\n            'type'\
        \ : 'web-app',\n            'storage' : 'gcs',\n            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html'\n\
        \            }, \n            {        \n          'type': 'web-app',\n  \
        \        'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Genders.html\"\
        ,\n        },\n            {        \n          'type': 'web-app',\n     \
        \     'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Multiple_lines.html\"\
        ,\n        },\n            {        \n          'type': 'web-app',\n     \
        \     'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis.html\"\
        ,\n        },\n            {        \n          'type': 'web-app',\n     \
        \     'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Servicewise_analysis2.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Categorical analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = categorical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Categorical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      categorical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-ip2
      - |-
        /tmp/inputs/df_churn_ip2/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef mixed_analysis(df_churn_ip , df_churn_ip2 , \n                   mlpipeline_ui_metadata\
        \ , df_churn_op ):\n\n    ## Import Required Libraries\n    import pandas\
        \ as pd\n    import numpy as np\n    import gcsfs\n    import seaborn as sns\n\
        \    import matplotlib.pyplot as plt\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n    df2 = pd.read_csv(df_churn_ip2)\n\
        \n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op, index=False)\n\
        \n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    df['total_charges_to_tenure_ratio'] = df['TotalCharges']\
        \ / df['tenure']\n    df['monthly_charges_diff'] = df['MonthlyCharges'] -\
        \ df['total_charges_to_tenure_ratio']\n    df['churn_rate'] = df['Churn'].replace(\"\
        No\", 0).replace(\"Yes\", 1)\n\n    #Internet Service vs Monthly Charges\n\
        \n    ax = sns.catplot(x=\"InternetService\", y=\"MonthlyCharges\", hue=\"\
        Churn\", kind=\"violin\",\n                     split=True, palette=\"pastel\"\
        , data=df, height=4.2, aspect=1.4);\n\n    fig = plt.gcf()\n    s = mpld3.fig_to_html(fig)\n\
        \n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/violinplot2.html',\
        \ 'w') as f:\n        f.write(s)\n\n    metadata = {\n        'version' :\
        \ 1, \n        'outputs' : [{        \n          'type': 'web-app',\n    \
        \      'storage': 'gcs',\n          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/violinplot2.html\"\
        ,\n        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Mixed analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-ip2\", dest=\"df_churn_ip2\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--df-churn-op\"\
        , dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = mixed_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
      - "name": |-
          categorical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip2/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_ip2"}], "name": "Mixed analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      mixed-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'gcsfs' 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'seaborn==0.9.0' 'matplotlib==3.1.1' 'mpld3==0.5.1' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef numerical_analysis(df_churn_ip , mlpipeline_ui_metadata , df_churn_op\
        \ ):\n\n    ## Import Required Libraries\n    import pandas as pd\n    import\
        \ numpy as np\n    import gcsfs\n    import seaborn as sns\n    import matplotlib.pyplot\
        \ as plt\n    from sklearn.ensemble import RandomForestClassifier\n\n    import\
        \ warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\
        \    warnings.simplefilter(action='ignore', category=UserWarning)\n\n    import\
        \ mpld3\n    from tensorflow.python.lib.io import file_io\n    import json\n\
        \n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n\
        \n    df = pd.read_csv(df_churn_ip)\n\n    df1 = df.copy(deep=True)\n    df1.to_csv(df_churn_op,\
        \ index=False)\n\n    sns.set(style=\"white\")\n    df['TotalCharges'] = df['TotalCharges'].replace(\"\
        \ \", 0).astype('float32')\n\n    #kdeplots - tenure, Monthly Charges, Total\
        \ Charges\n\n    def kdeplot(feature):\n        fig = plt.figure(figsize=(9,\
        \ 4))\n        plt.title(\"KDE for {}\".format(feature))\n        ax0 = sns.kdeplot(df[df['Churn']\
        \ == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No')\n       \
        \ ax1 = sns.kdeplot(df[df['Churn'] == 'Yes'][feature].dropna(), color= 'orange',\
        \ label= 'Churn: Yes')\n\n        fig = plt.gcf()\n        s = mpld3.fig_to_html(fig)\n\
        \n        with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/{}.html'.format(feature),\
        \ 'w') as f:\n            f.write(s)\n\n    kdeplot('TotalCharges')\n\n  \
        \  metadata = {\n        'version' : 1, \n        'outputs' : [{        \n\
        \          'type': 'web-app',\n          'storage': 'gcs',\n          'source':\
        \ \"gs://pipelines_artifacts/Artifacts/Graphs/TotalCharges.html\",\n     \
        \   }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Numerical analysis', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = numerical_analysis(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          mixed-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}], "name": "Numerical analysis", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "df_churn_op"}]}
    "name": |-
      numerical-analysis
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --df-churn-imputed
      - |-
        /tmp/inputs/df_churn_imputed/data
      - |-
        --df-one-hot
      - |-
        /tmp/outputs/df_one_hot/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'imbalanced-learn==0.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'imbalanced-learn==0.6.2' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def one_hot_encode(df_churn_ip , df_churn_imputed , df_one_hot ):

            import pandas as pd
            import numpy as np

            df_churn = pd.read_csv(df_churn_ip)
            df_churn_imp = pd.read_csv(df_churn_imputed)
            empty_cols = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',
                   'tenure', 'PhoneService', 'MultipleLines', 'InternetService',
                   'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',
                   'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                   'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']

            for i in empty_cols:
                df_churn[i]=df_churn[i].replace(" ",np.nan)

            df_churn.drop(['customerID'], axis=1, inplace=True)
            df_churn = df_churn.dropna()
            binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']

            for i in binary_cols:
                df_churn[i] = df_churn[i].replace({"Yes":1,"No":0})

            #Encoding column 'gender'
            df_churn['gender'] = df_churn['gender'].replace({"Male":1,"Female":0})

            category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',
                           'OnlineBackup','DeviceProtection',
                           'TechSupport','StreamingTV','StreamingMovies','Contract']

            for cc in category_cols:
                dummies = pd.get_dummies(df_churn[cc], drop_first=False)
                dummies = dummies.add_prefix("{}#".format(cc))
                df_churn.drop(cc, axis=1, inplace=True)
                df_churn = df_churn.join(dummies)

            df_churn_targets = df_churn['Churn'].unique()
            df_churn['Churn'] = df_churn['Churn'].replace({"Yes":1,"No":0})

            df_churn.to_csv(df_one_hot, index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='One hot encode', description='')
        _parser.add_argument("--df-churn-ip", dest="df_churn_ip", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-churn-imputed", dest="df_churn_imputed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--df-one-hot", dest="df_one_hot", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = one_hot_encode(**_parsed_args)

        _output_serializers = [

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          numerical-analysis-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_imputed/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/inputs/df_churn_ip/data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "df_churn_imputed"}], "name": "One hot encode", "outputs": [{"name": "df_one_hot"}]}
    "name": |-
      one-hot-encode
    "outputs":
      "artifacts":
      - "name": |-
          one-hot-encode-df_one_hot
        "path": |-
          /tmp/outputs/df_one_hot/data
  - "container":
      "args":
      - |-
        --file-name
      - |-
        {{inputs.parameters.file_path}}
      - |-
        --df-churn-op
      - |-
        /tmp/outputs/df_churn_op/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef read_data(file_name , df_churn_op , mlpipeline_ui_metadata ): \n\n \
        \   ## Import Required Libraries\n    import pandas as pd\n    import numpy\
        \ as np\n    import gcsfs\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic',\
        \ token = 'cloud')\n\n    df_churn = pd.read_csv(file_name)\n    df_churn.to_csv(df_churn_op,\
        \ index=False)\n\n    #A DataFrame too long cannot be displayed in the Artifacts\n\
        \n    df_disp = df_churn.iloc[0:5]\n    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n\
        \n    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv',\
        \ index=False)\n\n    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\"\
        )\n    categorical_cols = [c for c in df_show.columns if df_show[c].dtype\
        \ == 'object' or c == 'SeniorCitizen']\n\n    numerical_cols = ['tenure',\
        \ 'MonthlyCharges', 'TotalCharges']\n\n    schema = [{'name':c, 'type': 'CATEGORY'if\
        \ c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n\n    metadata\
        \ = {\n        'outputs' : [{\n          'type': 'table',\n          'storage':\
        \ 'gcs',\n          'format': 'csv',\n          'header': [x['name'] for x\
        \ in schema],\n          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n\
        \        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Read data', description='')\n_parser.add_argument(\"\
        --file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--df-churn-op\", dest=\"df_churn_op\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = read_data(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "parameters":
      - "name": |-
          file_path
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "file_name", "type": "String"}], "name": "Read data", "outputs": [{"name": "df_churn_op"}, {"name": "mlpipeline_ui_metadata"}]}
    "name": |-
      read-data
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          read-data-df_churn_op
        "path": |-
          /tmp/outputs/df_churn_op/data
  - "dag":
      "tasks":
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          read-data
        "name": |-
          categorical-analysis
        "template": |-
          categorical-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.categorical-analysis.outputs.artifacts.categorical-analysis-df_churn_op}}
            "name": |-
              categorical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          categorical-analysis
        - |-
          read-data
        "name": |-
          mixed-analysis
        "template": |-
          mixed-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.mixed-analysis.outputs.artifacts.mixed-analysis-df_churn_op}}
            "name": |-
              mixed-analysis-df_churn_op
        "dependencies":
        - |-
          mixed-analysis
        "name": |-
          numerical-analysis
        "template": |-
          numerical-analysis
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.numerical-analysis.outputs.artifacts.numerical-analysis-df_churn_op}}
            "name": |-
              numerical-analysis-df_churn_op
          - "from": |-
              {{tasks.read-data.outputs.artifacts.read-data-df_churn_op}}
            "name": |-
              read-data-df_churn_op
        "dependencies":
        - |-
          numerical-analysis
        - |-
          read-data
        "name": |-
          one-hot-encode
        "template": |-
          one-hot-encode
      - "arguments":
          "parameters":
          - "name": |-
              file_path
            "value": |-
              {{inputs.parameters.file_path}}
        "name": |-
          read-data
        "template": |-
          read-data
      - "arguments":
          "artifacts":
          - "from": |-
              {{tasks.one-hot-encode.outputs.artifacts.one-hot-encode-df_one_hot}}
            "name": |-
              one-hot-encode-df_one_hot
          "parameters":
          - "name": |-
              eta
            "value": |-
              {{inputs.parameters.eta}}
          - "name": |-
              max_depth
            "value": |-
              {{inputs.parameters.max_depth}}
          - "name": |-
              n_estimators
            "value": |-
              {{inputs.parameters.n_estimators}}
          - "name": |-
              silent
            "value": |-
              {{inputs.parameters.silent}}
          - "name": |-
              verbosity
            "value": |-
              {{inputs.parameters.verbosity}}
        "dependencies":
        - |-
          one-hot-encode
        "name": |-
          xgb-model
        "template": |-
          xgb-model
    "inputs":
      "parameters":
      - "name": |-
          eta
      - "name": |-
          file_path
      - "name": |-
          max_depth
      - "name": |-
          n_estimators
      - "name": |-
          silent
      - "name": |-
          verbosity
    "name": |-
      telco-merchant-churn-prediction-pipeline
  - "container":
      "args":
      - |-
        --df-churn-ip
      - |-
        /tmp/inputs/df_churn_ip/data
      - |-
        --n-estimators
      - |-
        {{inputs.parameters.n_estimators}}
      - |-
        --verbosity
      - |-
        {{inputs.parameters.verbosity}}
      - |-
        --max-depth
      - |-
        {{inputs.parameters.max_depth}}
      - |-
        --eta
      - |-
        {{inputs.parameters.eta}}
      - |-
        --silent
      - |-
        {{inputs.parameters.silent}}
      - |-
        --conf-matr
      - |-
        /tmp/outputs/conf_matr/data
      - |-
        --mlpipeline-ui-metadata
      - |-
        /tmp/outputs/mlpipeline_ui_metadata/data
      - |-
        --mlpipeline-metrics
      - |-
        /tmp/outputs/mlpipeline_metrics/data
      "command":
      - |-
        sh
      - |-
        -c
      - |-
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.22.2' 'numpy==1.17.2' 'pandas==1.0.3' 'xgboost==1.0.2' 'gcsfs' --user) && "$0" "$@"
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef xgb_model(df_churn_ip , \n              n_estimators , verbosity , max_depth\
        \ , eta , silent , \n              conf_matr ,\n              mlpipeline_ui_metadata\
        \ , mlpipeline_metrics ):\n\n    import pandas as pd\n    import numpy as\
        \ np\n    from sklearn.model_selection import train_test_split\n    import\
        \ xgboost as xgb\n    from sklearn.metrics import confusion_matrix, accuracy_score,\
        \ roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n    import\
        \ json\n    import os\n    import gcsfs\n    from tensorflow.python.lib.io\
        \ import file_io\n\n    df_churn = pd.read_csv(df_churn_ip)\n    df_churn.dropna(inplace=True)\n\
        \n    y1 = df_churn['Churn']\n    X1 = df_churn.drop(['Churn'],axis=1)\n\n\
        \    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n\
        \n    clfxg = xgb.XGBClassifier(objective='binary:logistic', verbosity=0,\
        \ max_depth=2, eta = 1, silent=0)\n    clfxg.fit(X_train, y_train)\n\n   \
        \ y_test_pred = clfxg.predict(X_test)\n\n    y_test_proba = clfxg.predict_proba(X_test)[:,0]\n\
        \n    xgb_score = float('%.4f' %accuracy_score(y_test, y_test_pred))   \n\
        \    xgb_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n\
        \    xgb_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n    xgb_f1\
        \ = float('%.4f' %f1_score(y_test, y_test_pred))\n\n    print(\"Accuracy,\
        \ Precision, Recall, f1: \")\n    print(xgb_score, xgb_precision, xgb_recall,\
        \ xgb_f1)\n\n    cm = confusion_matrix(y_test, y_test_pred)\n    print(\"\
        Confusion Matrix: {}\".format(cm))\n\n    fpr, tpr, thresholds = roc_curve(y_test,\
        \ y_test_proba) \n    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n\
        \    print('Auc score: ')\n    print(auc_score)\n\n    #Converting the matrix\
        \ to a Dataframe\n\n    flags = {0:'Not Churned',1:'Churned'}\n    flag_list\
        \ = ['Not Churned','Churned']\n    data = []\n    for target_index, target_row\
        \ in enumerate(cm):\n        for predicted_index, count in enumerate(target_row):\n\
        \            data.append((flags[target_index], flags[predicted_index], count))\n\
        \n    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n\
        \    print(df_cm)\n\n    with file_io.FileIO(conf_matr, 'w') as f:\n     \
        \   df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False,\
        \ index=False)\n\n    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token\
        \ = 'cloud')\n\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBConf_mat.csv',\
        \ 'w') as f:\n        df_cm.to_csv(f, columns=['target', 'predicted', 'count'],\
        \ header=False, index=False)\n\n    #roc curve\n\n    df_roc = pd.DataFrame({'fpr':\
        \ fpr, 'tpr': tpr, 'thresholds': thresholds})\n    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/XGBROC_curve.csv',\
        \ 'w') as f:\n        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'],\
        \ header=False, index=False)\n\n    #code to generate artifacts\n\n    #Artifact\
        \ generator - metadata\n\n    from tensorflow.python.lib.io import file_io\n\
        \    import json\n\n    metadata = {\n        'version' : 1, \n        'outputs'\
        \ : [{\n            'type': 'confusion_matrix',\n            'format': 'csv',\n\
        \            'storage': 'gcs',\n            'schema': [\n                {'name':\
        \ 'target', 'type': 'CATEGORY'},\n                {'name': 'predicted', 'type':\
        \ 'CATEGORY'},\n                {'name': 'count', 'type': 'NUMBER'},\n   \
        \         ],\n            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matXGB.csv',\
        \ #conf_matr\n\n       # Convert flags to string because for bealean values\
        \ we want \"True|False\" to match csv data.\n            'labels': flag_list\n\
        \        },    \n        {\n          'type': 'roc',\n          'format':\
        \ 'csv',\n          'storage': 'gcs',\n          'schema': [\n           \
        \ {'name': 'fpr', 'type': 'NUMBER'},\n            {'name': 'tpr', 'type':\
        \ 'NUMBER'},\n            {'name': 'thresholds', 'type': 'NUMBER'},\n    \
        \      ],\n          'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveXGB.csv'\n\
        \        }\n        ]\n    }\n\n    with file_io.FileIO('/mlpipeline-ui-metadata.json',\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    with file_io.FileIO(mlpipeline_ui_metadata,\
        \ 'w') as f:\n        json.dump(metadata, f)\n\n    metrics = {\n    'metrics':\
        \ [{\n      'name': 'accuracy-score', # The name of the metric. Visualized\
        \ as the column name in the runs table.\n      'numberValue':  xgb_score,\
        \ # The value of the metric. Must be a numeric value.\n      'format': \"\
        RAW\",   # The optional format of the metric. Supported values are \"RAW\"\
        \ (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n\
        \    },\n    {\n      'name': 'precision-score', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ xgb_precision, # The value of the metric. Must be a numeric value.\n   \
        \   'format': \"RAW\",   # The optional format of the metric. Supported values\
        \ are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'recall', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ xgb_recall, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'f1-score', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ xgb_f1, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    },\n    {\n      'name': 'auc-score', # The name of the metric.\
        \ Visualized as the column name in the runs table.\n      'numberValue': \
        \ auc_score, # The value of the metric. Must be a numeric value.\n      'format':\
        \ \"RAW\",   # The optional format of the metric. Supported values are \"\
        RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage\
        \ format).\n    }]\n    }\n\n    with file_io.FileIO('/mlpipeline-metrics.json',\
        \ 'w') as f:\n        json.dump(metrics, f)\n\n    with file_io.FileIO(mlpipeline_metrics,\
        \ 'w') as f:\n        json.dump(metrics, f)\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Xgb model', description='')\n_parser.add_argument(\"\
        --df-churn-ip\", dest=\"df_churn_ip\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--n-estimators\", dest=\"n_estimators\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--verbosity\"\
        , dest=\"verbosity\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--max-depth\", dest=\"max_depth\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--eta\", dest=\"eta\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --silent\", dest=\"silent\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--conf-matr\", dest=\"conf_matr\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\"\
        , dest=\"mlpipeline_ui_metadata\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\"\
        , dest=\"mlpipeline_metrics\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = xgb_model(**_parsed_args)\n\
        \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "image": |-
        tensorflow/tensorflow:1.13.2-py3
    "inputs":
      "artifacts":
      - "name": |-
          one-hot-encode-df_one_hot
        "path": |-
          /tmp/inputs/df_churn_ip/data
      "parameters":
      - "name": |-
          eta
      - "name": |-
          max_depth
      - "name": |-
          n_estimators
      - "name": |-
          silent
      - "name": |-
          verbosity
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "df_churn_ip"}, {"name": "n_estimators", "type": "Integer"}, {"name": "verbosity", "type": "Integer"}, {"name": "max_depth", "type": "Integer"}, {"name": "eta", "type": "Integer"}, {"name": "silent", "type": "Integer"}], "name": "Xgb model", "outputs": [{"name": "conf_matr"}, {"name": "mlpipeline_ui_metadata"}, {"name": "mlpipeline_metrics"}]}
    "name": |-
      xgb-model
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /tmp/outputs/mlpipeline_ui_metadata/data
      - "name": |-
          mlpipeline-metrics
        "path": |-
          /tmp/outputs/mlpipeline_metrics/data
      - "name": |-
          xgb-model-conf_matr
        "path": |-
          /tmp/outputs/conf_matr/data
