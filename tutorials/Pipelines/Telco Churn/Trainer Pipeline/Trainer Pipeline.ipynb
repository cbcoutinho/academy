{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Pipeline for Artifacts Generation, File Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Write Files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_write_files_gcs(project_name: str, bucket_name: str, file_directory_path: str, filename: str, \n",
    "                         outfilename: str, outjsonname: str): \n",
    "    \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    \n",
    "    #GCSFS library is required to sign into the Project and Access the Storage Bucket filesystem\n",
    "    #This needs to be done for each and every Pipeline block which needs to read/write to the bucket\n",
    "    #This is because each Pipeline block runs as a independent Kubernetes pod\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project=project_name, token = 'cloud')\n",
    "    \n",
    "    file_path = 'gs://' + bucket_name + '/' + file_directory_path + '/' + filename\n",
    "    \n",
    "    #read input\n",
    "    #pd.read_csv works simply by giving the GCS filepath\n",
    "    df_sample = pd.read_csv(file_path)\n",
    "    \n",
    "    #Printed Outputs can be viewed in the Pipeline Block 'Logs' section\n",
    "    print(\"\\n\\nInput File:\\n\")\n",
    "    print(df_sample)\n",
    "    print('Shape: {}'.format(df_sample.shape))\n",
    "\n",
    "    df = pd.DataFrame({'Name':['Alex','Lina','Ameya','Martin'], 'Age':[21,19,20,18],\n",
    "                   'Plays Football':['Yes','Yes','Yes','No']})\n",
    "\n",
    "    print(\"\\n\\nOutput File:\\n\")\n",
    "    print(df)\n",
    "    print('Shape: {}'.format(df.shape))\n",
    "\n",
    "    \n",
    "    file_out_path = 'gs://' + bucket_name + '/' + file_directory_path + '/' + outfilename\n",
    "    \n",
    "    #Output csv\n",
    "    #Keep index as False, to prevent an additional index row from getting added each time the file is read and written to\n",
    "    df.to_csv(file_out_path, index=False)\n",
    "    \n",
    "    #Output json using file_io\n",
    "    json_out_path = 'gs://' + bucket_name + '/' + file_directory_path + '/' + outjsonname\n",
    "    \n",
    "    \n",
    "    s = {'name':'file1', 'outputs':'None','Format':'Json'}\n",
    "    \n",
    "    #write to GCS\n",
    "    with file_io.FileIO(json_out_path, 'w') as f:\n",
    "        json.dump(s, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func_to_container_op packages the python function into a Pipeline Block which runs as a Kubernetes Pod\n",
    "\n",
    "kfp_read_write_gcs = kfp.components.func_to_container_op(func = read_write_files_gcs, \n",
    "                                                          output_component_file = './read-write-gcs.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n",
    "                                                        #Be mindful of Package versions and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output files to Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def exchange_files_within_pipeline_blocks1(project_name: str, bucket_name: str, file_directory_path: str, \n",
    "                                           filename: str, outfile: OutputPath()):\n",
    "    #OutputPath files are Output to the Minio pod's Persistent Volume\n",
    "    #Note that the OutputPath variable is still declared as a function input parameter\n",
    "    #Using parameter outputs and File Output (OutputPath()) at the same time does not work\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gcsfs\n",
    "    \n",
    "    #read file from GCS\n",
    "    #Access the bucket file-system\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project=project_name, token = 'cloud')\n",
    "    \n",
    "    file_path = 'gs://' + bucket_name + '/' + file_directory_path + '/' + filename\n",
    "    \n",
    "    #pd.read_csv works simply by giving the GCS filepath\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    #Output the file to Minio as OutputPath\n",
    "    \n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_exchange_files1 = kfp.components.func_to_container_op(func = exchange_files_within_pipeline_blocks1, \n",
    "                                                          output_component_file = './exchange_files1.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Files from Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchange_files_within_pipeline_blocks2(infile: InputPath(),\n",
    "                                           outfile: OutputPath()): \n",
    "    #OutputPath files are Output to the Minio pod's Persistent Volume\n",
    "    #Note that the OutputPath variable is still declared as a function input parameter\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #read file output by first container from the Minio Persistent Volume\n",
    "    df = pd.read_csv(infile)\n",
    "    \n",
    "    print('Input File: \\n')\n",
    "    print(df)\n",
    "    #Do your processing\n",
    "    df_trim = df.iloc[:,0:2]\n",
    "    \n",
    "    print('Output File: \\n')\n",
    "    print(df_trim)\n",
    "    \n",
    "    #Output the processed file\n",
    "    df_trim.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_exchange_files2 = kfp.components.func_to_container_op(func = exchange_files_within_pipeline_blocks2, \n",
    "                                                          output_component_file = './exchange_files2.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artifacts Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_artifacts(project_name: str, bucket_name: str, file_directory_path: str, markdown_filename: str,\n",
    "                       html_filename: str, mlpipeline_ui_metadata: OutputPath()): \n",
    "    #To generate artifacts, a metadata json file has to be created and written to the root level of the container's \n",
    "    #filesystem. Supported artifact types are -  tables, static html, confusion matrix, markdown and ROC curve\n",
    "    #tfdv and tfma are deprecated features\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    import mpld3\n",
    "    \n",
    "    markdown_path = 'gs://' + bucket_name + '/' +file_directory_path + '/' +markdown_filename\n",
    "\n",
    "    #enter the GCS filesystem\n",
    "    fs = gcsfs.GCSFileSystem(project=project_name, token = 'cloud')\n",
    "    \n",
    "    s = '''### This is a Introductory Trainer pipeline to help you understand the basics of -\\n\n",
    "    *Ingesting and Uploading Data to Cloud Storage with Security credentials\\n\n",
    "    *Data Exchange between Pipeline blocks\\n\n",
    "    *Process for generating Artifacts\\n\n",
    "    Please Complete this Pipeline before Proceeding with the Telco Customer Churn pipelines\\n'''\n",
    "    \n",
    "    #create and write the markdown file to gcs\n",
    "    with file_io.FileIO(markdown_path, 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "    \n",
    "    #create a seaborn artifact and write it to gcs\n",
    "    df = pd.DataFrame({'Name':['Alex','Lina','Ameya','Martin'], 'Age':[21,19,20,18],\n",
    "                   'Plays Football':['Yes','Yes','Yes','No']})\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    ax = sns.catplot(y=\"Plays Football\", kind=\"count\", data=df, height=2.0, aspect=3.0, \n",
    "                     legend = True)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    html = mpld3.fig_to_html(fig)\n",
    "    \n",
    "    \n",
    "    html_path = 'gs://' + bucket_name + '/' +file_directory_path + '/' + html_filename\n",
    "    with file_io.FileIO(html_path, 'w') as f:\n",
    "        f.write(html)    \n",
    "\n",
    "            \n",
    "    # A metadata json file has to be generated and dumped to the root level of the container for artifacts to\n",
    "    # be generated. Metadata supports the following artifacts - \n",
    "    #   Static HTML: 'web-app'\n",
    "    #   ROC Curve: 'roc'\n",
    "    #   Confusion Matrix: 'confusion_matrix'\n",
    "    #   Tables: 'table'\n",
    "    #   Markdown: 'markdown'\n",
    "    \n",
    "    # Storage can be 'gcs', 'minio', or 'inline'\n",
    "    # Source file path has to be provided - \n",
    "    # It can either be a GCS or Minio filepath. No need to specify anything for the 'inline' storage\n",
    "    \n",
    "    \n",
    "    # Some artifacts like Confusion Matrix, ROC curve and Table need a 'schema' field for Artifact visualisation\n",
    "    # The schema is basically the column headers and datatypes of the DataFrame being used to generate the Artifacts\n",
    "    \n",
    "    schema = [{'name':'Name', 'type': 'CATEGORY'}, {'name':'Age', 'type':'CATEGORY'}, \n",
    "              {'name':'Plays Football', 'type':'CATEGORY'}]\n",
    "    # This schema is to output the same DataFrame we had created and stored to GCS in the 'read_files_gcs' block\n",
    "    # The field 'name' is the name of the DataFrame column\n",
    "    # The field 'type' is the datatype - only two Datatypes are supported - 'NUMBER' and 'CATEGORY'\n",
    "    \n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, #Kubeflow Version\n",
    "        'outputs' : [\n",
    "        # Markdown that is hardcoded inline\n",
    "        {\n",
    "            'storage': 'inline',\n",
    "            'source': '# Welcome to the Trainer',\n",
    "            'type': 'markdown',\n",
    "        },\n",
    "        # Markdown that is read from a file\n",
    "        {\n",
    "            'storage' : 'gcs',\n",
    "            'source': markdown_path,\n",
    "            'type': 'markdown',\n",
    "        }, #html web-app(for output of matplotlib/seaborn)\n",
    "        {\n",
    "          'type': 'table',\n",
    "          'storage': 'gcs',\n",
    "          'format': 'csv',\n",
    "          'header': [x['name'] for x in schema],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/Player_data.csv'\n",
    "        },\n",
    "        {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': html_path\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    #dump file to root level of container's filesystem\n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    #dump file to Minio Output\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_generate_artifacts = kfp.components.func_to_container_op(func = generate_artifacts, \n",
    "                                                          output_component_file = './generate_artifacts.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs',\n",
    "                                                                                 'mpld3==0.5.1', 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "#kfp.dsl Decorator is used to designate the Pipeline Definition\n",
    "\n",
    "@dsl.pipeline(name='Trainer Pipeline',description='Data and Artifacts processing')\n",
    "def Trainer_func(project_name='YDataSynthetic', bucket_name='pipelines_artifacts', \n",
    "                 file_directory_path='Artifacts', filename='Data_Summary.csv', \n",
    "                 outfilename='Player_data.csv', outjsonname='sample.json', markdown_filename='Instructions.md',\n",
    "                 html_filename='sample_graph.html'):\n",
    "    \n",
    "    #Define the Input Parameters that you'd like for the User to control from the YData Pipelines Dashboard\n",
    "    #in the Pipeline function definition\n",
    "    \n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    read_write_files_gcs_task = kfp_read_write_gcs(project_name, bucket_name, file_directory_path, \n",
    "                                                   filename, outfilename, outjsonname)\n",
    "    exchange_files_task1 = kfp_exchange_files1(project_name, bucket_name, file_directory_path, outfilename)\n",
    "    exchange_files_task2 = kfp_exchange_files2(exchange_files_task1.outputs['outfile'])\n",
    "    generate_artifacts_task = kfp_generate_artifacts(project_name, bucket_name, file_directory_path, \n",
    "                                                     markdown_filename, html_filename)\n",
    "    \n",
    "    #Take care to define the pipeline block execution and the input-output dependancies properly\n",
    "    \n",
    "#For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "#For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_func = Trainer_func\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp            #this Compiler function will compile the Pipeline into .tar.gz format\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "#The ready-to-upload compiled file will be generated in the same folder as this Jupyter notebook\n",
    "#Upload it the the YData Pipelines Dashboard to Run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
