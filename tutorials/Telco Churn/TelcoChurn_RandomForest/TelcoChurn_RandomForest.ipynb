{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn Prediction using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Data file from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read the Data file from GCS Bucket## Read Data\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(file_name: str, df_churn_op: OutputPath(), mlpipeline_ui_metadata: OutputPath()): \n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df_churn = pd.read_csv(file_name)\n",
    "    df_churn.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    #A DataFrame too long cannot be displayed in the Artifacts\n",
    "    \n",
    "    df_disp = df_churn.iloc[0:5]\n",
    "    df_disp = df_disp[['customerID','gender','tenure','Contract','TotalCharges','Churn']]\n",
    " \n",
    "    df_disp.to_csv('gs://pipelines_artifacts/Artifacts/Data_Sample.csv', index=False)\n",
    "    \n",
    "    df_show = pd.read_csv(\"gs://pipelines_artifacts/Artifacts/Data_Sample.csv\")\n",
    "    categorical_cols = [c for c in df_show.columns if df_show[c].dtype == 'object' or c == 'SeniorCitizen']\n",
    "\n",
    "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "    schema = [{'name':c, 'type': 'CATEGORY'if c in categorical_cols else 'NUMBER'} for c in df_show.columns]\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'table',\n",
    "          'storage': 'gcs',\n",
    "          'format': 'csv',\n",
    "          'header': [x['name'] for x in schema],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/Data_Sample.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.func_to_container_op(func = read_data, \n",
    "                                                          output_component_file = './read-data-func.yaml',\n",
    "                                                          packages_to_install = ['numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', 'gcsfs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis and Artifact Generation for Categorical Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df_churn_ip :InputPath(), mlpipeline_ui_metadata: OutputPath(), df_churn_op :OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "\n",
    "\n",
    "    \n",
    "    #Categorical Analysis\n",
    "    \n",
    "    #Churn Plot\n",
    "\n",
    "    ax = sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.0, aspect=3.0, palette = 'bright',\n",
    "                     legend = True)\n",
    "    \n",
    "    #Seaborn Plots cannot be directly viewed on the YData Pipelines Dashboard\n",
    "    #The plot must be converted to .html format, and uploaded to be accessed by the Artifacts Generator\n",
    "    \n",
    "    fig = plt.gcf()    #gcf gets the current figure generated\n",
    "    s = mpld3.fig_to_html(fig)   #mpld3 is a Python library which converts matplotlib/dericative library\n",
    "                                 #plots to html \n",
    "\n",
    "    #write the .html file to your storage bucket using file_io\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "\n",
    "    def barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n",
    "        ratios = pd.DataFrame()\n",
    "        g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n",
    "        g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n",
    "        g[axis_name] = g[axis_name]/len(df)\n",
    "        if orient == 'v':\n",
    "            ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient, palette = 'bright')\n",
    "            ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n",
    "        else:\n",
    "            ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient, palette = 'bright')\n",
    "            ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n",
    "        ax.plot()\n",
    "\n",
    "        \n",
    "    #Partners and Dependents\n",
    "    \n",
    "    fig, axis = plt.subplots(1, 2, figsize=(12,4))\n",
    "    axis[0].set_title(\"Has partner\")\n",
    "    axis[1].set_title(\"Has dependents\")\n",
    "    axis_y = \"percentage of customers\"\n",
    "    # Plot Partner column\n",
    "    gp_partner = df.groupby('Partner')[\"Churn\"].value_counts()/len(df)\n",
    "    gp_partner = gp_partner.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n",
    "    ax = sns.barplot(x='Partner', y= axis_y, hue='Churn', data=gp_partner, ax=axis[0], palette = 'bright')\n",
    "    # Plot Dependents column\n",
    "    gp_dep = df.groupby('Dependents')[\"Churn\"].value_counts()/len(df)\n",
    "    gp_dep = gp_dep.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n",
    "    ax = sns.barplot(x='Dependents', y= axis_y, hue='Churn', data=gp_dep, ax=axis[1], palette = 'bright')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/Partners_dependents.html', 'w') as f:\n",
    "        f.write(s)\n",
    "    \n",
    "    \n",
    "    #A metadata json file has to be generated and dumped to the root level of the container for artifacts to\n",
    "    # be generated. Metadata supports the following artifacts - \n",
    "    # Static HTML\n",
    "    # ROC Curve\n",
    "    # Confusion Matrix\n",
    "    # Tables\n",
    "    # Markdown\n",
    "\n",
    "    #Generating Metadata\n",
    "    metadata = {\n",
    "        'version' : 1,   #Check the Version of your Kubeflow \n",
    "        'outputs' : [{\n",
    "            'type' : 'web-app',\n",
    "            'storage' : 'gcs',\n",
    "            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/churn_plot.html'\n",
    "            }, \n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/Partners_dependents.html\",\n",
    "        },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Output the metadata file, with the exact same name as below to the root level of the Pipeline-block container\n",
    "    # While troubleshooting artifact issues, first check at the container level if the file has been created properly\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    #Also output to Minio as an Output file\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_categorical_analysis = kfp.components.func_to_container_op(func = categorical_analysis, \n",
    "                                                          output_component_file = './categorical_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Statistical Analysis and Artifact Generation for Numerical and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_analysis(df_churn_ip :InputPath(), df_churn_ip2 :InputPath(), \n",
    "                   mlpipeline_ui_metadata: OutputPath(), df_churn_op :OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "    df2 = pd.read_csv(df_churn_ip2)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "    \n",
    "    \n",
    "    df['total_charges_to_tenure_ratio'] = df['TotalCharges'] / df['tenure']\n",
    "    df['monthly_charges_diff'] = df['MonthlyCharges'] - df['total_charges_to_tenure_ratio']\n",
    "    df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "    \n",
    "    \n",
    "    #Multiple-Lines vs Monthly Charges\n",
    "    \n",
    "    ax = sns.catplot(x=\"MultipleLines\", y=\"MonthlyCharges\", hue=\"Churn\", kind=\"violin\",\n",
    "                     split=True, palette=\"pastel\", data=df, height=4.2, aspect=1.4)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/violinplot1.html', 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type' : 'web-app',\n",
    "            'storage' : 'gcs',\n",
    "            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/violinplot1.html'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_mixed_analysis = kfp.components.func_to_container_op(func = mixed_analysis, \n",
    "                                                          output_component_file = './mixed_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis and Artifact Generation for Numerical Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(df_churn_ip :InputPath(), mlpipeline_ui_metadata: OutputPath(), df_churn_op: OutputPath()):\n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    import mpld3\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "\n",
    "    df = pd.read_csv(df_churn_ip)\n",
    "\n",
    "    df1 = df.copy(deep=True)\n",
    "    df1.to_csv(df_churn_op, index=False)\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "    \n",
    "    \n",
    "    #kdeplots - tenure, Monthly Charges, Total Charges\n",
    "    \n",
    "    def kdeplot(feature):\n",
    "        fig = plt.figure(figsize=(9, 4))\n",
    "        plt.title(\"KDE for {}\".format(feature))\n",
    "        ax0 = sns.kdeplot(df[df['Churn'] == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No')\n",
    "        ax1 = sns.kdeplot(df[df['Churn'] == 'Yes'][feature].dropna(), color= 'orange', label= 'Churn: Yes')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        s = mpld3.fig_to_html(fig)\n",
    "\n",
    "        with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/{}.html'.format(feature), 'w') as f:\n",
    "            f.write(s)\n",
    "\n",
    "                    \n",
    "    kdeplot('tenure')\n",
    "\n",
    "    #scatterplot - Monthly and Total Charges vs Tenure\n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    g = sns.PairGrid(df, y_vars=[\"tenure\"], x_vars=[\"MonthlyCharges\", \"TotalCharges\"], height=4.5, hue=\"Churn\", aspect=1.1)\n",
    "    ax = g.map(plt.scatter, alpha=0.6)\n",
    "    fig = plt.gcf()\n",
    "    s = mpld3.fig_to_html(fig)\n",
    "\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Graphs/scatterplot1.html', 'w') as f:\n",
    "        f.write(s)    \n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type' : 'web-app',\n",
    "            'storage' : 'gcs',\n",
    "            'source' : 'gs://pipelines_artifacts/Artifacts/Graphs/tenure.html'\n",
    "            }, \n",
    "            {        \n",
    "          'type': 'web-app',\n",
    "          'storage': 'gcs',\n",
    "          'source': \"gs://pipelines_artifacts/Artifacts/Graphs/scatterplot1.html\",\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_numerical_analysis = kfp.components.func_to_container_op(func = numerical_analysis, \n",
    "                                                          output_component_file = './numerical_analysis.yaml',\n",
    "                                                          packages_to_install = ['gcsfs', 'scikit-learn==0.22.2',\n",
    "                                                                                 'numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'seaborn==0.9.0',\n",
    "                                                                                 'matplotlib==3.1.1',\n",
    "                                                                                 'mpld3==0.5.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(df_churn_ip: InputPath(), df_churn_imputed :InputPath(), df_one_hot: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn_imp = pd.read_csv(df_churn_imputed)\n",
    "    empty_cols = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "    \n",
    "    #Replacing Empty values\n",
    "    for i in empty_cols:\n",
    "        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n",
    "\n",
    "    df_churn.drop(['customerID'], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n",
    "    \n",
    "    #Binary Encoding\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    #Encoding column 'gender'\n",
    "    df_churn['gender'] = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n",
    "\n",
    "\n",
    "    category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n",
    "                   'OnlineBackup','DeviceProtection',\n",
    "                   'TechSupport','StreamingTV','StreamingMovies','Contract']\n",
    "    \n",
    "    #One-hot Encoding of multiple-category columns\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "    \n",
    "    df_churn_targets = df_churn['Churn'].unique()\n",
    "    df_churn['Churn'] = df_churn['Churn'].replace({\"Yes\":1,\"No\":0})\n",
    "    \n",
    "    #Output the encoded file \n",
    "    df_churn.to_csv(df_one_hot, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_one_hot_encode = kfp.components.func_to_container_op(func = one_hot_encode, \n",
    "                                                          output_component_file = './one-hot-encode-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3',\n",
    "                                                                                 'imbalanced-learn==0.6.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Algorithm - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "\n",
    "def rf_model(df_churn_ip: InputPath(), n_estimators: int, max_depth: int, criterion: str, max_features: str,\n",
    "              min_samples_split: int,\n",
    "              conf_matr: OutputPath(),\n",
    "              mlpipeline_ui_metadata: OutputPath(), mlpipeline_metrics: OutputPath()):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "    import json\n",
    "    import os\n",
    "    import gcsfs\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "    n_est = n_estimators\n",
    "    m_dep = max_depth\n",
    "    crit = criterion\n",
    "    m_feat = max_features\n",
    "    min_ss = min_samples_split\n",
    "    \n",
    "    \n",
    "    df_churn = pd.read_csv(df_churn_ip)\n",
    "    df_churn.dropna(inplace=True)\n",
    "\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "    \n",
    "    #Split Data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    #fit the model on the Data and train it\n",
    "    rfc_best = RandomForestClassifier(random_state=42, max_features=m_feat, n_estimators = n_est, \n",
    "                                      max_depth = m_dep, criterion = crit, min_samples_split = min_ss)\n",
    "\n",
    "    rfc_best.fit(X_train, y_train) \n",
    "    y_test_pred = rfc_best.predict(X_test)\n",
    "    y_test_proba = rfc_best.predict_proba(X_test)[:,0]\n",
    "    \n",
    "    #Get Metrics scores\n",
    "    \n",
    "    rf_score = float('%.4f' %rfc_best.score(X_test, y_test))   \n",
    "    rf_precision = float('%.4f' %precision_score(y_test, y_test_pred))\n",
    "    rf_recall = float('%.4f' %recall_score(y_test, y_test_pred))\n",
    "    rf_f1 = float('%.4f' %f1_score(y_test, y_test_pred))\n",
    "    \n",
    "    print(\"Accuraccy, Precision, Recall, f1: \")\n",
    "    print(rf_score, rf_precision, rf_recall, rf_f1)\n",
    "    \n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix: {}\".format(cm))\n",
    "    \n",
    "    #True and False Positive Rates\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba) \n",
    "    auc_score = float('%.4f' %roc_auc_score(y_test, y_test_proba))\n",
    "    print('Auc score: ')\n",
    "    print(auc_score)\n",
    "    \n",
    "    #Converting the Confusion matrix to a Dataframe\n",
    "    #Note that for Generating the Confusion Matrix Artifact, the Confusion Matrix has to be converted from\n",
    "    #a numpy array to a DataFrame in the exact format as given below\n",
    "    \n",
    "    flags = {0:'Not Churned',1:'Churned'}\n",
    "    flag_list = ['Not Churned','Churned']\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((flags[target_index], flags[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    print(df_cm)\n",
    "    \n",
    "    \n",
    "    with file_io.FileIO(conf_matr, 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='YDataSynthetic', token = 'cloud')\n",
    "    \n",
    "    #Save confusion Matrix to GCS\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/Conf_matRF.csv', 'w') as f:\n",
    "        df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "    \n",
    "    \n",
    "    #roc curve\n",
    "    #For generating the ROC curve, the tpr, fpr and thresholds need to be output as a DataFrame in the \n",
    "    #exact format as given below\n",
    "    \n",
    "    df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    with file_io.FileIO('gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv', 'w') as f:\n",
    "        df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n",
    "\n",
    "    \n",
    "    #code to generate artifacts\n",
    "    \n",
    "    #Artifact generator - metadata\n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    metadata = {\n",
    "        'version' : 1, \n",
    "        'outputs' : [{\n",
    "            'type': 'confusion_matrix',\n",
    "            'format': 'csv',\n",
    "            'storage': 'gcs',\n",
    "            'schema': [   #schema is required in the exact same form for generating the artifact\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'source': 'gs://pipelines_artifacts/Artifacts/Conf_matRF.csv', #conf_matr\n",
    "            \n",
    "       # Convert flags to string because for bealean values we want \"True|False\" to match csv data.\n",
    "            'labels': flag_list\n",
    "        },    \n",
    "        {\n",
    "          'type': 'roc',\n",
    "          'format': 'csv',\n",
    "          'storage': 'gcs',\n",
    "          'schema': [  #schema is required in the exact same form for generating the artifact\n",
    "            {'name': 'fpr', 'type': 'NUMBER'},\n",
    "            {'name': 'tpr', 'type': 'NUMBER'},\n",
    "            {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "          ],\n",
    "          'source': 'gs://pipelines_artifacts/Artifacts/ROC_curveRF.csv'\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #Output the file to the container-level root with the exact same name as below \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    \n",
    "    #Also output to Minio \n",
    "    with file_io.FileIO(mlpipeline_ui_metadata, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    \n",
    "    #The metric scores can output as Pipeline Metrics by generating a json file as below\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) \n",
    "                            # and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'precision-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_precision, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'recall', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_recall, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'f1-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  rf_f1, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    },\n",
    "    {\n",
    "      'name': 'auc-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  auc_score, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "    }\n",
    "    \n",
    "    #Dump the metrics json file with the exact same name to the container-root directory\n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    #Also dump it to the Minio file storage\n",
    "    with file_io.FileIO(mlpipeline_metrics, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_rf_model = kfp.components.func_to_container_op(func = rf_model, \n",
    "                                                          output_component_file = './rf-model-func.yaml', \n",
    "                                                   packages_to_install = ['scikit-learn==0.22.2','numpy==1.17.2',\n",
    "                                                                                 'pandas==1.0.3', \n",
    "                                                                          'gcsfs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Pipeline Execution Sequence and Input-Output scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(name='Telco Merchant Churn Prediction Pipeline',description='Churn predictions using Random Forest Algorithm')\n",
    "def TelcoChurnRF_func(file_path = \"gs://pipelines_artifacts/Artifacts/Data.csv\", \n",
    "                n_estimators = 100, max_depth = 8, criterion = 'gini',  max_features='auto', min_samples_split=2):\n",
    "    \n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    read_data_task = kfp_read_data(file_name = file_path) \n",
    "    \n",
    "    cat_analysis_task = kfp_categorical_analysis(df_churn_ip = read_data_task.outputs['df_churn_op'])\n",
    "    mix_analysis_task = kfp_mixed_analysis(df_churn_ip = read_data_task.outputs['df_churn_op'], \n",
    "                                           df_churn_ip2 = cat_analysis_task.outputs['df_churn_op'])\n",
    "    num_analysis_task = kfp_numerical_analysis(df_churn_ip = mix_analysis_task.outputs['df_churn_op'])\n",
    "\n",
    "    ohe_task = kfp_one_hot_encode(df_churn_ip = read_data_task.outputs['df_churn_op'],\n",
    "                                  df_churn_imputed = num_analysis_task.outputs['df_churn_op'])\n",
    "    rf_model_task = kfp_rf_model(ohe_task.outputs['df_one_hot'],\n",
    "                                   n_estimators, max_depth, criterion, max_features, min_samples_split)\n",
    "\n",
    "#For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "#For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"100\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"8\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = TelcoChurnRF_func\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
